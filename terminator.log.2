2025-03-31 20:12:16,291 - terminator_agents - WARNING - OPENAI_API_KEY not set in environment variables
2025-03-31 20:17:15,130 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 20:27:46,351 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 20:45:51,628 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 20:45:51,629 - root - INFO - AI panel initialized successfully
2025-03-31 20:45:51,630 - root - INFO - AI panel initialized successfully
2025-03-31 20:52:03,557 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 20:52:03,559 - root - INFO - AI panel initialized successfully
2025-03-31 20:52:12,175 - root - ERROR - AI agent error: object Worker can't be used in 'await' expression
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/TerminatorV1_main.py", line 2006, in action_ai_request
    await self.call_ai_agent(prompt, code_context)
TypeError: object Worker can't be used in 'await' expression
2025-03-31 20:52:14,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 20:52:19,479 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 20:52:25,125 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 20:52:29,500 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 20:52:31,018 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 21:02:23,322 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:02:23,324 - root - INFO - AI panel initialized successfully
2025-03-31 21:05:33,921 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:05:33,923 - root - INFO - AI panel initialized successfully
2025-03-31 21:08:13,841 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:08:13,842 - root - INFO - AI panel initialized successfully
2025-03-31 21:13:59,942 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:13:59,945 - root - INFO - AI panel initialized successfully
2025-03-31 21:17:12,176 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:17:12,177 - root - INFO - AI panel initialized successfully
2025-03-31 21:21:15,361 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:21:15,362 - root - INFO - AI panel initialized successfully
2025-03-31 21:25:19,779 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-1246' coro=<Worker._run() done, defined at /Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/worker.py:356> exception=AttributeError("'AwaitComplete' object has no attribute '_pre_await'")>
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/worker.py", line 368, in _run
    self._result = await self.run()
                   ^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/worker.py", line 352, in run
    return await (
           ^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/worker.py", line 324, in _run_threaded
    return await loop.run_in_executor(None, runner, self._work)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/concurrent/futures/thread.py", line 58, in run
    result = self.fn(*self.args, **self.kwargs)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/worker.py", line 307, in run_callable
    return work()
           ^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/TerminatorV1_main.py", line 2109, in call_ai_agent
    # Get current content as a string - Markdown widgets use .update() in newer Textual
    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/widgets/_markdown.py", line 1049, in update
    return AwaitComplete(await_update())
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/await_complete.py", line 29, in __init__
    self._future: Future[Any] = gather(*awaitables)
                                ^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 817, in gather
    fut = _ensure_future(arg, loop=loop)
          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py", line 668, in _ensure_future
    loop = events._get_event_loop(stacklevel=4)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py", line 677, in get_event_loop
    raise RuntimeError('There is no current event loop in thread %r.'
RuntimeError: There is no current event loop in thread 'asyncio_1'.

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/worker.py", line 382, in _run
    app._handle_exception(worker_failed)
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/app.py", line 2981, in _handle_exception
    self._fatal_error()
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/app.py", line 2988, in _fatal_error
    traceback = Traceback(
                ^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/rich/traceback.py", line 269, in __init__
    trace = self.extract(
            ^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/rich/traceback.py", line 488, in extract
    {
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/rich/traceback.py", line 489, in <dictcomp>
    key: pretty.traverse(
         ^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/rich/pretty.py", line 874, in traverse
    node = _traverse(_object, root=True)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/rich/pretty.py", line 667, in _traverse
    args = list(iter_rich_args(rich_repr_result))
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/rich/pretty.py", line 634, in iter_rich_args
    for arg in rich_args:
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/textual/await_complete.py", line 35, in __rich_repr__
    yield "pre_await", self._pre_await, None
                       ^^^^^^^^^^^^^^^
AttributeError: 'AwaitComplete' object has no attribute '_pre_await'
2025-03-31 21:25:23,113 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:25:23,114 - root - INFO - AI panel initialized successfully
2025-03-31 21:25:32,116 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 21:25:33,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 21:25:35,600 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 21:25:39,719 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 21:38:14,812 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:38:14,813 - root - INFO - AI panel initialized successfully
2025-03-31 21:42:42,137 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 21:42:42,139 - root - INFO - AI panel initialized successfully
2025-03-31 21:42:56,482 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 21:42:58,078 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 21:42:58,260 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 21:43:04,059 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 22:51:15,786 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 22:51:21,586 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 22:51:24,578 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 22:51:26,728 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 22:52:28,504 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 22:52:28,955 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 22:52:29,870 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 22:52:34,329 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 22:52:34,831 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 22:52:40,214 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:05:03,924 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:05:18,826 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:05:20,073 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:05:51,246 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:05:53,455 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:05:56,410 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:05:57,324 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:05:57,945 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:08:16,033 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:08:18,025 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:08:20,126 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:08:21,749 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:39:22,169 - terminator_agents - ERROR - OPENAI_API_KEY not set in environment variables
2025-03-31 23:39:22,170 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-03-31 23:39:22,171 - root - INFO - AI panel initialized successfully
2025-03-31 23:40:16,698 - terminator_agents - INFO - Processing agent query: Hi, can you search for the file: /Users/kevinvanosch/Documents/AI\ agent/AIAgent0.1.py ?
2025-03-31 23:40:16,800 - terminator_agents - ERROR - Error in agent query: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/TerminatorV1_agents.py", line 1208, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 210, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 719, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 860, in _get_new_response
    model = cls._get_model(agent, run_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 904, in _get_model
    return run_config.model_provider.get_model(agent.model)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/models/openai_provider.py", line 85, in get_model
    client = self._get_client()
             ^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/models/openai_provider.py", line 71, in _get_client
    self._client = _openai_shared.get_default_openai_client() or AsyncOpenAI(
                                                                 ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/openai/_client.py", line 345, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
2025-03-31 23:40:17,983 - openai.agents - WARNING - OPENAI_API_KEY is not set, skipping trace export
2025-03-31 23:40:31,510 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=OpenAIError('The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable')>
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 719, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 860, in _get_new_response
    model = cls._get_model(agent, run_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 904, in _get_model
    return run_config.model_provider.get_model(agent.model)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/models/openai_provider.py", line 85, in get_model
    client = self._get_client()
             ^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/models/openai_provider.py", line 71, in _get_client
    self._client = _openai_shared.get_default_openai_client() or AsyncOpenAI(
                                                                 ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/openai/_client.py", line 345, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
2025-03-31 23:40:31,510 - asyncio - ERROR - Task was destroyed but it is pending!
task: <Task pending name='Task-1143' coro=<RunImpl.run_single_input_guardrail() running at /Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/_run_impl.py:644> wait_for=<_GatheringFuture finished exception=OpenAIError('The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable')> cb=[as_completed.<locals>._on_completion() at /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/tasks.py:592]>
2025-03-31 23:40:46,556 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-03-31 23:40:46,567 - terminator_agents - INFO - API key verification successful, found 65 models
2025-03-31 23:40:46,567 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 23:40:46,568 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-03-31 23:40:46,569 - root - INFO - AI panel initialized successfully
2025-03-31 23:41:05,543 - terminator_agents - INFO - Processing agent query: CAn you find the file: /Users/kevinvanosch/Documents/AI\ agent/AIAgent0.1.py ?
2025-03-31 23:41:06,126 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 400 Bad Request"
2025-03-31 23:41:06,128 - openai.agents - ERROR - Error getting response: Error code: 400 - {'error': {'message': "Invalid schema for function 'search_files': In context=('properties', 'recursive'), 'default' is not permitted.", 'type': 'invalid_request_error', 'param': 'tools[2].parameters', 'code': 'invalid_function_parameters'}}. (request_id: req_e9f3f43a6de2fd8a8268881f284d33ac)
2025-03-31 23:41:06,128 - terminator_agents - ERROR - Error in agent query: Error code: 400 - {'error': {'message': "Invalid schema for function 'search_files': In context=('properties', 'recursive'), 'default' is not permitted.", 'type': 'invalid_request_error', 'param': 'tools[2].parameters', 'code': 'invalid_function_parameters'}}
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/TerminatorV1_agents.py", line 1208, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 210, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 719, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 862, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/models/openai_responses.py", line 75, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/models/openai_responses.py", line 230, in _fetch_response
    return await self._client.responses.create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/openai/resources/responses/responses.py", line 1415, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/openai/_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/openai/_base_client.py", line 1461, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/openai/_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Invalid schema for function 'search_files': In context=('properties', 'recursive'), 'default' is not permitted.", 'type': 'invalid_request_error', 'param': 'tools[2].parameters', 'code': 'invalid_function_parameters'}}
2025-03-31 23:41:07,023 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:41:07,422 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:47:11,081 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-03-31 23:47:11,093 - terminator_agents - INFO - API key verification successful, found 65 models
2025-03-31 23:47:11,094 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 23:47:11,094 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-03-31 23:47:11,097 - root - INFO - AI panel initialized successfully
2025-03-31 23:47:47,982 - terminator_agents - INFO - Processing agent query: Can you find the file counting_words.py?
2025-03-31 23:47:51,603 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:47:58,927 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:48:02,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:48:03,448 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:48:03,609 - terminator_agents - INFO - Search request - term: 'counting_words.py', path: '/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual', pattern: 'counting_words.py', recursive: True
2025-03-31 23:48:08,524 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:48:17,513 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:48:17,672 - terminator_agents - INFO - Successfully completed agent query
2025-03-31 23:48:19,407 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:51:48,102 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-03-31 23:51:48,113 - terminator_agents - INFO - API key verification successful, found 65 models
2025-03-31 23:51:48,113 - terminator_agents - INFO - Agent system initialized successfully
2025-03-31 23:51:48,114 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-03-31 23:51:48,117 - root - INFO - AI panel initialized successfully
2025-03-31 23:52:05,811 - terminator_agents - INFO - Processing agent query: Hi there, Can you find the file counting_words.py?
2025-03-31 23:52:08,356 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:52:11,920 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:52:12,113 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:52:12,275 - terminator_agents - INFO - Search request - term: 'counting_words.py', path: '/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual', pattern: 'None', recursive: True
2025-03-31 23:52:12,282 - terminator_agents - INFO - Found content matches in: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/terminator.log
2025-03-31 23:52:14,203 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:52:21,715 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:52:21,866 - terminator_agents - INFO - Successfully completed agent query
2025-03-31 23:52:24,873 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:53:22,108 - terminator_agents - INFO - Processing agent query: My mistake it is count_words.py. Could you try that?
2025-03-31 23:53:26,523 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:53:28,498 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:53:28,648 - terminator_agents - INFO - Search request - term: 'count_words.py', path: '/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual', pattern: 'count_words.py', recursive: True
2025-03-31 23:53:30,052 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:53:32,319 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:53:33,822 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:53:33,974 - terminator_agents - INFO - Successfully completed agent query
2025-03-31 23:53:37,997 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:54:51,087 - terminator_agents - INFO - Processing agent query: It is in a sub directory, look there.
2025-03-31 23:54:54,985 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:54:59,847 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:54:59,999 - terminator_agents - INFO - Search request - term: 'count_words.py', path: '/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual', pattern: 'None', recursive: True
2025-03-31 23:55:00,004 - terminator_agents - INFO - Found content matches in: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/terminator.log
2025-03-31 23:55:00,647 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-03-31 23:55:04,394 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-03-31 23:55:04,398 - terminator_agents - ERROR - Error in agent query: Guardrail InputGuardrail triggered tripwire
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/TerminatorV1_agents.py", line 1179, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 210, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/Anthropic/terminal agent/venvImac/lib/python3.11/site-packages/agents/run.py", line 805, in _run_input_guardrails
    raise InputGuardrailTripwireTriggered(result)
agents.exceptions.InputGuardrailTripwireTriggered: Guardrail InputGuardrail triggered tripwire
2025-03-31 23:55:06,341 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:01:24,555 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 00:01:24,566 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 00:01:24,566 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 00:01:24,566 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 00:01:24,567 - root - INFO - AI panel initialized successfully
2025-04-01 00:01:42,615 - terminator_agents - INFO - Processing agent query: Can you search for the file count_words.py ?
2025-04-01 00:01:42,651 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: search.*file
2025-04-01 00:01:44,341 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:02:04,053 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:02:04,224 - terminator_agents - INFO - Searching for file: 'count_words.py', starting from: '/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual', recursive: True
2025-04-01 00:02:05,333 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:02:08,918 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:02:09,076 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 00:02:11,588 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:02:50,250 - terminator_agents - INFO - Processing agent query: It is in the countingthewords directory can you find that then?
2025-04-01 00:02:50,256 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: find 
2025-04-01 00:02:52,894 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:02:55,409 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:02:55,564 - terminator_agents - INFO - Attempting to list directory: countingthewords
2025-04-01 00:02:55,564 - terminator_agents - INFO - Resolved to absolute path: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/countingthewords
2025-04-01 00:02:55,565 - terminator_agents - ERROR - Directory not found: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/countingthewords
2025-04-01 00:02:58,705 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:03:06,122 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:03:09,512 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:03:20,098 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:03:20,253 - terminator_agents - INFO - Search request - term: 'countingthewords', path: '/Users/kevinvanosch/Documents/Anthropic/terminal agent/.', pattern: 'None', recursive: True
2025-04-01 00:03:25,351 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:03:28,465 - terminator_agents - INFO - Found content matches in: /Users/kevinvanosch/Documents/Anthropic/terminal agent/./Textual/terminator.log
2025-04-01 00:03:37,434 - terminator_agents - INFO - Found content matches in: /Users/kevinvanosch/Documents/Anthropic/terminal agent/./.git/index
2025-04-01 00:03:41,162 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:03:42,226 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:03:42,379 - terminator_agents - INFO - Attempting to list directory: Textual
2025-04-01 00:03:42,379 - terminator_agents - INFO - Resolved to absolute path: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 00:03:42,381 - terminator_agents - INFO - Successfully listed directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 00:03:46,982 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:03:47,677 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:03:47,828 - terminator_agents - INFO - Attempting to list directory: .
2025-04-01 00:03:47,828 - terminator_agents - INFO - Resolved to absolute path: /Users/kevinvanosch/Documents/Anthropic/terminal agent/.
2025-04-01 00:03:47,831 - terminator_agents - INFO - Successfully listed directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/.
2025-04-01 00:03:52,757 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:13:21,357 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 00:13:21,368 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 00:13:21,368 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 00:13:21,369 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 00:13:21,370 - root - INFO - AI panel initialized successfully
2025-04-01 00:13:40,343 - terminator_agents - INFO - Processing agent query: Hi, can you locate the count_words.py file for me?
2025-04-01 00:13:41,576 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:13:45,176 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:13:47,191 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:13:55,883 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:13:56,041 - terminator_agents - INFO - Searching for file: 'count_words.py', starting from: '/Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual', recursive: True
2025-04-01 00:13:57,994 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:14:00,704 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:14:00,859 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 00:14:03,787 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:14:20,937 - terminator_agents - INFO - Processing agent query: Look better and look in a parent.
2025-04-01 00:14:24,832 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:14:27,844 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:14:27,998 - terminator_agents - INFO - Attempting to list directory: ..
2025-04-01 00:14:27,998 - terminator_agents - INFO - Resolved to absolute path: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/..
2025-04-01 00:14:28,002 - terminator_agents - INFO - Successfully listed directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/..
2025-04-01 00:14:29,579 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:14:30,498 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:14:41,802 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:14:41,956 - terminator_agents - INFO - Attempting to list directory: ../..
2025-04-01 00:14:41,956 - terminator_agents - INFO - Resolved to absolute path: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/../..
2025-04-01 00:14:41,957 - terminator_agents - INFO - Successfully listed directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/../..
2025-04-01 00:14:46,538 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 00:23:36,881 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 00:23:37,492 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 00:23:41,291 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:01:05,099 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:01:05,111 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:01:05,112 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:01:05,112 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:01:05,114 - root - INFO - AI panel initialized successfully
2025-04-01 01:03:00,409 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:03:00,418 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:03:00,418 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:03:00,419 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:03:00,420 - root - INFO - AI panel initialized successfully
2025-04-01 01:07:08,493 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:07:08,503 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:07:08,503 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:07:08,504 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:07:08,505 - root - INFO - AI panel initialized successfully
2025-04-01 01:09:28,652 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:09:28,662 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:09:28,662 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:09:28,663 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:09:28,665 - root - INFO - AI panel initialized successfully
2025-04-01 01:19:17,012 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:19:17,023 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:19:17,024 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:19:17,024 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:19:17,026 - root - INFO - AI panel initialized successfully
2025-04-01 01:21:27,397 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:21:27,404 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:21:27,405 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:21:27,405 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:21:27,407 - root - INFO - AI panel initialized successfully
2025-04-01 01:27:17,474 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:27:17,483 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:27:17,483 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:27:17,484 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:27:17,485 - root - INFO - AI panel initialized successfully
2025-04-01 01:28:22,704 - terminator_agents - INFO - Processing agent query: Can you help me write a script in the editor?
2025-04-01 01:28:26,603 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:28:28,143 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:28:28,708 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:28:28,872 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 01:28:34,237 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:29:22,195 - terminator_agents - INFO - Processing agent query: I want to write a script that counts words in a txt file. Can you help me write it in the editor?
2025-04-01 01:29:26,370 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:29:26,655 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:29:27,208 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:29:27,861 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:29:32,612 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:35:49,294 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
#!
```

My question: test
2025-04-01 01:35:49,968 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:35:56,158 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:36:00,657 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:37:43,001 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:37:43,010 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:37:43,011 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:37:43,011 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:37:43,014 - root - INFO - AI panel initialized successfully
2025-04-01 01:37:50,645 - terminator_agents - INFO - Processing agent query: test
2025-04-01 01:37:52,701 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:37:54,555 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:37:55,009 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:37:55,167 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 01:37:58,342 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:38:14,861 - terminator_agents - INFO - Processing agent query: Can you put that into the editor?
2025-04-01 01:38:19,476 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:38:26,216 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:38:30,228 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:38:32,784 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:38:32,788 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 01:38:35,784 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:38:53,103 - terminator_agents - INFO - Processing agent query: Can you open terminator.log in the editor?
2025-04-01 01:38:56,717 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:38:57,590 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:39:00,241 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:39:00,395 - terminator_agents - INFO - Attempting to read file: terminator.log
2025-04-01 01:39:00,395 - terminator_agents - INFO - Current directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:39:00,395 - terminator_agents - INFO - Resolved file path: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/terminator.log
2025-04-01 01:39:00,396 - terminator_agents - INFO - Successfully read file: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual/terminator.log
2025-04-01 01:39:02,716 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:44:23,113 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 01:44:23,124 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 01:44:23,124 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 01:44:23,124 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 01:44:23,127 - root - INFO - AI panel initialized successfully
2025-04-01 01:45:18,622 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import sys


def count_words_in_file(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"Error reading file: {e}")
        return None
    
    # Split the content into words based on whitespace
    words = content.split()
    return len(words)


if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python word_counter.py <filename.txt>")
        sys.exit(1)
    
    file_path = sys.argv[1]
    word_count = count_words_in_file(file_path)
    if word_count is not None:
        print(f"The file '{file_path}' contains {word_count} words.")

```

My question: Can you see the file in the editor?
2025-04-01 01:45:18,630 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: open.*file
2025-04-01 01:45:24,100 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 01:45:30,261 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 01:45:30,430 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 01:45:34,920 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 10:13:45,183 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 10:13:45,193 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 10:13:45,193 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 10:13:45,194 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/Anthropic/terminal agent/Textual
2025-04-01 10:13:45,196 - root - INFO - AI panel initialized successfully
2025-04-01 10:13:56,777 - terminator_agents - INFO - Processing agent query: Hi, Just testing.
2025-04-01 10:14:00,109 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 10:14:02,142 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 10:14:02,595 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 10:14:06,063 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 10:20:28,986 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 10:20:29,612 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 10:20:33,524 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 10:47:12,417 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 10:47:12,426 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 10:47:12,426 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 10:47:12,426 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 10:47:12,428 - root - INFO - AI panel initialized successfully
2025-04-01 10:47:38,438 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
"""
TerminatorV1 Agents - Agent integration for the Terminator IDE
Provides OpenAI agent-based code assistance, analysis, and generation
"""

import os
import sys
import asyncio
import logging
import json
import time
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
from pydantic import BaseModel, Field

# Import OpenAI and agent framework
from openai import OpenAI
from agents import (
    Agent,
    ModelSettings, 
    Runner, 
    function_tool, 
    RunContextWrapper, 
    trace, 
    handoff,
    set_default_openai_key,
    input_guardrail,
    output_guardrail,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    OutputGuardrailTripwireTriggered,
    TResponseInputItem,
    ItemHelpers,
    RunConfig,
    MaxTurnsExceeded,
    ModelBehaviorError,
    AgentsException
)

# Import tools for the agents to use
from TerminatorV1_tools import FileSystem, CodeAnalyzer, GitManager, PythonRunner

# Set up logging
logger = logging.getLogger("terminator_agents")
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Set API key from environment
OpenAI.api_key = os.getenv("OPENAI_API_KEY")
set_default_openai_key(os.getenv("OPENAI_API_KEY"))

# Define security guardrail for malicious inputs
class SecurityCheckOutput(BaseModel):
    """Output model for security check guardrail"""
    is_malicious: bool = Field(..., description="Whether the input appears to be malicious")
    risk_type: Optional[str] = Field(None, description="Type of security risk identified")
    reasoning: str = Field(..., description="Reasoning behind the security assessment")

# Define the context for the agent
class AgentContext(BaseModel):
    """Context object to pass between agents and tools"""
    current_dir: str
    history_summary: str = ""
    token_count: int = 0
    max_tokens: int = 150000
    accessed_files: List[str] = Field(default_factory=list)
    executed_commands: List[str] = Field(default_factory=list)
    last_operation: Optional[str] = None
    session_id: str = Field(default_factory=lambda: f"session_{int(time.time())}")
    
    def update_token_count(self, new_tokens: int) -> bool:
        """
        Update token count and check if summarization is needed
        
        Args:
            new_tokens: Number of tokens to add to the count
            
        Returns:
            bool: True if summarization is needed, False otherwise
        """
        self.token_count += new_tokens
        return self.token_count >= self.max_tokens
    
    def reset_token_count(self) -> None:
        """Reset token count after summarization"""
        self.token_count = 0
        
    def track_file_access(self, file_path: str) -> None:
        """
        Track file access in the context
        
        Args:
            file_path: Path of the accessed file
        """
        if file_path not in self.accessed_files:
            self.accessed_files.append(file_path)
            
    def track_command(self, command: str) -> None:
        """
        Track command execution in the context
        
        Args:
            command: The executed command
        """
        self.executed_commands.append(command)
        
    def set_operation(self, operation: str) -> None:
        """
        Set the last operation performed
        
        Args:
            operation: Description of the operation
        """
        self.last_operation = operation

# Create a security check agent
# Create a security check agent with more permissive file system rules
security_check_agent = Agent(
    name="Security Guardrail",
    instructions="""You are a security guardrail for a Python development environment. Your job is to analyze user input and 
    determine if it might be trying to:
    
    1. Execute clearly malicious code with harmful intent
    2. Access highly sensitive system information (like /etc/passwd, SSH keys, etc.)
    3. Perform destructive operations with clear intent to damage (rm -rf /, format drives, etc.)
    4. Use the system for hacking, attacks, or other explicitly harmful purposes
    5. Execute code injections, XSS, or other attack vectors against external systems
    
    IMPORTANT: The following actions are ALLOWED and should NOT be flagged:
    - Normal directory traversal and searching (cd, ls, find, etc.)
    - Reading code files and documentation
    - File operations within the project directory structure 
    - Searching in subdirectories, even recursively
    - Git operations
    - Running or debugging Python code that appears legitimate
    
    Only flag requests that have clear malicious intent, not legitimate development activities.
    Directory traversal within a development project is entirely legitimate and should be ALLOWED.
    
    Return a structured assessment with your conclusion.""",
    model="o3-mini",
    output_type=SecurityCheckOutput
)

@input_guardrail
async def security_guardrail(
    ctx: RunContextWrapper[AgentContext], 
    agent: Agent, 
    input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    """
    Security guardrail to prevent malicious inputs, with exceptions for normal file operations
    
    Args:
        ctx: Context wrapper
        agent: The agent being protected
        input: The user input
        
    Returns:
        GuardrailFunctionOutput with security assessment
    """
    # Check if the input is asking for a basic file system operation that should be allowed
    input_str = input if isinstance(input, str) else ItemHelpers.stringify_input(input)
    
    # List of patterns that should be explicitly allowed
    safe_patterns = [
        r'search.*file',
        r'search.*directory',
        r'find.*file',
        r'look for.*file',
        r'search in subdirectory',
        r'search recursively',
        r'list.*directory',
        r'list.*file',
        r'explore.*directory',
        r'navigate.*directory',
        r'change directory',
        r'cd ',
        r'ls ',
        r'dir ',
        r'find ',
        r'open.*file',
        r'read.*file'
    ]
    
    # Check if input matches any safe pattern
    for pattern in safe_patterns:
        if re.search(pattern, input_str, re.IGNORECASE):
            logger.info(f"Security guardrail: Allowing file operation matching pattern: {pattern}")
            return GuardrailFunctionOutput(
                output_info=SecurityCheckOutput(
                    is_malicious=False,
                    risk_type=None,
                    reasoning="This is a legitimate file system operation for development purposes."
                ),
                tripwire_triggered=False
            )

    # Run the security check agent on the input
    result = await Runner.run(security_check_agent, input, context=ctx.context)
    security_check = result.final_output
    
    # Double-check for file operation false positives
    if security_check.is_malicious and "directory" in input_str.lower():
        # Reduce false positives for directory operations
        logger.info("Security guardrail: Overriding false positive for directory operation")
        return GuardrailFunctionOutput(
            output_info=SecurityCheckOutput(
                is_malicious=False,
                risk_type=None,
                reasoning="File system operation appears to be legitimate."
            ),
            tripwire_triggered=False
        )
    
    # Return the guardrail result
    return GuardrailFunctionOutput(
        output_info=security_check,
        tripwire_triggered=security_check.is_malicious,
    )

# Define Pydantic models for structured agent outputs
class CodeGenerationOutput(BaseModel):
    code: str = Field(..., description="The generated Python code")
    explanation: str = Field(..., description="Explanation of what the code does")
    file_path: Optional[str] = Field(None, description="Suggested file path for the code")

class CodeAnalysisOutput(BaseModel):
    issues: List[Dict[str, Any]] = Field(default_factory=list, description="List of identified issues")
    improvements: List[str] = Field(default_factory=list, description="Suggested improvements")
    summary: str = Field(..., description="Summary of code analysis")

class ProjectAnalysisOutput(BaseModel):
    structure: Dict[str, Any] = Field(..., description="Project structure information")
    dependencies: List[str] = Field(default_factory=list, description="Project dependencies")
    recommendations: List[str] = Field(default_factory=list, description="Recommendations for improvement")
    summary: str = Field(..., description="Summary of project analysis")

class TerminalAgentOutput(BaseModel):
    response: str = Field(..., description="The agent's response to the user query")
    files_accessed: List[str] = Field(default_factory=list, description="Files accessed during processing")
    commands_executed: List[str] = Field(default_factory=list, description="Commands executed during processing")

# File System Tools
@function_tool
async def list_directory(ctx: RunContextWrapper[AgentContext], path: Optional[str] = None) -> str:
    """
    List the contents of a directory.
    
    Args:
        path: Optional path to list. If not provided, uses current directory.
    
    Returns:
        A string representation of the directory contents.
    """
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return "Error: Agent context is invalid or missing current directory information."
        
        # Resolve target path
        target_path = path if path is not None else ctx.context.current_dir
        
        # Log the target path for debugging
        logger.info(f"Attempting to list directory: {target_path}")
        
        # Handle relative paths
        if not os.path.isabs(target_path):
            target_path = os.path.join(ctx.context.current_dir, target_path)
            logger.info(f"Resolved to absolute path: {target_path}")
        
        # Check if directory exists
        if not os.path.exists(target_path):
            logger.error(f"Directory not found: {target_path}")
            return f"Error: Directory not found: {target_path}"
            
        if not os.path.isdir(target_path):
            logger.error(f"Path is not a directory: {target_path}")
            return f"Error: Path is not a directory: {target_path}"
        
        # Use the FileSystem utility with max_depth=2 (handled inside function)
        structure = FileSystem.get_directory_structure(target_path, max_depth=2)
        
        # Check if we got an error
        if isinstance(structure, dict) and "error" in structure:
            logger.error(f"Error getting directory structure: {structure['error']}")
            return f"Error listing directory: {structure['error']}"
        
        # Log success
        logger.info(f"Successfully listed directory: {target_path}")
        
        # Track operation
        ctx.context.set_operation(f"Listed directory: {os.path.basename(target_path) or target_path}")
        
        return json.dumps(structure, indent=2)
    except Exception as e:
        logger.error(f"Exception listing directory {path}: {str(e)}", exc_info=True)
        return f"Error listing directory: {str(e)}"

@function_tool
async def find_file(
    ctx: RunContextWrapper[AgentContext], 
    filename: str,
    start_path: Optional[str] = None,
    recursive: Optional[bool] = None
) -> str:
    """
    Safely find a file by name in the filesystem
    
    Args:
        filename: Name of the file to find
        start_path: Directory to start searching from (uses current directory if not specified)
        recursive: Whether to search recursively in subdirectories (default True)
        
    Returns:
        Path to the file if found, or error message
    """
    if recursive is None:
        recursive = True
        
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return json.dumps({
                "error": "Agent context is invalid or missing current directory information."
            })
        
        # Get the starting search path
        search_path = start_path or ctx.context.current_dir
        
        # Handle special paths
        if search_path == "~" or search_path.startswith("~/"):
            search_path = os.path.expanduser(search_path)
        
        # Handle relative paths
        if not os.path.isabs(search_path):
            search_path = os.path.join(ctx.context.current_dir, search_path)
        
        logger.info(f"Searching for file: '{filename}', starting from: '{search_path}', recursive: {recursive}")
        
        # Check if the starting path exists
        if not os.path.exists(search_path):
            logger.error(f"Search path not found: {search_path}")
            return json.dumps({
                "error": f"Search path not found: {search_path}",
                "found": False
            })
            
        if not os.path.isdir(search_path):
            logger.error(f"Search path is not a directory: {search_path}")
            return json.dumps({
                "error": f"Search path is not a directory: {search_path}",
                "found": False
            })
        
        found_files = []
        
        # Function to recursively walk directories and find files
        def find_in_directory(current_path, max_depth=30, current_depth=0):
            if current_depth > max_depth:
                return
            
            try:
                # List all items in the current directory
                items = os.listdir(current_path)
                
                for item in items:
                    item_path = os.path.join(current_path, item)
                    
                    # Check if it's a file that matches
                    if os.path.isfile(item_path) and item == filename:
                        found_files.append(item_path)
                        logger.info(f"Found matching file: {item_path}")
                    
                    # If it's a directory and recursive search is enabled, search it too
                    elif os.path.isdir(item_path) and recursive:
                        find_in_directory(item_path, max_depth, current_depth + 1)
            
            except Exception as e:
                logger.warning(f"Error accessing directory {current_path}: {str(e)}")
                # Continue the search in other directories
        
        # Start the search
        find_in_directory(search_path)
        
        # Return the results
        if not found_files:
            return json.dumps({
                "found": False,
                "message": f"File '{filename}' not found starting from '{search_path}'"
            })
        
        return json.dumps({
            "found": True,
            "file_count": len(found_files),
            "files": found_files,
            "message": f"Found {len(found_files)} matching files"
        })
        
    except Exception as e:
        logger.error(f"Exception in find_file: {str(e)}", exc_info=True)
        return json.dumps({
            "error": f"Error searching for file: {str(e)}",
            "found": False
        })

@function_tool
async def read_file(ctx: RunContextWrapper[AgentContext], file_path: str) -> str:
    """
    Read and return the contents of a file.
    
    Args:
        file_path: Path to the file to read. Can be absolute or relative to current directory.
    
    Returns:
        The contents of the file as a string.
    """
    try:
        # Log the original file path for debugging
        logger.info(f"Attempting to read file: {file_path}")
        logger.info(f"Current directory: {ctx.context.current_dir}")
        
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return "Error: Agent context is invalid or missing current directory information."
        
        # Handle relative paths
        if not os.path.isabs(file_path):
            file_path = os.path.join(ctx.context.current_dir, file_path)
        
        # Log the resolved file path
        logger.info(f"Resolved file path: {file_path}")
        
        # Check if file exists before trying to read it
        if not os.path.exists(file_path):
            logger.error(f"File not found: {file_path}")
            return f"Error: File not found: {file_path}"
            
        if not os.path.isfile(file_path):
            logger.error(f"Path is not a file: {file_path}")
            return f"Error: Path is not a file: {file_path}"
        
        # Use the FileSystem utility
        success, content = FileSystem.read_file(file_path)
        
        if not success:
            logger.error(f"Failed to read file: {content}")
            return f"Error reading file: {content}"
        
        # Log success
        logger.info(f"Successfully read file: {file_path}")
        
        # Update token count (rough estimate)
        tokens_added = len(content.split())
        needs_summary = ctx.context.update_token_count(tokens_added)
        
        # Track file access in context
        ctx.context.track_file_access(file_path)
        ctx.context.set_operation(f"Read file: {os.path.basename(file_path)}")
        
        return content
    except Exception as e:
        logger.error(f"Exception reading file {file_path}: {str(e)}", exc_info=True)
        return f"Error reading file: {str(e)}"

@function_tool
async def search_files(
    ctx: RunContextWrapper[AgentContext], 
    search_term: str, 
    path: Optional[str] = None,
    file_pattern: Optional[str] = None,
    recursive: Optional[bool] = None,
    case_sensitive: Optional[bool] = None
) -> str:
    """
    Enhanced file search with improved robustness for finding files across directories
    
    Args:
        search_term: Term to search for (filename or content)
        path: Directory to search in (uses current directory if not specified)
        file_pattern: File pattern to match (e.g., "*.py")
        recursive: Whether to search recursively in subdirectories (default True)
        case_sensitive: Whether the search is case sensitive (default False)
        
    Returns:
        Structured search results
    """
    # Set default values for optional parameters inside the function body
    if recursive is None:
        recursive = True
    if case_sensitive is None:
        case_sensitive = False
        
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return json.dumps({
                "error": "Agent context is invalid or missing current directory information."
            })
        
        # Validate search term
        if not search_term or not isinstance(search_term, str):
            logger.error(f"Invalid search term: {search_term}")
            return json.dumps({
                "error": "Search term is required and must be a string."
            })
            
        # Get the target path, expanding user directory
        target_path = path or ctx.context.current_dir
        
        # Handle special paths
        if target_path == "~" or target_path.startswith("~/"):
            target_path = os.path.expanduser(target_path)
        
        # Handle relative paths
        if not os.path.isabs(target_path):
            target_path = os.path.join(ctx.context.current_dir, target_path)
        
        logger.info(f"Search request - term: '{search_term}', path: '{target_path}', pattern: '{file_pattern}', recursive: {recursive}")
        
        # Check if directory exists
        if not os.path.exists(target_path):
            logger.error(f"Directory not found: {target_path}")
            return json.dumps({
                "error": f"Directory not found: {target_path}"
            })
            
        if not os.path.isdir(target_path):
            logger.error(f"Path is not a directory: {target_path}")
            return json.dumps({
                "error": f"Path is not a directory: {target_path}"
            })
        
        results = {
            "filename_matches": [],
            "content_matches": []
        }
        
        # Track operation in context
        ctx.context.set_operation(f"Searching for '{search_term}' in {target_path}")
        
        # Function to recursively walk directories and find files
        def walk_directory(current_path, max_depth=20, current_depth=0):
            if current_depth > max_depth:
                return
            
            try:
                # List all items in the current directory
                items = os.listdir(current_path)
                
                for item in items:
                    item_path = os.path.join(current_path, item)
                    
                    # Check if it's a file
                    if os.path.isfile(item_path):
                        # Check if file matches search criteria
                        if file_pattern:
                            import fnmatch
                            if not fnmatch.fnmatch(item, file_pattern):
                                continue
                        
                        # Check for filename match
                        filename_match = False
                        if case_sensitive:
                            if search_term == item:
                                filename_match = True
                                match_type = "exact match"
                            elif search_term in item:
                                filename_match = True
                                match_type = "partial match"
                        else:
                            if search_term.lower() == item.lower():
                                filename_match = True
                                match_type = "exact match"
                            elif search_term.lower() in item.lower():
                                filename_match = True
                                match_type = "partial match"
                        
                        if filename_match:
                            results["filename_matches"].append({
                                "path": item_path,
                                "type": match_type
                            })
                            logger.info(f"Found filename match: {item_path}")
                        
                        # Check for content match if there's a search term
                        if search_term:
                            try:
                                success, content = FileSystem.read_file(item_path, max_size_mb=5)
                                if success:
                                    content_matches = []
                                    for i, line in enumerate(content.splitlines()):
                                        if (case_sensitive and search_term in line) or \
                                            (not case_sensitive and search_term.lower() in line.lower()):
                                            content_matches.append({
                                                "line": i + 1,
                                                "content": line.strip()
                                            })
                                    
                                    if content_matches:
                                        results["content_matches"].append({
                                            "file": item_path,
                                            "matches": content_matches[:5]  # Limit to 5 matches per file
                                        })
                                        logger.info(f"Found content matches in: {item_path}")
                                        ctx.context.track_file_access(item_path)
                            except Exception as e:
                                logger.warning(f"Error reading file {item_path}: {str(e)}")
                    
                    # If it's a directory and recursive search is enabled, search it too
                    elif os.path.isdir(item_path) and recursive:
                        walk_directory(item_path, max_depth, current_depth + 1)
            
            except Exception as e:
                logger.warning(f"Error accessing directory {current_path}: {str(e)}")
        
        # Start the directory walk from the target path
        walk_directory(target_path)
        
        # Clean up and format the results
        if not results["filename_matches"] and not results["content_matches"]:
            return json.dumps({
                "found": False,
                "message": f"No matches found for '{search_term}' in {target_path}"
            })
        
        return json.dumps({
            "found": True,
            "filename_matches": results["filename_matches"],
            "content_matches": results["content_matches"],
            "message": f"Found {len(results['filename_matches'])} files matching '{search_term}' and {len(results['content_matches'])} files with matching content"
        })
        
    except Exception as e:
        logger.error(f"Exception in search_files: {str(e)}", exc_info=True)
        return json.dumps({"error": f"Error searching files: {str(e)}"})

@function_tool
async def find_in_parent_directories(
    ctx: RunContextWrapper[AgentContext], 
    name: str,
    levels_up: Optional[int] = None,
    is_directory: Optional[bool] = None
) -> str:
    """
    Search for a file or directory in the current directory and parent directories
    
    Args:
        name: Name of the file or directory to find
        levels_up: Maximum number of parent directories to check (None for no limit)
        is_directory: Whether to look for a directory (True) or file (False). None means either.
        
    Returns:
        Path information if found, or error message
    """
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return json.dumps({
                "error": "Agent context is invalid or missing current directory information.",
                "found": False
            })
        
        # Start with current directory
        current_path = ctx.context.current_dir
        logger.info(f"Starting search for {'directory' if is_directory else 'file'} '{name}' from {current_path}")
        
        # Keep track of searched directories
        searched_dirs = []
        level = 0
        
        # Check current and parent directories
        while current_path and (levels_up is None or level <= levels_up):
            searched_dirs.append(current_path)
            
            # First check for direct match
            target_path = os.path.join(current_path, name)
            logger.info(f"Checking {target_path}")
            
            if os.path.exists(target_path):
                # Check if it's the right type (file or directory)
                if is_directory is None:
                    # Either type is fine
                    logger.info(f"Found match: {target_path}")
                    return json.dumps({
                        "found": True,
                        "path": target_path,
                        "type": "directory" if os.path.isdir(target_path) else "file",
                        "parent_dir": current_path,
                        "levels_up": level
                    })
                elif is_directory and os.path.isdir(target_path):
                    # Found matching directory
                    logger.info(f"Found directory: {target_path}")
                    return json.dumps({
                        "found": True,
                        "path": target_path,
                        "type": "directory",
                        "parent_dir": current_path,
                        "levels_up": level
                    })
                elif not is_directory and os.path.isfile(target_path):
                    # Found matching file
                    logger.info(f"Found file: {target_path}")
                    return json.dumps({
                        "found": True,
                        "path": target_path,
                        "type": "file",
                        "parent_dir": current_path,
                        "levels_up": level
                    })
            
            # Next, search inside directory for sub-matches if we're looking for files
            if is_directory is not True:  # If we're looking for files or either type
                try:
                    # List the directory and see if we can find a partial match
                    for item in os.listdir(current_path):
                        item_path = os.path.join(current_path, item)
                        if os.path.isfile(item_path) and name in item:
                            logger.info(f"Found partial file match: {item_path}")
                            return json.dumps({
                                "found": True,
                                "path": item_path,
                                "type": "file",
                                "parent_dir": current_path,
                                "levels_up": level,
                                "note": f"Partial match for '{name}'"
                            })
                except Exception as e:
                    logger.warning(f"Error listing directory {current_path}: {str(e)}")
            
            # Move up to parent directory
            parent_path = os.path.dirname(current_path)
            if parent_path == current_path:
                # We've reached the root
                break
                
            current_path = parent_path
            level += 1
        
        # If we get here, we didn't find it
        return json.dumps({
            "found": False,
            "searched_directories": searched_dirs,
            "message": f"Could not find {'directory' if is_directory else 'file'} '{name}' in current or parent directories"
        })
        
    except Exception as e:
        logger.error(f"Exception in find_in_parent_directories: {str(e)}", exc_info=True)
        return json.dumps({
            "error": f"Error searching for {name}: {str(e)}",
            "found": False
        })

@function_tool
async def search_project_directories(
    ctx: RunContextWrapper[AgentContext], 
    name: str,
    directory_level: Optional[str] = None,
    search_type: Optional[str] = None
) -> str:
    """
    Perform a comprehensive search for files or directories in the project
    
    Args:
        name: Name of the file or directory to find
        directory_level: Where to search ('current', 'parent', 'parent_parent', 'all' or 'root')
        search_type: What to search for ('file', 'directory', or 'both')
        
    Returns:
        JSON result with search findings
    """
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return json.dumps({
                "error": "Agent context is invalid or missing current directory information."
            })
        
        # Set default values
        if directory_level is None:
            directory_level = "all"
        
        if search_type is None:
            search_type = "both"
            
        # Determine what kind of object we're looking for
        is_directory = None
        if search_type == "file":
            is_directory = False
        elif search_type == "directory":
            is_directory = True
        # else leave as None for "both"
        
        # Determine starting directory
        current_dir = ctx.context.current_dir
        parent_dir = os.path.dirname(current_dir)
        parent_parent_dir = os.path.dirname(parent_dir)
        
        # Find the project root (usually the Git root if available)
        root_dir = None
        try:
            is_repo, repo_root = GitManager.check_git_repo(current_dir)
            if is_repo:
                root_dir = repo_root
        except Exception:
            # If we can't determine Git root, use 3 levels up as estimate
            temp = current_dir
            for _ in range(3):
                temp = os.path.dirname(temp)
                if temp == os.path.dirname(temp):  # We've hit the filesystem root
                    break
            root_dir = temp
        
        # Determine which directories to search
        dirs_to_search = []
        
        if directory_level == "current":
            dirs_to_search = [current_dir]
        elif directory_level == "parent":
            dirs_to_search = [parent_dir]
        elif directory_level == "parent_parent":
            dirs_to_search = [parent_parent_dir]
        elif directory_level == "root" and root_dir:
            dirs_to_search = [root_dir]
        else:  # "all" or unrecognized value
            # Build a path from root to current to search all relevant directories
            dirs_to_search = []
            temp = current_dir
            while temp and (not root_dir or os.path.commonpath([temp, root_dir]) == root_dir):
                dirs_to_search.append(temp)
                parent = os.path.dirname(temp)
                if parent == temp:  # We've hit the filesystem root
                    break
                temp = parent
            # Add project root if available and not already included
            if root_dir and root_dir not in dirs_to_search:
                dirs_to_search.append(root_dir)
        
        # Log search parameters
        logger.info(f"Searching for {search_type} '{name}' in: {dirs_to_search}")
        
        results = {
            "exact_matches": [],
            "partial_matches": [],
            "searched_directories": dirs_to_search
        }
        
        # Search each directory
        for dir_path in dirs_to_search:
            if not os.path.exists(dir_path) or not os.path.isdir(dir_path):
                continue
                
            # First check for direct matches in this directory
            target_path = os.path.join(dir_path, name)
            if os.path.exists(target_path):
                # Check if it's the right type
                if is_directory is None or (is_directory and os.path.isdir(target_path)) or (not is_directory and os.path.isfile(target_path)):
                    results["exact_matches"].append({
                        "path": target_path,
                        "parent": dir_path,
                        "type": "directory" if os.path.isdir(target_path) else "file"
                    })
            
            # Then do a more comprehensive search inside this directory
            try:
                # Walk through the directory
                for root, dirs, files in os.walk(dir_path):
                    # Check files if we're looking for files or both
                    if is_directory is not True:  # False or None
                        for file in files:
                            if name == file:  # Exact match
                                file_path = os.path.join(root, file)
                                results["exact_matches"].append({
                                    "path": file_path,
                                    "parent": root,
                                    "type": "file"
                                })
                            elif name in file:  # Partial match
                                file_path = os.path.join(root, file)
                                results["partial_matches"].append({
                                    "path": file_path,
                                    "parent": root,
                                    "type": "file"
                                })
                    
                    # Check directories if we're looking for directories or both
                    if is_directory is not False:  # True or None
                        for dir_name in dirs:
                            if name == dir_name:  # Exact match
                                dir_path = os.path.join(root, dir_name)
                                results["exact_matches"].append({
                                    "path": dir_path,
                                    "parent": root,
                                    "type": "directory"
                                })
                            elif name in dir_name:  # Partial match
                                dir_path = os.path.join(root, dir_name)
                                results["partial_matches"].append({
                                    "path": dir_path,
                                    "parent": root,
                                    "type": "directory"
                                })
            except Exception as e:
                logger.warning(f"Error searching directory {dir_path}: {str(e)}")
                # Continue with other directories
        
        # Format results
        summary = {
            "found": len(results["exact_matches"]) > 0 or len(results["partial_matches"]) > 0,
            "exact_match_count": len(results["exact_matches"]),
            "partial_match_count": len(results["partial_matches"]),
            "search_locations": len(results["searched_directories"]),
            "results": results
        }
        
        return json.dumps(summary, indent=2)
        
    except Exception as e:
        logger.error(f"Exception in search_project_directories: {str(e)}", exc_info=True)
        return json.dumps({
            "error": f"Error searching for {name}: {str(e)}",
            "found": False
        })

@function_tool
async def change_directory(ctx: RunContextWrapper[AgentContext], path: str) -> str:
    """
    Change the current working directory.
    
    Args:
        path: The path to change to. Can be absolute or relative.
    
    Returns:
        A message indicating success or failure.
    """
    try:
        # Handle special cases
        if path == "~":
            path = os.path.expanduser("~")
        elif path.startswith("~/"):
            path = os.path.expanduser(path)
        elif not os.path.isabs(path):
            # Handle relative paths
            path = os.path.normpath(os.path.join(ctx.context.current_dir, path))
        
        if not os.path.exists(path):
            return f"Error: Path {path} does not exist."
        
        if not os.path.isdir(path):
            return f"Error: {path} is not a directory."
        
        # Update the context
        ctx.context.current_dir = path
        
        return f"Changed directory to: {path}"
    except Exception as e:
        return f"Error changing directory: {str(e)}"

# Code Analysis Tools
@function_tool
async def analyze_python_file(
    ctx: RunContextWrapper[AgentContext], 
    file_path: str
) -> str:
    """
    Analyze a Python file to find potential bugs and issues.
    
    Args:
        file_path: Path to the Python file to analyze
    
    Returns:
        A string containing the analysis results.
    """
    try:
        # Handle relative paths
        if not os.path.isabs(file_path):
            file_path = os.path.join(ctx.context.current_dir, file_path)
        
        # Check file existence and type
        if not os.path.exists(file_path):
            return f"Error: File {file_path} does not exist."
        
        if not file_path.endswith('.py'):
            return f"Error: {file_path} is not a Python file."
        
        # Read the file
        success, content = FileSystem.read_file(file_path)
        if not success:
            return f"Error: {content}"
        
        # Use the CodeAnalyzer utility
        analysis = CodeAnalyzer.analyze_python_code(content)
        
        # Track file access
        ctx.context.track_file_access(file_path)
        
        return json.dumps(analysis, indent=2)
    except Exception as e:
        return f"Error analyzing Python file: {str(e)}"

@function_tool
async def compare_files(
    ctx: RunContextWrapper[AgentContext],
    original_file: str,
    modified_file: Optional[str] = None,
    original_content: Optional[str] = None,
    modified_content: Optional[str] = None
) -> str:
    """
    Compare two files or text contents and display their differences.
    
    Args:
        original_file: Path to the original file (or identifier if using content)
        modified_file: Path to the modified file (optional)
        original_content: Original content string (optional, instead of file)
        modified_content: Modified content string (optional, instead of file)
        
    Returns:
        A unified diff of the two files or contents
    """
    try:
        # Case 1: Compare file contents
        if modified_file and not (original_content or modified_content):
            # Read files
            if not os.path.isabs(original_file):
                original_file = os.path.join(ctx.context.current_dir, original_file)
            if not os.path.isabs(modified_file):
                modified_file = os.path.join(ctx.context.current_dir, modified_file)
                
            success1, original = FileSystem.read_file(original_file)
            if not success1:
                return f"Error with original file: {original}"
                
            success2, modified = FileSystem.read_file(modified_file)
            if not success2:
                return f"Error with modified file: {modified}"
                
            # Track file access
            ctx.context.track_file_access(original_file)
            ctx.context.track_file_access(modified_file)
            
        # Case 2: Compare original file with provided content
        elif original_file and modified_content and not modified_file:
            if not os.path.isabs(original_file):
                original_file = os.path.join(ctx.context.current_dir, original_file)
                
            success, original = FileSystem.read_file(original_file)
            if not success:
                return f"Error with original file: {original}"
                
            modified = modified_content
            
            # Track file access
            ctx.context.track_file_access(original_file)
            
        # Case 3: Compare provided content strings
        elif original_content and modified_content:
            original = original_content
            modified = modified_content
            
        else:
            return "Error: Must provide either two files, or a file and modified content, or two content strings."
            
        # Create diff
        diff = CodeAnalyzer.create_diff(original, modified)
        
        if not diff:
            return "No differences found."
        
        return diff
            
    except Exception as e:
        return f"Error comparing files: {str(e)}"

@function_tool
async def write_to_file(
    ctx: RunContextWrapper[AgentContext], 
    file_path: str, 
    content: str,
    mode: str
) -> str:
    """
    Write content to a file.
    
    Args:
        file_path: Path to the file to write to
        content: Content to write to the file
        mode: 'w' for write (overwrite), 'a' for append
        
    Returns:
        A message indicating success or failure
    """
    try:
        # Handle relative paths
        if not os.path.isabs(file_path):
            file_path = os.path.join(ctx.context.current_dir, file_path)
        
        # Use the FileSystem utility with create_dirs=True (handle this inside function)
        success, message = FileSystem.write_file(file_path, content, create_dirs=True)
        
        if success:
            # Track file access and operation in context
            ctx.context.track_file_access(file_path)
            operation = "Updated" if mode == 'w' else "Appended to"
            ctx.context.set_operation(f"{operation} file: {os.path.basename(file_path)}")
        
        return message
    except Exception as e:
        return f"Error writing to file: {str(e)}"

@function_tool
async def execute_python(
    ctx: RunContextWrapper[AgentContext], 
    code: str,
    use_file: Optional[bool] = None,
    file_path: Optional[str] = None
) -> str:
    """
    Execute Python code and return the result.
    
    Args:
        code: The Python code to execute
        use_file: Whether to write the code to a file first (default False)
        file_path: Optional path to write the code to before executing
    
    Returns:
        The output of the executed code.
    """
    # Handle default inside function
    if use_file is None:
        use_file = False
        
    try:
        # Track command execution
        command_description = "Execute Python code"
        
        # Use the PythonRunner utility
        if use_file:
            if file_path:
                # Use the specified file path
                if not os.path.isabs(file_path):
                    file_path = os.path.join(ctx.context.current_dir, file_path)
                
                # Write the file
                success, message = FileSystem.write_file(file_path, code)
                if not success:
                    return f"Error writing file: {message}"
                
                # Track file access
                ctx.context.track_file_access(file_path)
                command_description = f"Execute Python script: {os.path.basename(file_path)}"
            else:
                # Use temp file
                command_description = "Execute Python script from temporary file"
            
            # Execute the code
            result = PythonRunner.run_code(code, timeout=30)
            
            output = ""
            if result.get("stdout"):
                output += f"STDOUT:\n{result['stdout']}\n\n"
            if result.get("stderr"):
                output += f"STDERR:\n{result['stderr']}\n\n"
            if "error" in result:
                output += f"ERROR:\n{result['error']}\n\n"
            
            # If there's no output, mention it
            if not output:
                output = "Code executed successfully with no output."
        else:
            # Execute code directly via PythonRunner
            result = PythonRunner.run_code(code, timeout=10)
            
            output = ""
            if result.get("stdout"):
                output += f"STDOUT:\n{result['stdout']}\n\n"
            if result.get("stderr"):
                output += f"STDERR:\n{result['stderr']}\n\n"
            if "error" in result:
                output += f"ERROR:\n{result['error']}\n\n"
        
        # Track command and operation in context
        ctx.context.track_command(command_description)
        ctx.context.set_operation(command_description)
        
        return output
    except Exception as e:
        return f"Error executing Python code: {str(e)}"

# Git Tools
@function_tool
async def git_status(
    ctx: RunContextWrapper[AgentContext],
    repo_path: Optional[str] = None
) -> str:
    """
    Get the status of a Git repository.
    
    Args:
        repo_path: Path to the Git repository (optional, uses current directory if not specified)
        
    Returns:
        Git status information
    """
    try:
        path = repo_path or ctx.context.current_dir
        
        # Check if it's a git repository
        is_repo, repo_root = GitManager.check_git_repo(path)
        
        if not is_repo:
            return json.dumps({"error": "Not a Git repository"})
        
        # Get status
        status = GitManager.get_git_status(repo_root)
        
        # Track operation
        ctx.context.set_operation("Checked Git status")
        
        return json.dumps(status, indent=2)
    except Exception as e:
        return json.dumps({"error": f"Error getting Git status: {str(e)}"})

@function_tool
async def git_commit(
    ctx: RunContextWrapper[AgentContext],
    message: str,
    repo_path: Optional[str] = None
) -> str:
    """
    Commit changes to a Git repository.
    
    Args:
        message: Commit message
        repo_path: Path to the Git repository (optional, uses current directory if not specified)
        
    Returns:
        Result of the commit operation
    """
    try:
        path = repo_path or ctx.context.current_dir
        
        # Check if it's a git repository
        is_repo, repo_root = GitManager.check_git_repo(path)
        
        if not is_repo:
            return json.dumps({"error": "Not a Git repository"})
        
        # Commit
        success, result = GitManager.git_commit(repo_root, message)
        
        # Track operation
        ctx.context.track_command(f"Git commit: {message}")
        ctx.context.set_operation("Created Git commit")
        
        if success:
            return json.dumps({"success": True, "message": result})
        else:
            return json.dumps({"error": result})
    except Exception as e:
        return json.dumps({"error": f"Error committing to Git: {str(e)}"})

# Context Management Tool
@function_tool
async def summarize_context(ctx: RunContextWrapper[AgentContext]) -> str:
    """
    Summarize the current conversation context to manage token usage.
    This is automatically called when the context reaches the token limit.
    
    Returns:
        A confirmation message after summarizing.
    """
    # This would normally use an LLM to summarize, but for simplicity
    # we'll just simulate this process
    
    # Reset token count after summarization
    ctx.context.reset_token_count()
    ctx.context.history_summary += f"[Context window reached {ctx.context.max_tokens} tokens and was summarized]"
    
    return "Context summarized successfully. The conversation will continue with the summarized context."

# Define agents for specific tasks
# 1. Code Generation Agent
code_generation_agent = Agent(
    name="Code Generator",
    instructions="""You are an expert Python code generator. Your task is to write clean, efficient, 
    and well-documented Python code based on user requirements. Follow these guidelines:
    
    1. Always include docstrings for functions and classes
    2. Follow PEP 8 style guidelines
    3. Include error handling where appropriate
    4. Write modular, reusable code
    5. Include comments for complex sections
    
    You must return your response as a structured output with:
    - The generated code
    - An explanation of what the code does
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - An optional suggested file path for the code""",
    model="o3-mini",
    output_type=CodeGenerationOutput,
    tools=[write_to_file, read_file, list_directory, compare_files]
)

# Create specialized variants using clone
data_science_code_generator = code_generation_agent.clone(
    name="Data Science Code Generator",
    instructions="""You are an expert Python code generator specialized in data science. 
    Your task is to write clean, efficient, and well-documented Python code for data science tasks.
    Focus on libraries like pandas, numpy, scikit-learn, matplotlib, and other data science tools.
    
    Follow these guidelines:
    1. Always include docstrings for functions and classes
    2. Follow PEP 8 style guidelines
    3. Include error handling where appropriate
    4. Write modular, reusable code
    5. Include visualizations where appropriate
    6. Always include data validation steps
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - The generated code
    - An explanation of what the code does
    - An optional suggested file path for the code"""
)

web_code_generator = code_generation_agent.clone(
    name="Web Development Code Generator",
    instructions="""You are an expert Python code generator specialized in web development.
    Your task is to write clean, efficient, and well-documented Python code for web applications.
    Focus on frameworks like Flask, Django, FastAPI, and related web technologies.
    
    Follow these guidelines:
    1. Always include docstrings for functions and classes
    2. Follow PEP 8 style guidelines
    3. Include error handling where appropriate
    4. Write modular, reusable code
    5. Include security best practices
    6. Consider performance and scalability
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - The generated code
    - An explanation of what the code does
    - An optional suggested file path for the code"""
)

# 2. Code Analysis Agent
code_analysis_agent = Agent(
    name="Code Analyzer",
    instructions="""You are an expert code analyzer. Your task is to review Python code, 
    identify issues, and suggest improvements. Focus on:
    
    1. Code quality and adherence to best practices
    2. Potential bugs and error cases
    3. Performance issues
    4. Security vulnerabilities
    5. Readability and maintainability
    
    Use the analysis tools to help identify issues in the code.
    
    You can use the compare_files tool to show differences between original code and your suggested improvements.
    This provides a visual diff that makes it easier for users to understand your proposed changes.
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - A list of identified issues
    - A list of suggested improvements
    - A summary of your analysis""",
    model="o3-mini",
    output_type=CodeAnalysisOutput,
    tools=[analyze_python_file, read_file, list_directory, search_files, compare_files]
)

# 3. Project Analysis Agent
project_analysis_agent = Agent(
    name="Project Analyzer",
    instructions="""You are an expert project analyzer. Your task is to review Python projects, 
    understand their structure, dependencies, and architecture. You should:
    
    1. Identify the key components and their relationships
    2. Analyze the project structure and organization
    3. Identify dependencies and their versions
    4. Look for patterns and anti-patterns
    5. Suggest improvements to the project structure
    
    When suggesting structural changes or improvements to files, you can use the compare_files tool
    to show the differences between the original and your suggested version.
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - Project structure information
    - A list of project dependencies
    - A list of recommendations for improvement
    - A summary of your analysis""",
    model="o3-mini",
    output_type=ProjectAnalysisOutput,
    tools=[list_directory, read_file, search_files, compare_files]
)

# Define dynamic instructions function for terminal agent
# Define dynamic instructions function for terminal agent
def terminal_agent_instructions(ctx: RunContextWrapper[AgentContext], agent: Agent[AgentContext]) -> str:
    """
    Dynamic instructions for terminal agent that incorporates context information.
    
    Args:
        ctx: Context wrapper containing AgentContext
        agent: The agent object
        
    Returns:
        Formatted instruction string
    """
    # Build accessed files and commands for context
    accessed_files = ", ".join(ctx.context.accessed_files[-5:]) if ctx.context.accessed_files else "None yet"
    executed_commands = ", ".join(ctx.context.executed_commands[-5:]) if ctx.context.executed_commands else "None yet"
    
    return f"""You are a Terminal Agent for macOS and Linux, an expert coding assistant designed to help with 
    Python development tasks. You can navigate the file system, read and analyze code, execute Python code, 
    and provide expert assistance with coding tasks.
    
    You have access to the following capabilities:
    
    1. File System Operations: 
        - List directories, read files, change directories
        - Enhanced file search capabilities:
         * search_files: Search for files in a directory
         * find_file: Find a specific file by name
         * find_in_parent_directories: Search in current and parent directories
         * search_project_directories: Comprehensive project-wide search

    2. Code Analysis: Analyze Python files for bugs and issues using pylint
    3. Project Analysis: Analyze the structure and organization of Python projects
    4. Code Generation and Execution: Write code to files and execute Python code
    5. Code Comparison and Diff: Compare files or content and display differences with side-by-side highlighting
    6. Context Management: Maintain conversation context even for long sessions
    
    When searching for files or directories:
    - If not found in the current directory, try parent directories
    - Use search_project_directories for a comprehensive search
    - For recursive searches, make sure to explicitly set recursive=True
    
    You can also delegate tasks to specialized agents when needed:
    - Code Generator: For writing Python code
    - Code Analyzer: For in-depth code analysis
    - Project Analyzer: For analyzing project structure
    - Data Science Code Generator: For generating data science code
    - Web Development Code Generator: For generating web application code
    
    Current working directory: {ctx.context.current_dir}
    Current session ID: {ctx.context.session_id}
    Last operation: {ctx.context.last_operation or "None"}
    Recently accessed files: {accessed_files}
    Recent commands: {executed_commands}
    
    When helping with coding tasks:
    1. Be specific and thorough in your explanations
    2. Provide code examples when relevant
    3. Suggest best practices and improvements
    4. Explain your reasoning
    5. Be aware of the limitations of the tools
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - Your response to the user query
    - A list of files accessed during processing
    - A list of commands executed during processing
    """
# Main Terminal Agent
terminal_agent = Agent[AgentContext](
    name="Terminal Agent",
    instructions=terminal_agent_instructions,
    model="o3-mini",
    output_type=TerminalAgentOutput,
    input_guardrails=[security_guardrail],  # Using our updated security guardrail
    tools=[
        list_directory,
        read_file,
        search_files,
        find_file,  # Basic file finder
        find_in_parent_directories,  # Find in parent directories
        search_project_directories,  # Comprehensive project search
        change_directory,
        analyze_python_file,
        compare_files,
        git_status,
        git_commit,
        write_to_file,
        execute_python,
        summarize_context,
        # Add specialized agents as tools
        code_generation_agent.as_tool(
            tool_name="generate_code",
            tool_description="Generate Python code based on a detailed description. Returns code and explanation."
        ),
        data_science_code_generator.as_tool(
            tool_name="generate_data_science_code",
            tool_description="Generate Python code for data science tasks (pandas, numpy, scikit-learn, matplotlib)."
        ),
        web_code_generator.as_tool(
            tool_name="generate_web_code",
            tool_description="Generate Python code for web applications (Flask, Django, FastAPI)."
        ),
        code_analysis_agent.as_tool(
            tool_name="analyze_code",
            tool_description="Analyze Python code for issues and improvements. Returns issues, improvements, and summary."
        ),
        project_analysis_agent.as_tool(
            tool_name="analyze_project",
            tool_description="Analyze a Python project's structure and dependencies. Returns structure, dependencies, and recommendations."
        )
    ],
    handoffs=[
        handoff(
            code_generation_agent, 
            tool_name_override="handoff_to_code_generator",
            tool_description_override="Hand off the conversation to the Code Generator agent for in-depth code writing."
        ),
        handoff(
            data_science_code_generator,
            tool_name_override="handoff_to_data_science_generator",
            tool_description_override="Hand off the conversation to the Data Science Code Generator for specialized data science coding."
        ),
        handoff(
            web_code_generator,
            tool_name_override="handoff_to_web_generator",
            tool_description_override="Hand off the conversation to the Web Development Code Generator for specialized web application coding."
        ),
        handoff(
            code_analysis_agent,
            tool_name_override="handoff_to_code_analyzer", 
            tool_description_override="Hand off the conversation to the Code Analyzer agent for comprehensive code analysis."
        ),
        handoff(
            project_analysis_agent,
            tool_name_override="handoff_to_project_analyzer",
            tool_description_override="Hand off the conversation to the Project Analyzer agent for detailed project structure analysis."
        )
    ]
)

# Function to run a query through the agent system
async def run_agent_query(
    query: str,
    context: AgentContext,
    stream_callback = None
) -> Dict[str, Any]:
    """
    Run a query through the agent system
    
    Args:
        query: The user's query string
        context: The agent context
        stream_callback: Optional callback function for streaming responses
        
    Returns:
        Dictionary with the agent's response
    """
    try:
        # Log the query
        logger.info(f"Processing agent query: {query}")
        
        # Validate context
        if not context or not hasattr(context, 'current_dir') or not context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return {
                "error": "Agent context is invalid or missing current directory information.",
                "response": "I couldn't process that request because of an issue with the agent context."
            }
            
        # Validate the current directory exists
        if not os.path.exists(context.current_dir):
            logger.error(f"Current directory in context doesn't exist: {context.current_dir}")
            return {
                "error": f"Current directory doesn't exist: {context.current_dir}",
                "response": "I couldn't process that request because the working directory doesn't exist."
            }
        
        # Set up run config
        run_config = RunConfig(
            model_settings=ModelSettings(),
            trace_metadata={
                "user_id": "terminator_user",
                "session_type": "streaming" if stream_callback else "standard",
            },
            trace_include_sensitive_data=True,
            workflow_name="Terminator Agent Session",
            group_id=context.session_id,
        )
        
        if stream_callback:
            # Run with streaming
            try:
                result = Runner.run_streamed(
                    starting_agent=terminal_agent,
                    input=query,
                    context=context,
                    run_config=run_config
                )
                
                # Process streaming events if callback provided
                async for event in result.stream_events():
                    if stream_callback and callable(stream_callback):
                        await stream_callback(event)
                        
                # Return the final result
                if hasattr(result, 'final_output'):
                    logger.info("Successfully completed streaming agent query")
                    return {
                        "response": result.final_output.response,
                        "files_accessed": result.final_output.files_accessed,
                        "commands_executed": result.final_output.commands_executed
                    }
                else:
                    logger.error("No response from streaming agent query")
                    return {
                        "error": "No response from agent",
                        "response": "I couldn't process that request. No response was generated."
                    }
            except Exception as stream_error:
                logger.error(f"Error in streaming agent query: {str(stream_error)}", exc_info=True)
                return {
                    "error": f"Streaming error: {str(stream_error)}",
                    "response": f"I encountered an error while processing your request: {str(stream_error)}"
                }
                    
        else:
            # Run without streaming
            try:
                result = await Runner.run(
                    starting_agent=terminal_agent,
                    input=query,
                    context=context,
                    run_config=run_config
                )
                
                # Return the result
                if hasattr(result, 'final_output'):
                    logger.info("Successfully completed agent query")
                    return {
                        "response": result.final_output.response,
                        "files_accessed": result.final_output.files_accessed,
                        "commands_executed": result.final_output.commands_executed
                    }
                else:
                    logger.error("No response from agent query")
                    return {
                        "error": "No response from agent",
                        "response": "I couldn't process that request. No response was generated."
                    }
            except Exception as run_error:
                logger.error(f"Error in agent query: {str(run_error)}", exc_info=True)
                return {
                    "error": f"Run error: {str(run_error)}",
                    "response": f"I encountered an error while processing your request: {str(run_error)}"
                }
                
    except InputGuardrailTripwireTriggered as guard_error:
        # Handle security guardrail trigger
        try:
            if hasattr(guard_error.guardrail_result, 'output_info'):
                security_check = guard_error.guardrail_result.output_info
                risk_type = getattr(security_check, 'risk_type', 'Potential security risk')
                reasoning = getattr(security_check, 'reasoning', 'Security guardrail triggered')
            else:
                risk_type = "Potential security risk"
                reasoning = "Your request triggered a security guardrail"
                
            logger.warning(f"Security guardrail triggered: {risk_type} - {reasoning}")
            return {
                "error": f"Security Warning: {risk_type}",
                "details": reasoning,
                "response": f"I can't process that request because it was flagged as a potential security risk: {risk_type}. {reasoning}"
            }
        except Exception as e:
            logger.error(f"Error processing security guardrail result: {str(e)}", exc_info=True)
            return {
                "error": "Security Warning: Request blocked by security guardrail",
                "response": "I can't process that request because it was flagged by my security systems."
            }
            
    except Exception as e:
        logger.error(f"Exception running agent query: {str(e)}", exc_info=True)
        return {
            "error": f"Error: {str(e)}",
            "response": f"I encountered an unexpected error while processing your request: {str(e)}"
        }

# Initialize the agent system
def initialize_agent_system():
    """Initialize the agent system with API keys and logging"""
    try:
        # Set up logging if not already configured
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
            
            # Also add a file handler for persistent logs
            file_handler = logging.FileHandler("terminator_agent.log")
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        
        # Check for API key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("OPENAI_API_KEY not set in environment variables")
            return False
            
        # Set the API key for both OpenAI and agents
        OpenAI.api_key = api_key
        set_default_openai_key(api_key)
        
        # Verify API key works (optional - might add API call overhead)
        try:
            # This is a minimal API call to verify the key works
            client = OpenAI()
            models = client.models.list()
            if not models:
                logger.warning("API key verification: No models returned, but API didn't error")
            else:
                logger.info(f"API key verification successful, found {len(models.data)} models")
        except Exception as e:
            logger.error(f"API key verification failed: {str(e)}")
            return False
        
        # Log successful initialization
        logger.info("Agent system initialized successfully")
        return True
        
    except Exception as e:
        logger.error(f"Error initializing agent system: {str(e)}", exc_info=True)
        return False
```

My question: Hi, testing!
2025-04-01 10:47:38,450 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: search.*file
2025-04-01 10:47:42,256 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 10:47:45,514 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 10:47:45,679 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 10:47:47,824 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 10:48:00,821 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
"""
TerminatorV1 Agents - Agent integration for the Terminator IDE
Provides OpenAI agent-based code assistance, analysis, and generation
"""

import os
import sys
import asyncio
import logging
import json
import time
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple
from dataclasses import dataclass
from pydantic import BaseModel, Field

# Import OpenAI and agent framework
from openai import OpenAI
from agents import (
    Agent,
    ModelSettings, 
    Runner, 
    function_tool, 
    RunContextWrapper, 
    trace, 
    handoff,
    set_default_openai_key,
    input_guardrail,
    output_guardrail,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    OutputGuardrailTripwireTriggered,
    TResponseInputItem,
    ItemHelpers,
    RunConfig,
    MaxTurnsExceeded,
    ModelBehaviorError,
    AgentsException
)

# Import tools for the agents to use
from TerminatorV1_tools import FileSystem, CodeAnalyzer, GitManager, PythonRunner

# Set up logging
logger = logging.getLogger("terminator_agents")
handler = logging.StreamHandler()
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)
logger.setLevel(logging.INFO)

# Set API key from environment
OpenAI.api_key = os.getenv("OPENAI_API_KEY")
set_default_openai_key(os.getenv("OPENAI_API_KEY"))

# Define security guardrail for malicious inputs
class SecurityCheckOutput(BaseModel):
    """Output model for security check guardrail"""
    is_malicious: bool = Field(..., description="Whether the input appears to be malicious")
    risk_type: Optional[str] = Field(None, description="Type of security risk identified")
    reasoning: str = Field(..., description="Reasoning behind the security assessment")

# Define the context for the agent
class AgentContext(BaseModel):
    """Context object to pass between agents and tools"""
    current_dir: str
    history_summary: str = ""
    token_count: int = 0
    max_tokens: int = 150000
    accessed_files: List[str] = Field(default_factory=list)
    executed_commands: List[str] = Field(default_factory=list)
    last_operation: Optional[str] = None
    session_id: str = Field(default_factory=lambda: f"session_{int(time.time())}")
    
    def update_token_count(self, new_tokens: int) -> bool:
        """
        Update token count and check if summarization is needed
        
        Args:
            new_tokens: Number of tokens to add to the count
            
        Returns:
            bool: True if summarization is needed, False otherwise
        """
        self.token_count += new_tokens
        return self.token_count >= self.max_tokens
    
    def reset_token_count(self) -> None:
        """Reset token count after summarization"""
        self.token_count = 0
        
    def track_file_access(self, file_path: str) -> None:
        """
        Track file access in the context
        
        Args:
            file_path: Path of the accessed file
        """
        if file_path not in self.accessed_files:
            self.accessed_files.append(file_path)
            
    def track_command(self, command: str) -> None:
        """
        Track command execution in the context
        
        Args:
            command: The executed command
        """
        self.executed_commands.append(command)
        
    def set_operation(self, operation: str) -> None:
        """
        Set the last operation performed
        
        Args:
            operation: Description of the operation
        """
        self.last_operation = operation

# Create a security check agent
# Create a security check agent with more permissive file system rules
security_check_agent = Agent(
    name="Security Guardrail",
    instructions="""You are a security guardrail for a Python development environment. Your job is to analyze user input and 
    determine if it might be trying to:
    
    1. Execute clearly malicious code with harmful intent
    2. Access highly sensitive system information (like /etc/passwd, SSH keys, etc.)
    3. Perform destructive operations with clear intent to damage (rm -rf /, format drives, etc.)
    4. Use the system for hacking, attacks, or other explicitly harmful purposes
    5. Execute code injections, XSS, or other attack vectors against external systems
    
    IMPORTANT: The following actions are ALLOWED and should NOT be flagged:
    - Normal directory traversal and searching (cd, ls, find, etc.)
    - Reading code files and documentation
    - File operations within the project directory structure 
    - Searching in subdirectories, even recursively
    - Git operations
    - Running or debugging Python code that appears legitimate
    
    Only flag requests that have clear malicious intent, not legitimate development activities.
    Directory traversal within a development project is entirely legitimate and should be ALLOWED.
    
    Return a structured assessment with your conclusion.""",
    model="o3-mini",
    output_type=SecurityCheckOutput
)

@input_guardrail
async def security_guardrail(
    ctx: RunContextWrapper[AgentContext], 
    agent: Agent, 
    input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    """
    Security guardrail to prevent malicious inputs, with exceptions for normal file operations
    
    Args:
        ctx: Context wrapper
        agent: The agent being protected
        input: The user input
        
    Returns:
        GuardrailFunctionOutput with security assessment
    """
    # Check if the input is asking for a basic file system operation that should be allowed
    input_str = input if isinstance(input, str) else ItemHelpers.stringify_input(input)
    
    # List of patterns that should be explicitly allowed
    safe_patterns = [
        r'search.*file',
        r'search.*directory',
        r'find.*file',
        r'look for.*file',
        r'search in subdirectory',
        r'search recursively',
        r'list.*directory',
        r'list.*file',
        r'explore.*directory',
        r'navigate.*directory',
        r'change directory',
        r'cd ',
        r'ls ',
        r'dir ',
        r'find ',
        r'open.*file',
        r'read.*file'
    ]
    
    # Check if input matches any safe pattern
    for pattern in safe_patterns:
        if re.search(pattern, input_str, re.IGNORECASE):
            logger.info(f"Security guardrail: Allowing file operation matching pattern: {pattern}")
            return GuardrailFunctionOutput(
                output_info=SecurityCheckOutput(
                    is_malicious=False,
                    risk_type=None,
                    reasoning="This is a legitimate file system operation for development purposes."
                ),
                tripwire_triggered=False
            )

    # Run the security check agent on the input
    result = await Runner.run(security_check_agent, input, context=ctx.context)
    security_check = result.final_output
    
    # Double-check for file operation false positives
    if security_check.is_malicious and "directory" in input_str.lower():
        # Reduce false positives for directory operations
        logger.info("Security guardrail: Overriding false positive for directory operation")
        return GuardrailFunctionOutput(
            output_info=SecurityCheckOutput(
                is_malicious=False,
                risk_type=None,
                reasoning="File system operation appears to be legitimate."
            ),
            tripwire_triggered=False
        )
    
    # Return the guardrail result
    return GuardrailFunctionOutput(
        output_info=security_check,
        tripwire_triggered=security_check.is_malicious,
    )

# Define Pydantic models for structured agent outputs
class CodeGenerationOutput(BaseModel):
    code: str = Field(..., description="The generated Python code")
    explanation: str = Field(..., description="Explanation of what the code does")
    file_path: Optional[str] = Field(None, description="Suggested file path for the code")

class CodeAnalysisOutput(BaseModel):
    issues: List[Dict[str, Any]] = Field(default_factory=list, description="List of identified issues")
    improvements: List[str] = Field(default_factory=list, description="Suggested improvements")
    summary: str = Field(..., description="Summary of code analysis")

class ProjectAnalysisOutput(BaseModel):
    structure: Dict[str, Any] = Field(..., description="Project structure information")
    dependencies: List[str] = Field(default_factory=list, description="Project dependencies")
    recommendations: List[str] = Field(default_factory=list, description="Recommendations for improvement")
    summary: str = Field(..., description="Summary of project analysis")

class TerminalAgentOutput(BaseModel):
    response: str = Field(..., description="The agent's response to the user query")
    files_accessed: List[str] = Field(default_factory=list, description="Files accessed during processing")
    commands_executed: List[str] = Field(default_factory=list, description="Commands executed during processing")

# File System Tools
@function_tool
async def list_directory(ctx: RunContextWrapper[AgentContext], path: Optional[str] = None) -> str:
    """
    List the contents of a directory.
    
    Args:
        path: Optional path to list. If not provided, uses current directory.
    
    Returns:
        A string representation of the directory contents.
    """
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return "Error: Agent context is invalid or missing current directory information."
        
        # Resolve target path
        target_path = path if path is not None else ctx.context.current_dir
        
        # Log the target path for debugging
        logger.info(f"Attempting to list directory: {target_path}")
        
        # Handle relative paths
        if not os.path.isabs(target_path):
            target_path = os.path.join(ctx.context.current_dir, target_path)
            logger.info(f"Resolved to absolute path: {target_path}")
        
        # Check if directory exists
        if not os.path.exists(target_path):
            logger.error(f"Directory not found: {target_path}")
            return f"Error: Directory not found: {target_path}"
            
        if not os.path.isdir(target_path):
            logger.error(f"Path is not a directory: {target_path}")
            return f"Error: Path is not a directory: {target_path}"
        
        # Use the FileSystem utility with max_depth=2 (handled inside function)
        structure = FileSystem.get_directory_structure(target_path, max_depth=2)
        
        # Check if we got an error
        if isinstance(structure, dict) and "error" in structure:
            logger.error(f"Error getting directory structure: {structure['error']}")
            return f"Error listing directory: {structure['error']}"
        
        # Log success
        logger.info(f"Successfully listed directory: {target_path}")
        
        # Track operation
        ctx.context.set_operation(f"Listed directory: {os.path.basename(target_path) or target_path}")
        
        return json.dumps(structure, indent=2)
    except Exception as e:
        logger.error(f"Exception listing directory {path}: {str(e)}", exc_info=True)
        return f"Error listing directory: {str(e)}"

@function_tool
async def find_file(
    ctx: RunContextWrapper[AgentContext], 
    filename: str,
    start_path: Optional[str] = None,
    recursive: Optional[bool] = None
) -> str:
    """
    Safely find a file by name in the filesystem
    
    Args:
        filename: Name of the file to find
        start_path: Directory to start searching from (uses current directory if not specified)
        recursive: Whether to search recursively in subdirectories (default True)
        
    Returns:
        Path to the file if found, or error message
    """
    if recursive is None:
        recursive = True
        
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return json.dumps({
                "error": "Agent context is invalid or missing current directory information."
            })
        
        # Get the starting search path
        search_path = start_path or ctx.context.current_dir
        
        # Handle special paths
        if search_path == "~" or search_path.startswith("~/"):
            search_path = os.path.expanduser(search_path)
        
        # Handle relative paths
        if not os.path.isabs(search_path):
            search_path = os.path.join(ctx.context.current_dir, search_path)
        
        logger.info(f"Searching for file: '{filename}', starting from: '{search_path}', recursive: {recursive}")
        
        # Check if the starting path exists
        if not os.path.exists(search_path):
            logger.error(f"Search path not found: {search_path}")
            return json.dumps({
                "error": f"Search path not found: {search_path}",
                "found": False
            })
            
        if not os.path.isdir(search_path):
            logger.error(f"Search path is not a directory: {search_path}")
            return json.dumps({
                "error": f"Search path is not a directory: {search_path}",
                "found": False
            })
        
        found_files = []
        
        # Function to recursively walk directories and find files
        def find_in_directory(current_path, max_depth=30, current_depth=0):
            if current_depth > max_depth:
                return
            
            try:
                # List all items in the current directory
                items = os.listdir(current_path)
                
                for item in items:
                    item_path = os.path.join(current_path, item)
                    
                    # Check if it's a file that matches
                    if os.path.isfile(item_path) and item == filename:
                        found_files.append(item_path)
                        logger.info(f"Found matching file: {item_path}")
                    
                    # If it's a directory and recursive search is enabled, search it too
                    elif os.path.isdir(item_path) and recursive:
                        find_in_directory(item_path, max_depth, current_depth + 1)
            
            except Exception as e:
                logger.warning(f"Error accessing directory {current_path}: {str(e)}")
                # Continue the search in other directories
        
        # Start the search
        find_in_directory(search_path)
        
        # Return the results
        if not found_files:
            return json.dumps({
                "found": False,
                "message": f"File '{filename}' not found starting from '{search_path}'"
            })
        
        return json.dumps({
            "found": True,
            "file_count": len(found_files),
            "files": found_files,
            "message": f"Found {len(found_files)} matching files"
        })
        
    except Exception as e:
        logger.error(f"Exception in find_file: {str(e)}", exc_info=True)
        return json.dumps({
            "error": f"Error searching for file: {str(e)}",
            "found": False
        })

@function_tool
async def read_file(ctx: RunContextWrapper[AgentContext], file_path: str) -> str:
    """
    Read and return the contents of a file.
    
    Args:
        file_path: Path to the file to read. Can be absolute or relative to current directory.
    
    Returns:
        The contents of the file as a string.
    """
    try:
        # Log the original file path for debugging
        logger.info(f"Attempting to read file: {file_path}")
        logger.info(f"Current directory: {ctx.context.current_dir}")
        
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return "Error: Agent context is invalid or missing current directory information."
        
        # Handle relative paths
        if not os.path.isabs(file_path):
            file_path = os.path.join(ctx.context.current_dir, file_path)
        
        # Log the resolved file path
        logger.info(f"Resolved file path: {file_path}")
        
        # Check if file exists before trying to read it
        if not os.path.exists(file_path):
            logger.error(f"File not found: {file_path}")
            return f"Error: File not found: {file_path}"
            
        if not os.path.isfile(file_path):
            logger.error(f"Path is not a file: {file_path}")
            return f"Error: Path is not a file: {file_path}"
        
        # Use the FileSystem utility
        success, content = FileSystem.read_file(file_path)
        
        if not success:
            logger.error(f"Failed to read file: {content}")
            return f"Error reading file: {content}"
        
        # Log success
        logger.info(f"Successfully read file: {file_path}")
        
        # Update token count (rough estimate)
        tokens_added = len(content.split())
        needs_summary = ctx.context.update_token_count(tokens_added)
        
        # Track file access in context
        ctx.context.track_file_access(file_path)
        ctx.context.set_operation(f"Read file: {os.path.basename(file_path)}")
        
        return content
    except Exception as e:
        logger.error(f"Exception reading file {file_path}: {str(e)}", exc_info=True)
        return f"Error reading file: {str(e)}"

@function_tool
async def search_files(
    ctx: RunContextWrapper[AgentContext], 
    search_term: str, 
    path: Optional[str] = None,
    file_pattern: Optional[str] = None,
    recursive: Optional[bool] = None,
    case_sensitive: Optional[bool] = None
) -> str:
    """
    Enhanced file search with improved robustness for finding files across directories
    
    Args:
        search_term: Term to search for (filename or content)
        path: Directory to search in (uses current directory if not specified)
        file_pattern: File pattern to match (e.g., "*.py")
        recursive: Whether to search recursively in subdirectories (default True)
        case_sensitive: Whether the search is case sensitive (default False)
        
    Returns:
        Structured search results
    """
    # Set default values for optional parameters inside the function body
    if recursive is None:
        recursive = True
    if case_sensitive is None:
        case_sensitive = False
        
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return json.dumps({
                "error": "Agent context is invalid or missing current directory information."
            })
        
        # Validate search term
        if not search_term or not isinstance(search_term, str):
            logger.error(f"Invalid search term: {search_term}")
            return json.dumps({
                "error": "Search term is required and must be a string."
            })
            
        # Get the target path, expanding user directory
        target_path = path or ctx.context.current_dir
        
        # Handle special paths
        if target_path == "~" or target_path.startswith("~/"):
            target_path = os.path.expanduser(target_path)
        
        # Handle relative paths
        if not os.path.isabs(target_path):
            target_path = os.path.join(ctx.context.current_dir, target_path)
        
        logger.info(f"Search request - term: '{search_term}', path: '{target_path}', pattern: '{file_pattern}', recursive: {recursive}")
        
        # Check if directory exists
        if not os.path.exists(target_path):
            logger.error(f"Directory not found: {target_path}")
            return json.dumps({
                "error": f"Directory not found: {target_path}"
            })
            
        if not os.path.isdir(target_path):
            logger.error(f"Path is not a directory: {target_path}")
            return json.dumps({
                "error": f"Path is not a directory: {target_path}"
            })
        
        results = {
            "filename_matches": [],
            "content_matches": []
        }
        
        # Track operation in context
        ctx.context.set_operation(f"Searching for '{search_term}' in {target_path}")
        
        # Function to recursively walk directories and find files
        def walk_directory(current_path, max_depth=20, current_depth=0):
            if current_depth > max_depth:
                return
            
            try:
                # List all items in the current directory
                items = os.listdir(current_path)
                
                for item in items:
                    item_path = os.path.join(current_path, item)
                    
                    # Check if it's a file
                    if os.path.isfile(item_path):
                        # Check if file matches search criteria
                        if file_pattern:
                            import fnmatch
                            if not fnmatch.fnmatch(item, file_pattern):
                                continue
                        
                        # Check for filename match
                        filename_match = False
                        if case_sensitive:
                            if search_term == item:
                                filename_match = True
                                match_type = "exact match"
                            elif search_term in item:
                                filename_match = True
                                match_type = "partial match"
                        else:
                            if search_term.lower() == item.lower():
                                filename_match = True
                                match_type = "exact match"
                            elif search_term.lower() in item.lower():
                                filename_match = True
                                match_type = "partial match"
                        
                        if filename_match:
                            results["filename_matches"].append({
                                "path": item_path,
                                "type": match_type
                            })
                            logger.info(f"Found filename match: {item_path}")
                        
                        # Check for content match if there's a search term
                        if search_term:
                            try:
                                success, content = FileSystem.read_file(item_path, max_size_mb=5)
                                if success:
                                    content_matches = []
                                    for i, line in enumerate(content.splitlines()):
                                        if (case_sensitive and search_term in line) or \
                                            (not case_sensitive and search_term.lower() in line.lower()):
                                            content_matches.append({
                                                "line": i + 1,
                                                "content": line.strip()
                                            })
                                    
                                    if content_matches:
                                        results["content_matches"].append({
                                            "file": item_path,
                                            "matches": content_matches[:5]  # Limit to 5 matches per file
                                        })
                                        logger.info(f"Found content matches in: {item_path}")
                                        ctx.context.track_file_access(item_path)
                            except Exception as e:
                                logger.warning(f"Error reading file {item_path}: {str(e)}")
                    
                    # If it's a directory and recursive search is enabled, search it too
                    elif os.path.isdir(item_path) and recursive:
                        walk_directory(item_path, max_depth, current_depth + 1)
            
            except Exception as e:
                logger.warning(f"Error accessing directory {current_path}: {str(e)}")
        
        # Start the directory walk from the target path
        walk_directory(target_path)
        
        # Clean up and format the results
        if not results["filename_matches"] and not results["content_matches"]:
            return json.dumps({
                "found": False,
                "message": f"No matches found for '{search_term}' in {target_path}"
            })
        
        return json.dumps({
            "found": True,
            "filename_matches": results["filename_matches"],
            "content_matches": results["content_matches"],
            "message": f"Found {len(results['filename_matches'])} files matching '{search_term}' and {len(results['content_matches'])} files with matching content"
        })
        
    except Exception as e:
        logger.error(f"Exception in search_files: {str(e)}", exc_info=True)
        return json.dumps({"error": f"Error searching files: {str(e)}"})

@function_tool
async def find_in_parent_directories(
    ctx: RunContextWrapper[AgentContext], 
    name: str,
    levels_up: Optional[int] = None,
    is_directory: Optional[bool] = None
) -> str:
    """
    Search for a file or directory in the current directory and parent directories
    
    Args:
        name: Name of the file or directory to find
        levels_up: Maximum number of parent directories to check (None for no limit)
        is_directory: Whether to look for a directory (True) or file (False). None means either.
        
    Returns:
        Path information if found, or error message
    """
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return json.dumps({
                "error": "Agent context is invalid or missing current directory information.",
                "found": False
            })
        
        # Start with current directory
        current_path = ctx.context.current_dir
        logger.info(f"Starting search for {'directory' if is_directory else 'file'} '{name}' from {current_path}")
        
        # Keep track of searched directories
        searched_dirs = []
        level = 0
        
        # Check current and parent directories
        while current_path and (levels_up is None or level <= levels_up):
            searched_dirs.append(current_path)
            
            # First check for direct match
            target_path = os.path.join(current_path, name)
            logger.info(f"Checking {target_path}")
            
            if os.path.exists(target_path):
                # Check if it's the right type (file or directory)
                if is_directory is None:
                    # Either type is fine
                    logger.info(f"Found match: {target_path}")
                    return json.dumps({
                        "found": True,
                        "path": target_path,
                        "type": "directory" if os.path.isdir(target_path) else "file",
                        "parent_dir": current_path,
                        "levels_up": level
                    })
                elif is_directory and os.path.isdir(target_path):
                    # Found matching directory
                    logger.info(f"Found directory: {target_path}")
                    return json.dumps({
                        "found": True,
                        "path": target_path,
                        "type": "directory",
                        "parent_dir": current_path,
                        "levels_up": level
                    })
                elif not is_directory and os.path.isfile(target_path):
                    # Found matching file
                    logger.info(f"Found file: {target_path}")
                    return json.dumps({
                        "found": True,
                        "path": target_path,
                        "type": "file",
                        "parent_dir": current_path,
                        "levels_up": level
                    })
            
            # Next, search inside directory for sub-matches if we're looking for files
            if is_directory is not True:  # If we're looking for files or either type
                try:
                    # List the directory and see if we can find a partial match
                    for item in os.listdir(current_path):
                        item_path = os.path.join(current_path, item)
                        if os.path.isfile(item_path) and name in item:
                            logger.info(f"Found partial file match: {item_path}")
                            return json.dumps({
                                "found": True,
                                "path": item_path,
                                "type": "file",
                                "parent_dir": current_path,
                                "levels_up": level,
                                "note": f"Partial match for '{name}'"
                            })
                except Exception as e:
                    logger.warning(f"Error listing directory {current_path}: {str(e)}")
            
            # Move up to parent directory
            parent_path = os.path.dirname(current_path)
            if parent_path == current_path:
                # We've reached the root
                break
                
            current_path = parent_path
            level += 1
        
        # If we get here, we didn't find it
        return json.dumps({
            "found": False,
            "searched_directories": searched_dirs,
            "message": f"Could not find {'directory' if is_directory else 'file'} '{name}' in current or parent directories"
        })
        
    except Exception as e:
        logger.error(f"Exception in find_in_parent_directories: {str(e)}", exc_info=True)
        return json.dumps({
            "error": f"Error searching for {name}: {str(e)}",
            "found": False
        })

@function_tool
async def search_project_directories(
    ctx: RunContextWrapper[AgentContext], 
    name: str,
    directory_level: Optional[str] = None,
    search_type: Optional[str] = None
) -> str:
    """
    Perform a comprehensive search for files or directories in the project
    
    Args:
        name: Name of the file or directory to find
        directory_level: Where to search ('current', 'parent', 'parent_parent', 'all' or 'root')
        search_type: What to search for ('file', 'directory', or 'both')
        
    Returns:
        JSON result with search findings
    """
    try:
        # Validate context
        if not ctx.context or not ctx.context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return json.dumps({
                "error": "Agent context is invalid or missing current directory information."
            })
        
        # Set default values
        if directory_level is None:
            directory_level = "all"
        
        if search_type is None:
            search_type = "both"
            
        # Determine what kind of object we're looking for
        is_directory = None
        if search_type == "file":
            is_directory = False
        elif search_type == "directory":
            is_directory = True
        # else leave as None for "both"
        
        # Determine starting directory
        current_dir = ctx.context.current_dir
        parent_dir = os.path.dirname(current_dir)
        parent_parent_dir = os.path.dirname(parent_dir)
        
        # Find the project root (usually the Git root if available)
        root_dir = None
        try:
            is_repo, repo_root = GitManager.check_git_repo(current_dir)
            if is_repo:
                root_dir = repo_root
        except Exception:
            # If we can't determine Git root, use 3 levels up as estimate
            temp = current_dir
            for _ in range(3):
                temp = os.path.dirname(temp)
                if temp == os.path.dirname(temp):  # We've hit the filesystem root
                    break
            root_dir = temp
        
        # Determine which directories to search
        dirs_to_search = []
        
        if directory_level == "current":
            dirs_to_search = [current_dir]
        elif directory_level == "parent":
            dirs_to_search = [parent_dir]
        elif directory_level == "parent_parent":
            dirs_to_search = [parent_parent_dir]
        elif directory_level == "root" and root_dir:
            dirs_to_search = [root_dir]
        else:  # "all" or unrecognized value
            # Build a path from root to current to search all relevant directories
            dirs_to_search = []
            temp = current_dir
            while temp and (not root_dir or os.path.commonpath([temp, root_dir]) == root_dir):
                dirs_to_search.append(temp)
                parent = os.path.dirname(temp)
                if parent == temp:  # We've hit the filesystem root
                    break
                temp = parent
            # Add project root if available and not already included
            if root_dir and root_dir not in dirs_to_search:
                dirs_to_search.append(root_dir)
        
        # Log search parameters
        logger.info(f"Searching for {search_type} '{name}' in: {dirs_to_search}")
        
        results = {
            "exact_matches": [],
            "partial_matches": [],
            "searched_directories": dirs_to_search
        }
        
        # Search each directory
        for dir_path in dirs_to_search:
            if not os.path.exists(dir_path) or not os.path.isdir(dir_path):
                continue
                
            # First check for direct matches in this directory
            target_path = os.path.join(dir_path, name)
            if os.path.exists(target_path):
                # Check if it's the right type
                if is_directory is None or (is_directory and os.path.isdir(target_path)) or (not is_directory and os.path.isfile(target_path)):
                    results["exact_matches"].append({
                        "path": target_path,
                        "parent": dir_path,
                        "type": "directory" if os.path.isdir(target_path) else "file"
                    })
            
            # Then do a more comprehensive search inside this directory
            try:
                # Walk through the directory
                for root, dirs, files in os.walk(dir_path):
                    # Check files if we're looking for files or both
                    if is_directory is not True:  # False or None
                        for file in files:
                            if name == file:  # Exact match
                                file_path = os.path.join(root, file)
                                results["exact_matches"].append({
                                    "path": file_path,
                                    "parent": root,
                                    "type": "file"
                                })
                            elif name in file:  # Partial match
                                file_path = os.path.join(root, file)
                                results["partial_matches"].append({
                                    "path": file_path,
                                    "parent": root,
                                    "type": "file"
                                })
                    
                    # Check directories if we're looking for directories or both
                    if is_directory is not False:  # True or None
                        for dir_name in dirs:
                            if name == dir_name:  # Exact match
                                dir_path = os.path.join(root, dir_name)
                                results["exact_matches"].append({
                                    "path": dir_path,
                                    "parent": root,
                                    "type": "directory"
                                })
                            elif name in dir_name:  # Partial match
                                dir_path = os.path.join(root, dir_name)
                                results["partial_matches"].append({
                                    "path": dir_path,
                                    "parent": root,
                                    "type": "directory"
                                })
            except Exception as e:
                logger.warning(f"Error searching directory {dir_path}: {str(e)}")
                # Continue with other directories
        
        # Format results
        summary = {
            "found": len(results["exact_matches"]) > 0 or len(results["partial_matches"]) > 0,
            "exact_match_count": len(results["exact_matches"]),
            "partial_match_count": len(results["partial_matches"]),
            "search_locations": len(results["searched_directories"]),
            "results": results
        }
        
        return json.dumps(summary, indent=2)
        
    except Exception as e:
        logger.error(f"Exception in search_project_directories: {str(e)}", exc_info=True)
        return json.dumps({
            "error": f"Error searching for {name}: {str(e)}",
            "found": False
        })

@function_tool
async def change_directory(ctx: RunContextWrapper[AgentContext], path: str) -> str:
    """
    Change the current working directory.
    
    Args:
        path: The path to change to. Can be absolute or relative.
    
    Returns:
        A message indicating success or failure.
    """
    try:
        # Handle special cases
        if path == "~":
            path = os.path.expanduser("~")
        elif path.startswith("~/"):
            path = os.path.expanduser(path)
        elif not os.path.isabs(path):
            # Handle relative paths
            path = os.path.normpath(os.path.join(ctx.context.current_dir, path))
        
        if not os.path.exists(path):
            return f"Error: Path {path} does not exist."
        
        if not os.path.isdir(path):
            return f"Error: {path} is not a directory."
        
        # Update the context
        ctx.context.current_dir = path
        
        return f"Changed directory to: {path}"
    except Exception as e:
        return f"Error changing directory: {str(e)}"

# Code Analysis Tools
@function_tool
async def analyze_python_file(
    ctx: RunContextWrapper[AgentContext], 
    file_path: str
) -> str:
    """
    Analyze a Python file to find potential bugs and issues.
    
    Args:
        file_path: Path to the Python file to analyze
    
    Returns:
        A string containing the analysis results.
    """
    try:
        # Handle relative paths
        if not os.path.isabs(file_path):
            file_path = os.path.join(ctx.context.current_dir, file_path)
        
        # Check file existence and type
        if not os.path.exists(file_path):
            return f"Error: File {file_path} does not exist."
        
        if not file_path.endswith('.py'):
            return f"Error: {file_path} is not a Python file."
        
        # Read the file
        success, content = FileSystem.read_file(file_path)
        if not success:
            return f"Error: {content}"
        
        # Use the CodeAnalyzer utility
        analysis = CodeAnalyzer.analyze_python_code(content)
        
        # Track file access
        ctx.context.track_file_access(file_path)
        
        return json.dumps(analysis, indent=2)
    except Exception as e:
        return f"Error analyzing Python file: {str(e)}"

@function_tool
async def compare_files(
    ctx: RunContextWrapper[AgentContext],
    original_file: str,
    modified_file: Optional[str] = None,
    original_content: Optional[str] = None,
    modified_content: Optional[str] = None
) -> str:
    """
    Compare two files or text contents and display their differences.
    
    Args:
        original_file: Path to the original file (or identifier if using content)
        modified_file: Path to the modified file (optional)
        original_content: Original content string (optional, instead of file)
        modified_content: Modified content string (optional, instead of file)
        
    Returns:
        A unified diff of the two files or contents
    """
    try:
        # Case 1: Compare file contents
        if modified_file and not (original_content or modified_content):
            # Read files
            if not os.path.isabs(original_file):
                original_file = os.path.join(ctx.context.current_dir, original_file)
            if not os.path.isabs(modified_file):
                modified_file = os.path.join(ctx.context.current_dir, modified_file)
                
            success1, original = FileSystem.read_file(original_file)
            if not success1:
                return f"Error with original file: {original}"
                
            success2, modified = FileSystem.read_file(modified_file)
            if not success2:
                return f"Error with modified file: {modified}"
                
            # Track file access
            ctx.context.track_file_access(original_file)
            ctx.context.track_file_access(modified_file)
            
        # Case 2: Compare original file with provided content
        elif original_file and modified_content and not modified_file:
            if not os.path.isabs(original_file):
                original_file = os.path.join(ctx.context.current_dir, original_file)
                
            success, original = FileSystem.read_file(original_file)
            if not success:
                return f"Error with original file: {original}"
                
            modified = modified_content
            
            # Track file access
            ctx.context.track_file_access(original_file)
            
        # Case 3: Compare provided content strings
        elif original_content and modified_content:
            original = original_content
            modified = modified_content
            
        else:
            return "Error: Must provide either two files, or a file and modified content, or two content strings."
            
        # Create diff
        diff = CodeAnalyzer.create_diff(original, modified)
        
        if not diff:
            return "No differences found."
        
        return diff
            
    except Exception as e:
        return f"Error comparing files: {str(e)}"

@function_tool
async def write_to_file(
    ctx: RunContextWrapper[AgentContext], 
    file_path: str, 
    content: str,
    mode: str
) -> str:
    """
    Write content to a file.
    
    Args:
        file_path: Path to the file to write to
        content: Content to write to the file
        mode: 'w' for write (overwrite), 'a' for append
        
    Returns:
        A message indicating success or failure
    """
    try:
        # Handle relative paths
        if not os.path.isabs(file_path):
            file_path = os.path.join(ctx.context.current_dir, file_path)
        
        # Use the FileSystem utility with create_dirs=True (handle this inside function)
        success, message = FileSystem.write_file(file_path, content, create_dirs=True)
        
        if success:
            # Track file access and operation in context
            ctx.context.track_file_access(file_path)
            operation = "Updated" if mode == 'w' else "Appended to"
            ctx.context.set_operation(f"{operation} file: {os.path.basename(file_path)}")
        
        return message
    except Exception as e:
        return f"Error writing to file: {str(e)}"

@function_tool
async def execute_python(
    ctx: RunContextWrapper[AgentContext], 
    code: str,
    use_file: Optional[bool] = None,
    file_path: Optional[str] = None
) -> str:
    """
    Execute Python code and return the result.
    
    Args:
        code: The Python code to execute
        use_file: Whether to write the code to a file first (default False)
        file_path: Optional path to write the code to before executing
    
    Returns:
        The output of the executed code.
    """
    # Handle default inside function
    if use_file is None:
        use_file = False
        
    try:
        # Track command execution
        command_description = "Execute Python code"
        
        # Use the PythonRunner utility
        if use_file:
            if file_path:
                # Use the specified file path
                if not os.path.isabs(file_path):
                    file_path = os.path.join(ctx.context.current_dir, file_path)
                
                # Write the file
                success, message = FileSystem.write_file(file_path, code)
                if not success:
                    return f"Error writing file: {message}"
                
                # Track file access
                ctx.context.track_file_access(file_path)
                command_description = f"Execute Python script: {os.path.basename(file_path)}"
            else:
                # Use temp file
                command_description = "Execute Python script from temporary file"
            
            # Execute the code
            result = PythonRunner.run_code(code, timeout=30)
            
            output = ""
            if result.get("stdout"):
                output += f"STDOUT:\n{result['stdout']}\n\n"
            if result.get("stderr"):
                output += f"STDERR:\n{result['stderr']}\n\n"
            if "error" in result:
                output += f"ERROR:\n{result['error']}\n\n"
            
            # If there's no output, mention it
            if not output:
                output = "Code executed successfully with no output."
        else:
            # Execute code directly via PythonRunner
            result = PythonRunner.run_code(code, timeout=10)
            
            output = ""
            if result.get("stdout"):
                output += f"STDOUT:\n{result['stdout']}\n\n"
            if result.get("stderr"):
                output += f"STDERR:\n{result['stderr']}\n\n"
            if "error" in result:
                output += f"ERROR:\n{result['error']}\n\n"
        
        # Track command and operation in context
        ctx.context.track_command(command_description)
        ctx.context.set_operation(command_description)
        
        return output
    except Exception as e:
        return f"Error executing Python code: {str(e)}"

# Git Tools
@function_tool
async def git_status(
    ctx: RunContextWrapper[AgentContext],
    repo_path: Optional[str] = None
) -> str:
    """
    Get the status of a Git repository.
    
    Args:
        repo_path: Path to the Git repository (optional, uses current directory if not specified)
        
    Returns:
        Git status information
    """
    try:
        path = repo_path or ctx.context.current_dir
        
        # Check if it's a git repository
        is_repo, repo_root = GitManager.check_git_repo(path)
        
        if not is_repo:
            return json.dumps({"error": "Not a Git repository"})
        
        # Get status
        status = GitManager.get_git_status(repo_root)
        
        # Track operation
        ctx.context.set_operation("Checked Git status")
        
        return json.dumps(status, indent=2)
    except Exception as e:
        return json.dumps({"error": f"Error getting Git status: {str(e)}"})

@function_tool
async def git_commit(
    ctx: RunContextWrapper[AgentContext],
    message: str,
    repo_path: Optional[str] = None
) -> str:
    """
    Commit changes to a Git repository.
    
    Args:
        message: Commit message
        repo_path: Path to the Git repository (optional, uses current directory if not specified)
        
    Returns:
        Result of the commit operation
    """
    try:
        path = repo_path or ctx.context.current_dir
        
        # Check if it's a git repository
        is_repo, repo_root = GitManager.check_git_repo(path)
        
        if not is_repo:
            return json.dumps({"error": "Not a Git repository"})
        
        # Commit
        success, result = GitManager.git_commit(repo_root, message)
        
        # Track operation
        ctx.context.track_command(f"Git commit: {message}")
        ctx.context.set_operation("Created Git commit")
        
        if success:
            return json.dumps({"success": True, "message": result})
        else:
            return json.dumps({"error": result})
    except Exception as e:
        return json.dumps({"error": f"Error committing to Git: {str(e)}"})

# Context Management Tool
@function_tool
async def summarize_context(ctx: RunContextWrapper[AgentContext]) -> str:
    """
    Summarize the current conversation context to manage token usage.
    This is automatically called when the context reaches the token limit.
    
    Returns:
        A confirmation message after summarizing.
    """
    # This would normally use an LLM to summarize, but for simplicity
    # we'll just simulate this process
    
    # Reset token count after summarization
    ctx.context.reset_token_count()
    ctx.context.history_summary += f"[Context window reached {ctx.context.max_tokens} tokens and was summarized]"
    
    return "Context summarized successfully. The conversation will continue with the summarized context."

# Define agents for specific tasks
# 1. Code Generation Agent
code_generation_agent = Agent(
    name="Code Generator",
    instructions="""You are an expert Python code generator. Your task is to write clean, efficient, 
    and well-documented Python code based on user requirements. Follow these guidelines:
    
    1. Always include docstrings for functions and classes
    2. Follow PEP 8 style guidelines
    3. Include error handling where appropriate
    4. Write modular, reusable code
    5. Include comments for complex sections
    
    You must return your response as a structured output with:
    - The generated code
    - An explanation of what the code does
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - An optional suggested file path for the code""",
    model="o3-mini",
    output_type=CodeGenerationOutput,
    tools=[write_to_file, read_file, list_directory, compare_files]
)

# Create specialized variants using clone
data_science_code_generator = code_generation_agent.clone(
    name="Data Science Code Generator",
    instructions="""You are an expert Python code generator specialized in data science. 
    Your task is to write clean, efficient, and well-documented Python code for data science tasks.
    Focus on libraries like pandas, numpy, scikit-learn, matplotlib, and other data science tools.
    
    Follow these guidelines:
    1. Always include docstrings for functions and classes
    2. Follow PEP 8 style guidelines
    3. Include error handling where appropriate
    4. Write modular, reusable code
    5. Include visualizations where appropriate
    6. Always include data validation steps
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - The generated code
    - An explanation of what the code does
    - An optional suggested file path for the code"""
)

web_code_generator = code_generation_agent.clone(
    name="Web Development Code Generator",
    instructions="""You are an expert Python code generator specialized in web development.
    Your task is to write clean, efficient, and well-documented Python code for web applications.
    Focus on frameworks like Flask, Django, FastAPI, and related web technologies.
    
    Follow these guidelines:
    1. Always include docstrings for functions and classes
    2. Follow PEP 8 style guidelines
    3. Include error handling where appropriate
    4. Write modular, reusable code
    5. Include security best practices
    6. Consider performance and scalability
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - The generated code
    - An explanation of what the code does
    - An optional suggested file path for the code"""
)

# 2. Code Analysis Agent
code_analysis_agent = Agent(
    name="Code Analyzer",
    instructions="""You are an expert code analyzer. Your task is to review Python code, 
    identify issues, and suggest improvements. Focus on:
    
    1. Code quality and adherence to best practices
    2. Potential bugs and error cases
    3. Performance issues
    4. Security vulnerabilities
    5. Readability and maintainability
    
    Use the analysis tools to help identify issues in the code.
    
    You can use the compare_files tool to show differences between original code and your suggested improvements.
    This provides a visual diff that makes it easier for users to understand your proposed changes.
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - A list of identified issues
    - A list of suggested improvements
    - A summary of your analysis""",
    model="o3-mini",
    output_type=CodeAnalysisOutput,
    tools=[analyze_python_file, read_file, list_directory, search_files, compare_files]
)

# 3. Project Analysis Agent
project_analysis_agent = Agent(
    name="Project Analyzer",
    instructions="""You are an expert project analyzer. Your task is to review Python projects, 
    understand their structure, dependencies, and architecture. You should:
    
    1. Identify the key components and their relationships
    2. Analyze the project structure and organization
    3. Identify dependencies and their versions
    4. Look for patterns and anti-patterns
    5. Suggest improvements to the project structure
    
    When suggesting structural changes or improvements to files, you can use the compare_files tool
    to show the differences between the original and your suggested version.
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - Project structure information
    - A list of project dependencies
    - A list of recommendations for improvement
    - A summary of your analysis""",
    model="o3-mini",
    output_type=ProjectAnalysisOutput,
    tools=[list_directory, read_file, search_files, compare_files]
)

# Define dynamic instructions function for terminal agent
# Define dynamic instructions function for terminal agent
def terminal_agent_instructions(ctx: RunContextWrapper[AgentContext], agent: Agent[AgentContext]) -> str:
    """
    Dynamic instructions for terminal agent that incorporates context information.
    
    Args:
        ctx: Context wrapper containing AgentContext
        agent: The agent object
        
    Returns:
        Formatted instruction string
    """
    # Build accessed files and commands for context
    accessed_files = ", ".join(ctx.context.accessed_files[-5:]) if ctx.context.accessed_files else "None yet"
    executed_commands = ", ".join(ctx.context.executed_commands[-5:]) if ctx.context.executed_commands else "None yet"
    
    return f"""You are a Terminal Agent for macOS and Linux, an expert coding assistant designed to help with 
    Python development tasks. You can navigate the file system, read and analyze code, execute Python code, 
    and provide expert assistance with coding tasks.
    
    You have access to the following capabilities:
    
    1. File System Operations: 
        - List directories, read files, change directories
        - Enhanced file search capabilities:
         * search_files: Search for files in a directory
         * find_file: Find a specific file by name
         * find_in_parent_directories: Search in current and parent directories
         * search_project_directories: Comprehensive project-wide search

    2. Code Analysis: Analyze Python files for bugs and issues using pylint
    3. Project Analysis: Analyze the structure and organization of Python projects
    4. Code Generation and Execution: Write code to files and execute Python code
    5. Code Comparison and Diff: Compare files or content and display differences with side-by-side highlighting
    6. Context Management: Maintain conversation context even for long sessions
    
    When searching for files or directories:
    - If not found in the current directory, try parent directories
    - Use search_project_directories for a comprehensive search
    - For recursive searches, make sure to explicitly set recursive=True
    
    You can also delegate tasks to specialized agents when needed:
    - Code Generator: For writing Python code
    - Code Analyzer: For in-depth code analysis
    - Project Analyzer: For analyzing project structure
    - Data Science Code Generator: For generating data science code
    - Web Development Code Generator: For generating web application code
    
    Current working directory: {ctx.context.current_dir}
    Current session ID: {ctx.context.session_id}
    Last operation: {ctx.context.last_operation or "None"}
    Recently accessed files: {accessed_files}
    Recent commands: {executed_commands}
    
    When helping with coding tasks:
    1. Be specific and thorough in your explanations
    2. Provide code examples when relevant
    3. Suggest best practices and improvements
    4. Explain your reasoning
    5. Be aware of the limitations of the tools
    
    You must return your response as a structured output with:
    - Python code always in triple backticks ```python ...```
    - Always output in Markdown format
    - Your response to the user query
    - A list of files accessed during processing
    - A list of commands executed during processing
    """
# Main Terminal Agent
terminal_agent = Agent[AgentContext](
    name="Terminal Agent",
    instructions=terminal_agent_instructions,
    model="o3-mini",
    output_type=TerminalAgentOutput,
    input_guardrails=[security_guardrail],  # Using our updated security guardrail
    tools=[
        list_directory,
        read_file,
        search_files,
        find_file,  # Basic file finder
        find_in_parent_directories,  # Find in parent directories
        search_project_directories,  # Comprehensive project search
        change_directory,
        analyze_python_file,
        compare_files,
        git_status,
        git_commit,
        write_to_file,
        execute_python,
        summarize_context,
        # Add specialized agents as tools
        code_generation_agent.as_tool(
            tool_name="generate_code",
            tool_description="Generate Python code based on a detailed description. Returns code and explanation."
        ),
        data_science_code_generator.as_tool(
            tool_name="generate_data_science_code",
            tool_description="Generate Python code for data science tasks (pandas, numpy, scikit-learn, matplotlib)."
        ),
        web_code_generator.as_tool(
            tool_name="generate_web_code",
            tool_description="Generate Python code for web applications (Flask, Django, FastAPI)."
        ),
        code_analysis_agent.as_tool(
            tool_name="analyze_code",
            tool_description="Analyze Python code for issues and improvements. Returns issues, improvements, and summary."
        ),
        project_analysis_agent.as_tool(
            tool_name="analyze_project",
            tool_description="Analyze a Python project's structure and dependencies. Returns structure, dependencies, and recommendations."
        )
    ],
    handoffs=[
        handoff(
            code_generation_agent, 
            tool_name_override="handoff_to_code_generator",
            tool_description_override="Hand off the conversation to the Code Generator agent for in-depth code writing."
        ),
        handoff(
            data_science_code_generator,
            tool_name_override="handoff_to_data_science_generator",
            tool_description_override="Hand off the conversation to the Data Science Code Generator for specialized data science coding."
        ),
        handoff(
            web_code_generator,
            tool_name_override="handoff_to_web_generator",
            tool_description_override="Hand off the conversation to the Web Development Code Generator for specialized web application coding."
        ),
        handoff(
            code_analysis_agent,
            tool_name_override="handoff_to_code_analyzer", 
            tool_description_override="Hand off the conversation to the Code Analyzer agent for comprehensive code analysis."
        ),
        handoff(
            project_analysis_agent,
            tool_name_override="handoff_to_project_analyzer",
            tool_description_override="Hand off the conversation to the Project Analyzer agent for detailed project structure analysis."
        )
    ]
)

# Function to run a query through the agent system
async def run_agent_query(
    query: str,
    context: AgentContext,
    stream_callback = None
) -> Dict[str, Any]:
    """
    Run a query through the agent system
    
    Args:
        query: The user's query string
        context: The agent context
        stream_callback: Optional callback function for streaming responses
        
    Returns:
        Dictionary with the agent's response
    """
    try:
        # Log the query
        logger.info(f"Processing agent query: {query}")
        
        # Validate context
        if not context or not hasattr(context, 'current_dir') or not context.current_dir:
            logger.error("Invalid agent context or missing current_dir")
            return {
                "error": "Agent context is invalid or missing current directory information.",
                "response": "I couldn't process that request because of an issue with the agent context."
            }
            
        # Validate the current directory exists
        if not os.path.exists(context.current_dir):
            logger.error(f"Current directory in context doesn't exist: {context.current_dir}")
            return {
                "error": f"Current directory doesn't exist: {context.current_dir}",
                "response": "I couldn't process that request because the working directory doesn't exist."
            }
        
        # Set up run config
        run_config = RunConfig(
            model_settings=ModelSettings(),
            trace_metadata={
                "user_id": "terminator_user",
                "session_type": "streaming" if stream_callback else "standard",
            },
            trace_include_sensitive_data=True,
            workflow_name="Terminator Agent Session",
            group_id=context.session_id,
        )
        
        if stream_callback:
            # Run with streaming
            try:
                result = Runner.run_streamed(
                    starting_agent=terminal_agent,
                    input=query,
                    context=context,
                    run_config=run_config
                )
                
                # Process streaming events if callback provided
                async for event in result.stream_events():
                    if stream_callback and callable(stream_callback):
                        await stream_callback(event)
                        
                # Return the final result
                if hasattr(result, 'final_output'):
                    logger.info("Successfully completed streaming agent query")
                    return {
                        "response": result.final_output.response,
                        "files_accessed": result.final_output.files_accessed,
                        "commands_executed": result.final_output.commands_executed
                    }
                else:
                    logger.error("No response from streaming agent query")
                    return {
                        "error": "No response from agent",
                        "response": "I couldn't process that request. No response was generated."
                    }
            except Exception as stream_error:
                logger.error(f"Error in streaming agent query: {str(stream_error)}", exc_info=True)
                return {
                    "error": f"Streaming error: {str(stream_error)}",
                    "response": f"I encountered an error while processing your request: {str(stream_error)}"
                }
                    
        else:
            # Run without streaming
            try:
                result = await Runner.run(
                    starting_agent=terminal_agent,
                    input=query,
                    context=context,
                    run_config=run_config
                )
                
                # Return the result
                if hasattr(result, 'final_output'):
                    logger.info("Successfully completed agent query")
                    return {
                        "response": result.final_output.response,
                        "files_accessed": result.final_output.files_accessed,
                        "commands_executed": result.final_output.commands_executed
                    }
                else:
                    logger.error("No response from agent query")
                    return {
                        "error": "No response from agent",
                        "response": "I couldn't process that request. No response was generated."
                    }
            except Exception as run_error:
                logger.error(f"Error in agent query: {str(run_error)}", exc_info=True)
                return {
                    "error": f"Run error: {str(run_error)}",
                    "response": f"I encountered an error while processing your request: {str(run_error)}"
                }
                
    except InputGuardrailTripwireTriggered as guard_error:
        # Handle security guardrail trigger
        try:
            if hasattr(guard_error.guardrail_result, 'output_info'):
                security_check = guard_error.guardrail_result.output_info
                risk_type = getattr(security_check, 'risk_type', 'Potential security risk')
                reasoning = getattr(security_check, 'reasoning', 'Security guardrail triggered')
            else:
                risk_type = "Potential security risk"
                reasoning = "Your request triggered a security guardrail"
                
            logger.warning(f"Security guardrail triggered: {risk_type} - {reasoning}")
            return {
                "error": f"Security Warning: {risk_type}",
                "details": reasoning,
                "response": f"I can't process that request because it was flagged as a potential security risk: {risk_type}. {reasoning}"
            }
        except Exception as e:
            logger.error(f"Error processing security guardrail result: {str(e)}", exc_info=True)
            return {
                "error": "Security Warning: Request blocked by security guardrail",
                "response": "I can't process that request because it was flagged by my security systems."
            }
            
    except Exception as e:
        logger.error(f"Exception running agent query: {str(e)}", exc_info=True)
        return {
            "error": f"Error: {str(e)}",
            "response": f"I encountered an unexpected error while processing your request: {str(e)}"
        }

# Initialize the agent system
def initialize_agent_system():
    """Initialize the agent system with API keys and logging"""
    try:
        # Set up logging if not already configured
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
            logger.setLevel(logging.INFO)
            
            # Also add a file handler for persistent logs
            file_handler = logging.FileHandler("terminator_agent.log")
            file_handler.setFormatter(formatter)
            logger.addHandler(file_handler)
        
        # Check for API key
        api_key = os.getenv("OPENAI_API_KEY")
        if not api_key:
            logger.error("OPENAI_API_KEY not set in environment variables")
            return False
            
        # Set the API key for both OpenAI and agents
        OpenAI.api_key = api_key
        set_default_openai_key(api_key)
        
        # Verify API key works (optional - might add API call overhead)
        try:
            # This is a minimal API call to verify the key works
            client = OpenAI()
            models = client.models.list()
            if not models:
                logger.warning("API key verification: No models returned, but API didn't error")
            else:
                logger.info(f"API key verification successful, found {len(models.data)} models")
        except Exception as e:
            logger.error(f"API key verification failed: {str(e)}")
            return False
        
        # Log successful initialization
        logger.info("Agent system initialized successfully")
        return True
        
    except Exception as e:
        logger.error(f"Error initializing agent system: {str(e)}", exc_info=True)
        return False
```

My question: Can you see the code in the editor?
2025-04-01 10:48:00,834 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: search.*file
2025-04-01 10:48:00,855 - openai._base_client - INFO - Retrying request to /responses in 0.394531 seconds
2025-04-01 10:48:03,697 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 10:48:10,300 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 10:48:10,455 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 10:48:14,493 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 13:11:26,740 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 13:11:26,751 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 13:11:26,751 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 13:11:26,752 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 13:11:26,754 - root - INFO - AI panel initialized successfully
2025-04-01 13:11:45,909 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 13:11:45,918 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 13:11:45,918 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 13:11:45,919 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 13:11:45,920 - root - INFO - AI panel initialized successfully
2025-04-01 18:07:20,910 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 18:07:20,921 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 18:07:20,921 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 18:07:20,922 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 18:07:20,925 - root - INFO - AI panel initialized successfully
2025-04-01 18:13:32,140 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 18:13:32,152 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 18:13:32,153 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 18:13:32,154 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 18:13:32,157 - root - INFO - AI panel initialized successfully
2025-04-01 18:14:03,732 - terminator_agents - INFO - Processing agent query: Just testing.
2025-04-01 18:14:03,741 - terminator_agents - INFO - Added 2 tokens for query, total: 2
2025-04-01 18:14:04,524 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 400 Bad Request"
2025-04-01 18:14:04,525 - openai.agents - ERROR - Error getting response: Error code: 400 - {'error': {'message': "Invalid schema for function 'edit_current_file': In context=('properties', 'show_diff'), 'default' is not permitted.", 'type': 'invalid_request_error', 'param': 'tools[12].parameters', 'code': 'invalid_function_parameters'}}. (request_id: req_15dac524db3bbfa16efafc1409978464)
2025-04-01 18:14:04,526 - terminator_agents - ERROR - Error in agent query: Error code: 400 - {'error': {'message': "Invalid schema for function 'edit_current_file': In context=('properties', 'show_diff'), 'default' is not permitted.", 'type': 'invalid_request_error', 'param': 'tools[12].parameters', 'code': 'invalid_function_parameters'}}
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1804, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 215, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 739, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 896, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 75, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 234, in _fetch_response
    return await self._client.responses.create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/resources/responses/responses.py", line 1415, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1461, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1562, in _request
    raise self._make_status_error_from_response(err.response) from None
openai.BadRequestError: Error code: 400 - {'error': {'message': "Invalid schema for function 'edit_current_file': In context=('properties', 'show_diff'), 'default' is not permitted.", 'type': 'invalid_request_error', 'param': 'tools[12].parameters', 'code': 'invalid_function_parameters'}}
2025-04-01 18:14:07,701 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 18:21:35,081 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 18:21:35,090 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 18:21:35,090 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 18:21:35,090 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 18:21:35,094 - root - INFO - AI panel initialized successfully
2025-04-01 18:21:45,976 - terminator_agents - INFO - Processing agent query: Test.
2025-04-01 18:21:45,986 - terminator_agents - INFO - Added 1 tokens for query, total: 1
2025-04-01 18:21:48,966 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 18:21:50,267 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 18:22:00,307 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 18:22:00,440 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 18:22:01,346 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 18:24:02,273 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
print("WTF!@!@"):
```

My question: Can you help me complete the code in the editor?
2025-04-01 18:24:02,277 - terminator_agents - INFO - Added 20 tokens for query, total: 21
2025-04-01 18:24:02,301 - openai._base_client - INFO - Retrying request to /responses in 0.403469 seconds
2025-04-01 18:24:04,603 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 18:24:12,067 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 18:24:14,223 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 18:24:15,743 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 18:29:38,845 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 18:29:39,152 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 18:29:43,081 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 18:31:05,898 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 18:31:31,239 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
#!/usr/bin/env python3
"""
Terminator v1 - A terminal-based Python IDE with integrated AI assistant
Combines code editing, file management, Git integration, and Claude AI assistance
"""

import os
import sys
import asyncio
import logging
import time
import json
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List
from rich.syntax import Syntax
# Textual imports
from textual.app import App, ComposeResult
from textual.containers import Container, Horizontal, Vertical, ScrollableContainer
from textual.events import MouseDown, MouseUp, MouseMove
from textual.widgets import (
    Header, Footer, Static, Button, Input, TextArea, Tree, DirectoryTree,
    Label, Markdown, LoadingIndicator, TabbedContent, TabPane
)
from textual.widgets.tree import TreeNode
from textual.screen import Screen, ModalScreen
from textual.suggester import SuggestFromList
from textual.widgets import DataTable
from textual.binding import Binding
from textual.reactive import reactive
from textual import events, work
from textual.message import Message
from textual.timer import Timer
import re

# Import agent and tools modules
from TerminatorV1_agents import initialize_agent_system, run_agent_query, AgentContext
from TerminatorV1_tools import (
    FileSystem, CodeAnalyzer, GitManager, PythonRunner, PythonDebugger,
    CollaborationManager, CollaborationSession
)

# Git Commit Dialog Screen
class CommitDialog(ModalScreen):
    """Git commit dialog screen with Escape key support"""
    
    # Add key bindings for the dialog
    BINDINGS = [
        Binding("escape", "cancel", "Cancel"),
    ]
    
    def compose(self) -> ComposeResult:
        """Create the dialog layout"""
        with Container(id="commit-dialog"):
            with Horizontal(id="commit-header"):
                yield Label("Commit Message", classes="title")
                yield Label("Press ESC to cancel", classes="escape-hint")
            yield TextArea(language="text", id="commit-message")
            with Horizontal():
                yield Button("Cancel", id="cancel-commit", variant="error")
                yield Button("Commit", id="confirm-commit", variant="success")
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        if event.button.id == "cancel-commit":
            self.app.pop_screen()
        elif event.button.id == "confirm-commit":
            # Get the commit message
            commit_message = self.query_one("#commit-message").text
            if not commit_message:
                self.notify("Please enter a commit message", severity="error")
                return
                
            # Trigger the commit and close the dialog
            self.app.git_commit(commit_message)
            self.app.pop_screen()
            
    async def action_cancel(self) -> None:
        """Cancel the commit dialog (called when ESC is pressed)"""
        self.app.pop_screen()
        
    async def on_key(self, event) -> None:
        """Handle key presses in the dialog"""
        # If the Escape key was already handled by bindings, we don't need to do anything
        # This is a fallback in case the binding doesn't work
        if event.key == "escape":
            await self.action_cancel()

# Code Analysis Dialog
class CodeAnalysisDialog(ModalScreen):
    """Code analysis results screen"""
    
    def compose(self) -> ComposeResult:
        """Create the dialog layout"""
        with Container(id="analysis-dialog"):
            yield Label("Code Analysis Results", classes="title")
            yield ScrollableContainer(Markdown("Analyzing code..."), id="analysis-result")
            yield Button("Close", id="close-analysis")
    
    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        if event.button.id == "close-analysis":
            self.app.pop_screen()

# Theme Selection Screen
class ThemeSelectionScreen(ModalScreen):
    """Screen for selecting a TextArea theme"""
    
    def compose(self) -> ComposeResult:
        """Create the theme selection layout"""
        with Container(id="theme-dialog"):
            yield Label("Select Theme", classes="title")
            
            # Use a predefined list of known themes
            available_themes = ["css", "monokai", "dracula", "github_light", "vscode_dark"]
            
            with ScrollableContainer(id="theme-list"):
                for theme in sorted(available_themes):
                    yield Button(theme, id=f"theme-{theme}", classes="theme-button")
            
            yield Button("Cancel", id="cancel-theme", variant="error")
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        
        if button_id == "cancel-theme":
            self.app.pop_screen()
        elif button_id.startswith("theme-"):
            theme_name = button_id[6:]  # Remove "theme-" prefix
            self.app.set_editor_theme(theme_name)
            self.app.pop_screen()

class DiffViewScreen(ModalScreen):
    """Modal screen for displaying code diffs"""
    
    def __init__(self, 
                 original_content: str, 
                 modified_content: str, 
                 title: str = "Code Changes",
                 original_title: str = "Original",
                 modified_title: str = "Modified",
                 highlight_language: str = "python"):
        """
        Initialize the diff view screen
        
        Args:
            original_content: Original content
            modified_content: Modified content
            title: Title of the diff view
            original_title: Title for the original content panel
            modified_title: Title for the modified content panel
            highlight_language: Language for syntax highlighting
        """
        super().__init__()
        self.original_content = original_content
        self.modified_content = modified_content
        self.screen_title = title
        self.original_title = original_title
        self.modified_title = modified_title
        self.language = highlight_language
        
        # Calculate the diff
        self.unified_diff = CodeAnalyzer.create_diff(original_content, modified_content)
        
        # Extract line changes from diff
        self.changed_lines = self._extract_line_changes(self.unified_diff)
    
    def _extract_line_changes(self, diff_text):
        """
        Extract line numbers that were added or removed in the diff
        
        Args:
            diff_text: The unified diff text
            
        Returns:
            Dictionary with original and modified line numbers that changed
        """
        original_changes = set()
        modified_changes = set()
        
        current_original_line = 0
        current_modified_line = 0
        
        for line in diff_text.splitlines():
            # Check if this is a hunk header line (e.g., @@ -1,7 +1,9 @@)
            if line.startswith("@@"):
                # Extract line numbers from the hunk header
                # Format is @@ -original_start,original_count +modified_start,modified_count @@
                match = re.search(r'@@ -(\d+)(?:,\d+)? \+(\d+)(?:,\d+)? @@', line)
                if match:
                    current_original_line = int(match.group(1)) - 1  # Adjust to 0-based indexing
                    current_modified_line = int(match.group(2)) - 1  # Adjust to 0-based indexing
            elif line.startswith("-"):
                original_changes.add(current_original_line)
                current_original_line += 1
            elif line.startswith("+"):
                modified_changes.add(current_modified_line)
                current_modified_line += 1
            else:
                # Context line or other (moves both counters)
                current_original_line += 1
                current_modified_line += 1
        
        return {
            "original": original_changes,
            "modified": modified_changes
        }
        
    def compose(self) -> ComposeResult:
        """Create the diff view layout"""
        yield Label(self.screen_title, id="diff-title", classes="title")
        
        with Container(id="diff-view-container"):
            with Horizontal(id="diff-split-view"):
                # Left panel: Original code
                with Vertical(id="diff-original-panel"):
                    yield Label(self.original_title, classes="subtitle")
                    # Use TextArea with line numbers for original content
                    original_editor = yield TextArea(
                        language=self.language,
                        theme="monokai",
                        show_line_numbers=True,
                        read_only=True,
                        id="diff-original-content"
                    )
                    original_editor.text = self.original_content
                
                # Right panel: Modified code
                with Vertical(id="diff-modified-panel"):
                    yield Label(self.modified_title, classes="subtitle")
                    # Use TextArea with line numbers for modified content
                    modified_editor = yield TextArea(
                        language=self.language,
                        theme="monokai",
                        show_line_numbers=True,
                        read_only=True,
                        id="diff-modified-content"
                    )
                    modified_editor.text = self.modified_content
            
            # Bottom panel: Unified diff view (optional, can be toggled)
            with Vertical(id="unified-diff-panel", classes="hidden"):
                yield Label("Unified Diff View", classes="subtitle")
                diff_editor = yield TextArea(
                    language="diff",
                    theme="monokai",
                    show_line_numbers=True,
                    read_only=True,
                    id="unified-diff-content"
                )
                diff_editor.text = self.unified_diff
            
            with Horizontal(id="diff-buttons"):
                yield Button("Apply Changes", id="apply-diff", variant="success")
                yield Button("Toggle Unified View", id="toggle-unified-view")
                yield Button("Close", id="close-diff", variant="error")
    
    def on_mount(self) -> None:
        """Called when the screen is mounted"""
        # Apply custom CSS classes to highlight changed lines
        self._highlight_changes()
    
    def _highlight_changes(self) -> None:
        """Highlight the lines that have changed in both panels"""
        # This is a basic implementation - for a production app, 
        # you would use proper CSS styling to highlight changes
        try:
            # Create CSS for highlighting original lines that were changed
            original_editor = self.query_one("#diff-original-content")
            for line_num in self.changed_lines["original"]:
                # Apply highlighting to the lines that were changed in the original
                pass  # This would require custom rendering in a full implementation
            
            # Create CSS for highlighting modified lines that were changed    
            modified_editor = self.query_one("#diff-modified-content")
            for line_num in self.changed_lines["modified"]:
                # Apply highlighting to the lines that were changed in the modified
                pass  # This would require custom rendering in a full implementation
        
        except Exception as e:
            logging.error(f"Error highlighting changes: {str(e)}", exc_info=True)
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        
        if button_id == "close-diff":
            # Close without applying changes
            self.app.pop_screen()
        elif button_id == "apply-diff":
            # Apply the changes and close
            self.app.apply_diff_changes(self.modified_content)
            self.app.pop_screen()
        elif button_id == "toggle-unified-view":
            # Toggle visibility of unified diff panel
            unified_panel = self.query_one("#unified-diff-panel")
            if "hidden" in unified_panel.classes:
                unified_panel.remove_class("hidden")
            else:
                unified_panel.add_class("hidden")
# Debugger Screen
class DebuggerScreen(Screen):
    """Debugger interface screen"""
    
    def compose(self) -> ComposeResult:
        """Create the debugger layout"""
        yield Header()
        
        with Horizontal():
            # Left panel: Code with breakpoints
            with Vertical(id="debug-code-panel"):
                yield Label("Source Code", classes="title")
                yield TextArea(language="python", id="debug-code", read_only=True)
                with Horizontal():
                    yield Button("Step Over", id="debug-step-over-btn")
                    yield Button("Step Into", id="debug-step-into-btn")
                    yield Button("Step Out", id="debug-step-out-btn")
                    yield Button("Continue", id="debug-continue-btn")
                    yield Button("Stop", id="debug-stop-btn", variant="error")
            
            # Right panel: Variable inspector and output
            with Vertical(id="debug-info-panel"):
                yield Label("Variables", classes="title")
                yield DataTable(id="debug-variables")
                
                yield Label("Call Stack", classes="title")
                yield DataTable(id="debug-stack")
                
                yield Label("Output", classes="title")
                yield TextArea(id="debug-output", read_only=True)
    
    def on_mount(self):
        """Initialize the debugger UI"""
        # Set up variables table
        variables_table = self.query_one("#debug-variables")
        variables_table.add_columns("Name", "Type", "Value")
        
        # Set up call stack table
        stack_table = self.query_one("#debug-stack")
        stack_table.add_columns("Frame", "Function", "File", "Line")
        
    
    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle debug control buttons"""
        button_id = event.button.id
        
        if button_id == "debug-step-over-btn":
            self.app.debug_step_over()
        elif button_id == "debug-step-into-btn":
            self.app.debug_step_into()
        elif button_id == "debug-step-out-btn":
            self.app.debug_step_out()
        elif button_id == "debug-continue-btn":
            self.app.debug_continue()
        elif button_id == "debug-stop-btn":
            self.app.debug_stop()

# Git Branch Visualization Screen
class BranchVisualizationScreen(Screen):
    """Screen for visualizing Git branches and history"""
    
    def compose(self) -> ComposeResult:
        """Create the branch visualization layout"""
        yield Header()
        
        with Horizontal():
            # Left panel: Branch tree
            with Vertical(id="branch-tree-panel"):
                yield Label("Branch Structure", classes="title")
                yield TextArea(language="git", id="branch-graph", read_only=True)
                with Horizontal():
                    yield Button("Refresh", id="refresh-branches-btn")
                    yield Button("Back", id="back-to-main-btn", variant="primary")
            
            # Right panel: Branch and commit info
            with Vertical(id="branch-info-panel"):
                yield Label("Current Branch", classes="title")
                yield Static("", id="current-branch-info")
                
                yield Label("All Branches", classes="title")
                yield ScrollableContainer(id="all-branches-container")
                
                yield Label("Recent Commits", classes="title")
                yield ScrollableContainer(id="recent-commits-container")
                
                with Horizontal():
                    yield Input(placeholder="New branch name...", id="new-branch-input")
                    yield Button("Create Branch", id="create-branch-btn")
    
    def on_mount(self):
        """Set up the branch visualization screen"""
        # Load branches and commit data
        self.load_branch_data()
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        
        if button_id == "refresh-branches-btn":
            self.load_branch_data()
        elif button_id == "back-to-main-btn":
            self.app.pop_screen()
        elif button_id == "create-branch-btn":
            self.create_new_branch()
    
    def load_branch_data(self):
        """Load and display branch and commit data"""
        if not self.app.git_repository:
            self.app.notify("Not a Git repository", severity="error")
            return
            
        # Get branch graph data
        branch_data = GitManager.get_branch_graph(self.app.git_repository)
        
        if "error" in branch_data:
            self.app.notify(f"Failed to load branch data: {branch_data['error']}", severity="error")
            return
            
        # Update branch graph visualization
        branch_graph = self.query_one("#branch-graph")
        branch_graph.text = branch_data.get("graph_output", "")
        
        # Update current branch info
        current_branch_info = self.query_one("#current-branch-info")
        current_branch = branch_data.get("branches", {}).get("current_branch", "unknown")
        current_branch_info.update(f"Current branch: [bold]{current_branch}[/bold]")
        
        # Update all branches list
        all_branches_container = self.query_one("#all-branches-container")
        all_branches_container.remove_children()
        
        local_branches = branch_data.get("branches", {}).get("local_branches", [])
        remote_branches = branch_data.get("branches", {}).get("remote_branches", [])
        
        if local_branches:
            all_branches_container.mount(Label("Local Branches:"))
            for branch in local_branches:
                # Create a button for each branch to switch to it
                all_branches_container.mount(
                    Button(branch, id=f"switch-branch-{branch}", classes="branch-button")
                )
                
        if remote_branches:
            all_branches_container.mount(Label("Remote Branches:"))
            for branch in remote_branches:
                # Create a button for each remote branch to check it out
                all_branches_container.mount(
                    Button(branch, id=f"checkout-remote-{branch}", classes="remote-branch-button")
                )
                
        # Update recent commits list
        commits_container = self.query_one("#recent-commits-container")
        commits_container.remove_children()
        
        commits = branch_data.get("commits", [])
        for commit in commits:
            commit_message = commit.get("message", "")
            short_hash = commit.get("short_hash", "")
            author = commit.get("author", "")
            date = commit.get("date", "")
            
            # Create a button for the commit
            commit_label = f"{short_hash} ({date}) - {commit_message}"
            commits_container.mount(
                Button(commit_label, id=f"view-commit-{short_hash}", classes="commit-button")
            )
    
    def create_new_branch(self):
        """Create a new branch"""
        if not self.app.git_repository:
            self.app.notify("Not a Git repository", severity="error")
            return
            
        # Get the branch name
        branch_input = self.query_one("#new-branch-input")
        branch_name = branch_input.value.strip()
        
        if not branch_name:
            self.app.notify("Please enter a branch name", severity="warning")
            return
            
        # Create the branch
        success, message = GitManager.create_branch(self.app.git_repository, branch_name)
        
        if success:
            self.app.notify(message, severity="success")
            # Refresh the branch data
            self.load_branch_data()
            # Clear the input
            branch_input.value = ""
        else:
            self.app.notify(message, severity="error")
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        
        if button_id == "refresh-branches-btn":
            self.load_branch_data()
        elif button_id == "back-to-main-btn":
            self.app.pop_screen()
        elif button_id == "create-branch-btn":
            self.create_new_branch()
        elif button_id.startswith("switch-branch-"):
            # Switch to local branch
            branch_name = button_id[14:]  # Remove "switch-branch-" prefix
            self.switch_branch(branch_name)
        elif button_id.startswith("checkout-remote-"):
            # Checkout remote branch
            branch_name = button_id[16:]  # Remove "checkout-remote-" prefix
            self.switch_branch(branch_name)
    
    def switch_branch(self, branch_name: str):
        """Switch to a different branch"""
        if not self.app.git_repository:
            self.app.notify("Not a Git repository", severity="error")
            return
            
        # Switch branch
        success, message = GitManager.switch_branch(self.app.git_repository, branch_name)
        
        if success:
            self.app.notify(message, severity="success")
            # Refresh the branch data
            self.load_branch_data()
            # Update Git status in main app
            self.app.update_git_status()
        else:
            self.app.notify(message, severity="error")

# Remote Connection Dialog
class RemoteConnectionDialog(ModalScreen):
    """Dialog for setting up a remote development connection"""
    
    def compose(self) -> ComposeResult:
        """Create the dialog layout"""
        with Container(id="remote-dialog"):
            yield Label("Remote Connection", classes="title")
            
            yield Label("Connection Type:")
            with Horizontal():
                yield Button("SSH", id="connection-ssh", variant="primary")
                yield Button("SFTP", id="connection-sftp")
            
            yield Label("Server Details:")
            yield Input(placeholder="hostname or IP", id="remote-host")
            yield Input(placeholder="username", id="remote-username")
            yield Input(placeholder="port (default: 22)", id="remote-port", value="22")
            yield Input(placeholder="password (leave empty for key auth)", id="remote-password", password=True)
            
            yield Label("Remote Directory:")
            yield Input(placeholder="/path/to/project", id="remote-path")
            
            with Horizontal():
                yield Button("Cancel", id="cancel-remote", variant="error")
                yield Button("Connect", id="confirm-remote", variant="success")
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        
        if button_id == "cancel-remote":
            self.app.pop_screen()
        elif button_id == "confirm-remote":
            self.setup_remote_connection()
        elif button_id == "connection-ssh":
            # Highlight SSH button
            self.query_one("#connection-ssh").variant = "primary"
            self.query_one("#connection-sftp").variant = "default"
        elif button_id == "connection-sftp":
            # Highlight SFTP button
            self.query_one("#connection-sftp").variant = "primary"
            self.query_one("#connection-ssh").variant = "default"
    
    def setup_remote_connection(self):
        """Set up the remote connection"""
        # Get connection details
        host = self.query_one("#remote-host").value
        username = self.query_one("#remote-username").value
        port = self.query_one("#remote-port").value
        password = self.query_one("#remote-password").value
        remote_path = self.query_one("#remote-path").value
        
        # Validate inputs
        if not host or not username or not remote_path:
            self.app.notify("Please fill in all required fields", severity="error")
            return
        
        # Determine connection type
        connection_type = "ssh"
        if self.query_one("#connection-sftp").variant == "primary":
            connection_type = "sftp"
            
        # Configure the remote connection
        self.app.configure_remote(
            connection_type=connection_type,
            host=host,
            username=username,
            port=int(port) if port.isdigit() else 22,
            password=password,
            remote_path=remote_path
        )
        
        # Close the dialog
        self.app.pop_screen()

# Remote Files Browser Screen
class RemoteFilesBrowser(Screen):
    """Browser for remote files"""
    
    def compose(self) -> ComposeResult:
        """Create the remote files browser layout"""
        yield Header()
        
        with Horizontal():
            # Left panel: Remote files tree
            with Vertical(id="remote-files-panel"):
                with Horizontal():
                    yield Label("Remote Files", classes="title")
                    yield Button("Refresh", id="refresh-remote-btn")
                yield Tree("Remote Files", id="remote-files-tree")
                with Horizontal():
                    yield Button("Download", id="download-remote-btn")
                    yield Button("Upload", id="upload-remote-btn")
                    yield Button("Disconnect", id="disconnect-remote-btn", variant="error")
                    yield Button("Back", id="back-from-remote-btn")
            
            # Right panel: File preview
            with Vertical(id="remote-preview-panel"):
                yield Label("File Preview", classes="title")
                yield TextArea(id="remote-file-preview", read_only=True)
    
    def on_mount(self):
        """Initialize the remote files browser"""
        # Populate the remote files tree
        self.populate_remote_files()
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        
        if button_id == "refresh-remote-btn":
            self.populate_remote_files()
        elif button_id == "download-remote-btn":
            self.download_selected_file()
        elif button_id == "upload-remote-btn":
            self.app.action_upload_to_remote()
        elif button_id == "disconnect-remote-btn":
            self.app.disconnect_remote()
            self.app.pop_screen()
        elif button_id == "back-from-remote-btn":
            self.app.pop_screen()
    
    def populate_remote_files(self):
        """Populate the remote files tree"""
        if not self.app.remote_connected:
            self.app.notify("Not connected to remote server", severity="error")
            return
            
        # In a real implementation, fetch the remote files
        # For now, we'll use a placeholder
        tree = self.query_one("#remote-files-tree")
        tree.clear()
        
        # Add some dummy remote files
        remote_files = [
            "project/",
            "project/main.py",
            "project/utils.py",
            "project/data/",
            "project/data/config.json"
        ]
        
        # Create tree nodes
        root = tree.root
        root.expand()
        
        directories = {}
        
        for path in sorted(remote_files):
            parts = path.strip('/').split('/')
            
            if len(parts) == 1:
                # Top-level item
                if path.endswith('/'):
                    # Directory
                    directories[path] = root.add(path, expand=True)
                else:
                    # File
                    root.add_leaf(path)
            else:
                # Nested item
                parent_path = '/'.join(parts[:-1]) + '/'
                if parent_path in directories:
                    parent = directories[parent_path]
                    
                    if path.endswith('/'):
                        # Subdirectory
                        directories[path] = parent.add(parts[-1], expand=True)
                    else:
                        # File in directory
                        parent.add_leaf(parts[-1])
        
        self.app.notify("Remote files refreshed", severity="information")
    
    def download_selected_file(self):
        """Download the selected remote file"""
        # In a real implementation, download the selected file
        tree = self.query_one("#remote-files-tree")
        node = tree.cursor_node
        
        if node is None or not node.is_leaf:
            self.app.notify("Please select a file to download", severity="warning")
            return
            
        # Get the full path of the selected file
        file_path = self.get_node_path(node)
        
        self.app.notify(f"Downloading {file_path}... (simulation)", severity="information")
        
        # Simulate download
        preview = self.query_one("#remote-file-preview")
        preview.text = f"# Content of {file_path}\n\nThis is a simulated file content."
    
    def get_node_path(self, node):
        """Get the full path of a tree node"""
        path_parts = []
        
        # Traverse up to build the path
        current = node
        while current is not None and current != self.query_one("#remote-files-tree").root:
            path_parts.append(current.label)
            current = current.parent
            
        path_parts.reverse()
        return '/'.join(path_parts)
    
    def on_tree_node_selected(self, event: Tree.NodeSelected):
        """Handle node selection in the tree"""
        node = event.node
        
        if node.is_leaf:
            # Preview file
            file_path = self.get_node_path(node)
            
            # In a real implementation, get the file content
            preview = self.query_one("#remote-file-preview")
            preview.text = f"# Content of {file_path}\n\nThis is a simulated file content for a remote file."

# Semantic Search Screen
class SemanticSearchScreen(ModalScreen):
    """Screen for performing semantic code search using natural language"""
    
    def compose(self) -> ComposeResult:
        """Create the semantic search layout"""
        with Container(id="semantic-search-dialog"):
            yield Label("Semantic Code Search", classes="title")
            yield Input(placeholder="Describe what you're looking for...", id="semantic-search-input")
            
            yield Label("Search Results:", id="search-results-label")
            yield ScrollableContainer(id="semantic-results-container")
            
            with Horizontal():
                yield Button("Search", id="semantic-search-btn", variant="primary")
                yield Button("Cancel", id="cancel-semantic-search", variant="error")
    
    def on_mount(self):
        """Set up the search screen"""
        self.query_one("#semantic-search-input").focus()
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        
        if button_id == "semantic-search-btn":
            asyncio.create_task(self.perform_semantic_search())
        elif button_id == "cancel-semantic-search":
            asyncio.create_task(self.app.pop_screen())
    
    async def on_input_submitted(self, event: Input.Submitted) -> None:
        """Handle input submission"""
        if event.input.id == "semantic-search-input":
            asyncio.create_task(self.perform_semantic_search())
    
    @work
    async def perform_semantic_search(self) -> None:
        """Perform semantic code search using the AI"""
        search_query = self.query_one("#semantic-search-input").value.strip()
        
        if not search_query:
            self.app.notify("Please enter a search query", severity="warning")
            return
        
        # Show a loading message
        results_container = self.query_one("#semantic-results-container")
        results_container.remove_children()
        results_container.mount(Static("Searching...", classes="search-loading"))
        
        # Craft a prompt for the AI to convert the natural language query into semantic search results
        prompt = f"""Perform a semantic code search for the following query:
        
        "{search_query}"
        
        You need to:
        1. Interpret what files or code snippets would best match this natural language query
        2. Analyze the code in the project to find the most relevant matches
        3. Return a list of files and relevant code snippets that match the search intent
        4. Include a brief explanation of why each result matches the query
        
        FORMAT YOUR RESPONSE AS JSON with the following structure:
        {{
            "results": [
                {{
                    "file": "file_path",
                    "snippet": "code snippet",
                    "explanation": "why this matches"
                }}
            ]
        }}
        """
        
        # Call the AI to perform the search
        try:
            # Get the response from the AI
            result = await run_agent_query(prompt, self.app.agent_context)
            response = result.get("response", "")
            
            # Extract the JSON from the response
            json_pattern = r'\{[\s\S]*\}'
            matches = re.search(json_pattern, response)
            
            if matches:
                json_str = matches.group(0)
                search_results = json.loads(json_str)
                
                # Display the results
                results_container.remove_children()
                
                if not search_results.get("results"):
                    results_container.mount(Static("No results found.", classes="search-no-results"))
                    return
                
                # Display each result
                for idx, result in enumerate(search_results.get("results", [])):
                    file_path = result.get("file", "Unknown file")
                    snippet = result.get("snippet", "")
                    explanation = result.get("explanation", "")
                    
                    # Create a button for the file
                    result_container = Container(classes="search-result")
                    result_container.mount(Label(f"Result {idx+1}: {file_path}", classes="search-result-title"))
                    result_container.mount(Static(explanation, classes="search-result-explanation"))
                    result_container.mount(TextArea(snippet, language="python", classes="search-result-snippet", read_only=True))
                    result_container.mount(Button(f"Open {os.path.basename(file_path)}", id=f"open-result-{idx}", classes="search-result-open-btn"))
                    
                    results_container.mount(result_container)
            else:
                # No JSON found in the response
                results_container.remove_children()
                results_container.mount(Static("Failed to parse search results. Please try again.", classes="search-error"))
                
        except Exception as e:
            # Handle search errors
            results_container.remove_children()
            results_container.mount(Static(f"Search error: {str(e)}", classes="search-error"))

# Collaboration Session Dialog Screen
class CollaborationSessionDialog(ModalScreen):
    """Dialog for starting or joining a collaboration session"""
    
    def compose(self) -> ComposeResult:
        """Create the collaboration dialog layout"""
        with Container(id="collab-dialog"):
            yield Label("Real-time Collaboration", classes="title")
            
            with Vertical():
                with Horizontal():
                    yield Label("Username:", classes="label")
                    yield Input(placeholder="Your display name", id="collab-username-input")
                
                with Vertical():
                    yield Button("Create New Session", id="collab-create-btn", variant="primary")
                    
                    with Horizontal():
                        yield Label("Join Session:", classes="label")
                        yield Input(placeholder="Session ID", id="collab-session-id-input")
                        yield Button("Join", id="collab-join-btn")
                
                with Horizontal():
                    yield Button("Cancel", id="collab-cancel-btn", variant="error")
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        username_input = self.query_one("#collab-username-input", Input)
        username = username_input.value
        
        if not username:
            self.notify("Please enter a username", severity="warning")
            return
        
        if button_id == "collab-cancel-btn":
            self.dismiss()
        elif button_id == "collab-create-btn":
            # Create a new session
            self.dismiss({"action": "create", "username": username})
        elif button_id == "collab-join-btn":
            # Join an existing session
            session_id_input = self.query_one("#collab-session-id-input", Input)
            session_id = session_id_input.value
            
            if not session_id:
                self.notify("Please enter a session ID", severity="warning")
                return
                
            self.dismiss({"action": "join", "username": username, "session_id": session_id})


# Collaboration Screen
class CollaborationScreen(Screen):
    """Real-time collaboration interface screen"""
    
    def __init__(self, session_details: Dict[str, Any]):
        """
        Initialize the collaboration screen
        
        Args:
            session_details: Session details including username and session ID
        """
        super().__init__()
        self.session_details = session_details
        self.session_id = session_details.get("session_id", "")
        self.username = session_details.get("username", "")
        self.active_file = ""
        self.collaboration_status = "connecting"
        self.user_list = []
        
    def compose(self) -> ComposeResult:
        """Create the collaboration layout"""
        yield Header()
        
        with Horizontal():
            # Left panel: Users and chat
            with Vertical(id="collab-left-panel"):
                yield Label("Collaboration Session", classes="title")
                with Horizontal():
                    yield Label(f"Session ID: {self.session_id}", id="collab-session-id")
                    yield Button("Copy", id="collab-copy-id-btn")
                
                yield Label("Connected Users", classes="subtitle")
                yield ScrollableContainer(id="collab-users-container")
                
                yield Label("Chat", classes="subtitle")
                with Vertical(id="collab-chat-container"):
                    yield ScrollableContainer(id="collab-chat-messages")
                    with Horizontal():
                        yield Input(placeholder="Type a message...", id="collab-chat-input")
                        yield Button("Send", id="collab-chat-send-btn")
            
            # Right panel: Shared editor
            with Vertical(id="collab-editor-panel"):
                with Horizontal():
                    yield Label("Shared Editor", classes="title")
                    yield Label("Status: ", classes="label")
                    yield Label("Connecting...", id="collab-status")
                
                with Horizontal(id="collab-file-select"):
                    yield Label("File:", classes="label")
                    with Container(id="collab-file-dropdown-container"):
                        yield Input(placeholder="Select or open a file", id="collab-file-input")
                        yield Button("Open", id="collab-open-file-btn")
                
                yield TextArea(language="python", id="collab-editor")
                
                with Horizontal():
                    yield Button("Save", id="collab-save-btn")
                    yield Button("End Session", id="collab-end-btn", variant="error")
    
    def on_mount(self) -> None:
        """Set up the collaboration screen"""
        # Set up the users container
        users_container = self.query_one("#collab-users-container", ScrollableContainer)
        
        # Add self as first user
        users_container.mount(
            Static(f" {self.username} (You)", classes="collab-user-item self")
        )
        
        # Initialize the chat container
        chat_container = self.query_one("#collab-chat-messages", ScrollableContainer)
        chat_container.mount(
            Static(" Welcome to the collaboration session! You can chat with other users here.", 
                    classes="collab-chat-system")
        )
        
        # Set initial status
        self.update_status("Connecting to session...")
        
        # Connect to collaboration server
        self.connect_to_session()
    
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        
        if button_id == "collab-copy-id-btn":
            # Copy session ID to clipboard
            # (Textual doesn't have clipboard API, so just notify)
            self.notify(f"Session ID copied: {self.session_id}")
        elif button_id == "collab-chat-send-btn":
            # Send chat message
            self.send_chat_message()
        elif button_id == "collab-open-file-btn":
            # Open file for collaboration
            self.open_file_for_collaboration()
        elif button_id == "collab-save-btn":
            # Save changes
            self.save_collaborative_file()
        elif button_id == "collab-end-btn":
            # End collaboration session
            self.end_collaboration_session()
    
    def on_input_submitted(self, event: Input.Submitted) -> None:
        """Handle input submitted events"""
        input_id = event.input.id
        
        if input_id == "collab-chat-input":
            # Send chat message
            self.send_chat_message()
    
    @work
    async def connect_to_session(self) -> None:
        """Connect to the collaboration session"""
        try:
            # In a real app, this would connect to the WebSocket server
            await asyncio.sleep(1)  # Simulate connection
            
            self.update_status("Connected")
            
            # Add some fake users for demo
            self.add_user("Alice", "user1")
            self.add_user("Bob", "user2")
            
            # Add welcome system message
            chat_container = self.query_one("#collab-chat-messages", ScrollableContainer)
            chat_container.mount(
                Static(" Alice and Bob have joined the session.", 
                        classes="collab-chat-system")
            )
            
        except Exception as e:
            self.update_status(f"Connection error: {str(e)}")
    
    def update_status(self, status: str) -> None:
        """
        Update the collaboration status
        
        Args:
            status: New status message
        """
        status_label = self.query_one("#collab-status", Label)
        status_label.update(status)
        self.collaboration_status = status
    
    def add_user(self, username: str, client_id: str) -> None:
        """
        Add a user to the collaboration session
        
        Args:
            username: User's display name
            client_id: Client ID
        """
        users_container = self.query_one("#collab-users-container", ScrollableContainer)
        users_container.mount(
            Static(f" {username}", classes=f"collab-user-item {client_id}")
        )
        self.user_list.append({"username": username, "client_id": client_id})
    
    def remove_user(self, client_id: str) -> None:
        """
        Remove a user from the collaboration session
        
        Args:
            client_id: Client ID
        """
        users_container = self.query_one("#collab-users-container", ScrollableContainer)
        
        # Find and remove the user element
        for user_element in users_container.query(f".{client_id}"):
            user_element.remove()
        
        # Remove from user list
        self.user_list = [user for user in self.user_list if user["client_id"] != client_id]

    def send_chat_message(self) -> None:
        """Send a chat message"""
        chat_input = self.query_one("#collab-chat-input", Input)
        message = chat_input.value
        
        if not message:
            # Add notification for empty message
            self.notify("Please enter a message", severity="warning")
            return
            
        # Add message to chat container
        chat_container = self.query_one("#collab-chat-messages", ScrollableContainer)
        chat_container.mount(
            Static(f" You: {message}", classes="collab-chat-self")
        )
        
        # Clear input
        chat_input.value = ""
        
        # In a real app, this would send the message to the WebSocket server
        # For demo, add fake responses
        self.add_fake_chat_response(message)
    
    @work
    async def add_fake_chat_response(self, message: str) -> None:
        """
        Add a fake chat response for demo purposes
        
        Args:
            message: Original message
        """
        await asyncio.sleep(1)
        
        # Fake response
        chat_container = self.query_one("#collab-chat-messages", ScrollableContainer)
        
        if "hello" in message.lower():
            chat_container.mount(
                Static(" Alice: Hi there! How's the coding going?", classes="collab-chat-other")
            )
        elif "help" in message.lower():
            chat_container.mount(
                Static(" Bob: I can help with that! What do you need?", classes="collab-chat-other")
            )
        else:
            chat_container.mount(
                Static(" Alice: Interesting! Let's work on this together.", classes="collab-chat-other")
            )
    
    def open_file_for_collaboration(self) -> None:
        """Open a file for collaboration"""
        file_input = self.query_one("#collab-file-input", Input)
        file_path = file_input.value
        
        if not file_path:
            self.notify("Please enter a file path", severity="warning")
            return
            
        # Update active file
        self.active_file = file_path
        
        # Update editor with file contents (simulated)
        editor = self.query_one("#collab-editor", TextArea)
        
        # Simulate loading file
        editor.load_text(f"# Collaborative editing of {file_path}\n\ndef main():\n    print('Hello, Collaborators!')\n\nif __name__ == '__main__':\n    main()")
        
        # In a real app, this would load the file and sync it to all users
        self.notify(f"Opened {file_path} for collaboration")
    
    def save_collaborative_file(self) -> None:
        """Save the collaborative file"""
        if not self.active_file:
            self.notify("No file is currently active", severity="warning")
            return
            
        editor = self.query_one("#collab-editor", TextArea)
        content = editor.text
        
        # In a real app, this would save the file
        self.notify(f"Saved {self.active_file} successfully")
    
    def end_collaboration_session(self) -> None:
        """End the collaboration session"""
        # In a real app, this would notify all users and close the WebSocket connection
        self.notify("Ending collaboration session...")
        self.app.pop_screen()


# Collaboration User Presence Indicator Widget
class UserPresenceIndicator(Static):
    """Widget to show user cursor position in collaborative editing"""
    
    def __init__(self, username: str, client_id: str, color: str = "#3498db"):
        """
        Initialize the user presence indicator
        
        Args:
            username: User's display name
            client_id: Client ID
            color: User's color
        """
        super().__init__()
        self.username = username
        self.client_id = client_id
        self.color = color
        self.position = (0, 0)  # (line, column)
        
    def on_mount(self) -> None:
        """Set up the user presence indicator"""
        self.update_style()
        
    def update_position(self, position: tuple[int, int]) -> None:
        """
        Update the cursor position
        
        Args:
            position: New position (line, column)
        """
        self.position = position
        self.update_style()
        
    def update_style(self) -> None:
        """Update the indicator's style based on position"""
        # In a real app, this would calculate the actual UI position from editor coordinates
        self.styles.background = self.color
        self.styles.color = "#ffffff"
        self.update(f" {self.username}")
        
    def render(self) -> str:
        """Render the indicator"""
        return f" {self.username}"

# Command Palette Screen
class CommandPalette(ModalScreen):
    """Command palette screen for quick access to commands"""
    
    def __init__(self):
        super().__init__()
        self.commands = {
            "Save": "save",
            "Open": "open",
            "Run": "run",
            "Format Code": "format_code",
            "Git Commit": "git_commit",
            "Git Pull": "git_pull",
            "Git Push": "git_push",
            "Toggle Split View": "toggle_split_view",
            "Switch Editor": "switch_editor",
            "Toggle Terminal": "toggle_terminal",
            "Analyze Code": "analyze_code",
            "AI Request": "ai_request",
            "Quit": "quit"
        }
    
    def compose(self) -> ComposeResult:
        """Create the command palette layout"""
        with Container(id="command-palette"):
            yield Label("Command Palette", classes="title")
            yield Input(
                placeholder="Search commands...",
                id="command-search",
                suggester=SuggestFromList(list(self.commands.keys()))
            )
            yield ScrollableContainer(id="command-list")
            
    def on_mount(self):
        """Called when screen is mounted"""
        # Focus the search box
        self.query_one("#command-search").focus()
        
        # Display all commands initially
        self.display_commands(list(self.commands.keys()))
        
    
    def display_commands(self, commands: List[str]):
        """Display a filtered list of commands"""
        command_list = self.query_one("#command-list")
        command_list.remove_children()
        
        for command in commands:
            command_list.mount(Button(command, id=f"cmd-{self.commands[command]}"))
    
    def on_input_changed(self, event: Input.Changed) -> None:
        """Handle input changes to filter commands"""
        search_text = event.value.lower()
        
        # Filter commands
        if search_text:
            filtered_commands = [
                cmd for cmd in self.commands.keys()
                if search_text in cmd.lower()
            ]
        else:
            filtered_commands = list(self.commands.keys())
            
        # Update the display
        self.display_commands(filtered_commands)
        
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle command selection"""
        button_id = event.button.id
        
        if button_id.startswith("cmd-"):
            command = button_id[4:]  # Remove "cmd-" prefix
            self.app.action(command)
            self.app.pop_screen()

# Main application class
class TerminatorApp(App):
    """
    Terminator - A terminal-based Python IDE with AI superpowers
    """
    
    # Event classes
    class CodeAnalysisComplete(Message):
        """Event fired when code analysis is complete"""
        def __init__(self, analysis_result: str) -> None:
            self.analysis_result = analysis_result
            super().__init__()
    
    # CSS section for the app
    
    CSS = """
        /* Panel layout */
        #main-layout {
            height: 100%;
        }
        
        #sidebar {
            width: 20%;
            min-width: 20;
            max-width: 40%;
            background: $surface-darken-1;
        }
        
        #editor-container {
            width: 60%;
            min-width: 30%;
            max-width: 80%;
        }
        
        #ai-panel {
            width: 20%;
            min-width: 15%;
            max-width: 40%;
        }
        
        /* Gutter styles for resizing */
        .gutter {
            width: 1;
            background: $accent-darken-2;
            color: $text-muted;
            text-align: center;
            transition: background 0.1s;
        }
        
        .gutter:hover {
            background: $accent;
            color: $text;
        }
        
        /* Real-time Collaboration Styles */
        #collab-dialog {
            background: $surface;
            padding: 1;
            border: solid $accent;
            min-width: 50%;
            max-width: 80%;
            margin: 0 1;
        }
        
        #collab-left-panel {
            width: 30%;
            min-width: 20;
            background: $surface-darken-1;
            padding: 1;
        }
        
        #collab-editor-panel {
            width: 70%;
            padding: 1;
        }
        
        #collab-users-container {
            height: 20%;
            border: solid $panel-darken-1;
            padding: 1;
            margin-bottom: 1;
        }
        
        #collab-chat-messages {
            height: 60%;
            border: solid $panel-darken-1;
            padding: 1;
            margin-bottom: 1;
        }
        
        #collab-editor {
            height: 80%;
            border: solid $panel-darken-1;
        }
        
        .collab-user-item {
            padding: 1 2;
            margin: 1 0;
            border: solid $accent;
        }
        
        .collab-user-item.self {
            background: $accent-darken-2;
            color: $text;
        }
        
        .collab-chat-system {
            color: $text-muted;
            padding: 1;
        }
        
        .collab-chat-self {
            background: $accent-darken-2;
            padding: 1 2;
            margin: 1 0;
            border: solid $accent;
            text-align: right;
        }
        
        .collab-chat-other {
            background: $panel-darken-2;
            padding: 1 2;
            margin: 1 0;
            border: solid $panel-darken-1;
        }
        
        #ai-panel {
            width: 30%;
            min-width: 30;
            border-left: solid $primary;
        }
        
        #file-explorer {
            height: 60%;
            border-bottom: solid $primary;
        }
        
        #git-status {
            height: 40%;
        }
        
        #editor-split-view {
            height: 100%;
        }
        
        #editor-primary {
            height: 100%;
            width: 100%;
        }
        
        #editor-secondary {
            height: 100%;
            width: 50%;
            border-left: solid $primary;
        }
        
        #editor-secondary.hidden {
            display: none;
        }
        
        .split-view #editor-primary {
            width: 50%;
        }
        
        .multi-cursor .cursor {
            background: $accent;
        }
        
        #editor-tabs {
            height: 3;
        }
        
        #editor-content, #terminal-content {
            height: 100%;
        }
        
        #terminal-content.hidden {
            display: none;
        }
        
        #terminal-output {
            height: 85%;
            background: $surface-darken-2;
            color: $text;
        }
        
        #terminal-input {
            width: 80%;
        }
        
        #terminal-execute-btn {
            width: 20%;
            background: $success;
        }
        
        #ai-output {
            height: 75%;
            border-bottom: solid $primary;
            overflow-y: scroll;
        }
        
        #ai-input {
            height: 25%;
        }
        
        .title {
            background: $primary;
            color: $text;
            padding: 1 2;
            text-align: center;
            text-style: bold;
        }
        
        Button {
            margin: 1 1;
        }
        
        #run-btn {
            background: $success;
        }
        
        #ai-submit {
            background: $accent;
        }
        
        /* Modal dialog styling */
        #commit-dialog, #analysis-dialog, #command-palette {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 60%;
            width: 60%;
            margin: 2 2;
        }
        
        #command-palette {
            height: 50%;
            width: 50%;
        }
        
        #command-search {
            margin-bottom: 1;
            border: solid $primary;
        }
        
        #command-list {
            height: 100%;
            overflow-y: auto;
        }
        
        #command-list Button {
            width: 100%;
            margin: 0 0 1 0;
            text-align: left;
        }
        
        /* Debugger styling */
        #debug-code-panel {
            width: 60%;
            height: 100%;
        }
        
        #debug-info-panel {
            width: 40%;
            height: 100%;
            border-left: solid $primary;
        }
        
        #debug-code {
            height: 80%;
        }
        
        #debug-variables, #debug-stack {
            height: 30%;
            margin-bottom: 1;
        }
        
        #debug-output {
            height: 35%;
        }
        
        .current-debug-line {
            background: $accent-lighten-2;
        }
        
        .breakpoint-line {
            background: $error-lighten-2;
        }
        
        /* Branch visualization styling */
        #branch-tree-panel {
            width: 60%;
            height: 100%;
        }
        
        #branch-info-panel {
            width: 40%;
            height: 100%;
            border-left: solid $primary;
        }
        
        #branch-graph {
            height: 85%;
            background: $surface-darken-2;
        }
        
        #all-branches-container, #recent-commits-container {
            height: 30%;
            margin-bottom: 1;
            overflow-y: auto;
        }
        
        .branch-button {
            background: $success-darken-1;
            margin: 0 0 1 0;
            width: 100%;
            text-align: left;
        }
        
        .remote-branch-button {
            background: $accent-darken-1;
            margin: 0 0 1 0;
            width: 100%;
            text-align: left;
        }
        
        .commit-button {
            background: $surface-darken-1;
            margin: 0 0 1 0;
            width: 100%;
            text-align: left;
        }
        
        /* Remote development styling */
        #remote-dialog {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 70%;
            width: 60%;
            margin: 2 2;
        }
        
        #remote-files-panel {
            width: 60%;
            height: 100%;
        }
        
        #remote-preview-panel {
            width: 40%;
            height: 100%;
            border-left: solid $primary;
        }
        
        #remote-files-tree {
            height: 80%;
            margin-bottom: 1;
            overflow-y: auto;
        }
        
        #remote-file-preview {
            height: 95%;
        }
        
        /* Semantic search styling */
        #semantic-search-dialog {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 80%;
            width: 80%;
            margin: 2 2;
        }
        
        #semantic-search-input {
            margin-bottom: 1;
            border: solid $primary;
        }
        
        #semantic-results-container {
            height: 85%;
            margin-bottom: 1;
            overflow-y: auto;
        }
        
        .search-result {
            margin-bottom: 2;
            border: solid $primary;
            padding: 1;
        }
        
        .search-result-title {
            background: $primary-darken-1;
            padding: 0 1;
            margin-bottom: 1;
        }
        
        .search-result-explanation {
            margin-bottom: 1;
            padding: 0 1;
            color: $text-muted;
        }
        
        .search-result-snippet {
            height: 10;
            margin-bottom: 1;
            background: $surface-darken-1;
        }
        
        .search-result-open-btn {
            background: $success;
        }
        
        .search-loading {
            text-align: center;
            margin-top: 2;
            color: $text-muted;
        }
        
        .search-error {
            text-align: center;
            margin-top: 2;
            color: $error;
        }
        
        /* Diff view styling */
        #diff-view-container {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 90%;
            width: 95%;
            margin: 1 2;
        }
        
        #diff-split-view {
            height: 80%;
            margin-bottom: 1;
        }
        
        #diff-original-panel {
            width: 50%;
            height: 100%;
            border-right: solid $primary;
            padding-right: 1;
        }
        
        #diff-modified-panel {
            width: 50%;
            height: 100%;
            padding-left: 1;
        }
        
        #unified-diff-panel {
            height: 30%;
            margin-top: 1;
            border-top: solid $primary;
            padding-top: 1;
        }
        
        #diff-buttons {
            margin-top: 1;
            height: 3;
            align: center middle;
        }
        
        #diff-original-content .line-deleted {
            background: rgba(255, 0, 0, 0.2);
            color: $error;
        }
        
        #diff-modified-content .line-added {
            background: rgba(0, 255, 0, 0.2);
            color: $success;
        }
        
        #diff-title {
            text-align: center;
            background: $primary;
            color: $text;
            margin-bottom: 1;
        }
        
        .subtitle {
            background: $primary-darken-2;
            color: $text;
            text-align: center;
            margin-bottom: 1;
        }
        
        /* Git commit dialog styling */
        #commit-header {
            width: 100%;
            margin-bottom: 1;
        }
        
        .escape-hint {
            color: $text-muted;
            text-align: right;
            padding-right: 1;
            width: 50%;
        }
        
        /* Resizable panel styling */
        #main-layout {
            width: 100%;
            height: 100%;
        }
        
        #sidebar {
            width: 20%;
            min-width: 10%;
            max-width: 40%;
        }
        
        #editor-container {
            width: 60%;
            min-width: 30%;
            max-width: 80%;
        }
        
        #ai-panel {
            width: 20%;
            min-width: 10%;
            max-width: 40%;
        }
        
        .panel {
            overflow: auto;
        }
        
        .gutter {
            background: $primary;
            color: $text;
            width: 1;
            text-align: center;
            margin: 0 1;
        }
                
        .search-no-results {
            text-align: center;
            margin-top: 2;
            color: $warning;
        }
        
        #commit-message {
            height: 80%;
            margin: 1;
        }
        
        #analysis-result {
            height: 85%;
            margin: 1;
            overflow-y: scroll;
        }
        
        /* Status indicators */
        .git-status {
            color: $success;
        }
        
        .git-modified {
            color: $warning;
        }
        
        .git-untracked {
            color: $error;
        }
        
        /* AI styling */
        .ai-thinking {
            color: $accent;
            text-style: italic;
        }
        
        .ai-response {
            border-left: solid $primary;
            padding-left: 1;
        }
        
        .code-block {
            background: $surface-darken-2;
            margin: 1;
            padding: 1;
        }
        /* Add these to your CSS section */

        #theme-dialog {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 60%;
            width: 40%;
            margin: 2 2;
        }

        #theme-list {
            height: 80%;
            overflow-y: auto;
        }

        .theme-button {
            width: 100%;
            margin: 0 0 1 0;
            text-align: left;
        }
    """
    
    BINDINGS = [
        Binding("ctrl+s", "save", "Save"),
        Binding("ctrl+o", "open", "Open"),
        Binding("f5", "run", "Run"),
        Binding("ctrl+q", "quit", "Quit"),
        Binding("ctrl+g", "git_commit", "Git"),
        Binding("ctrl+r", "ai_request", "AI"),
        Binding("ctrl+d", "add_multi_cursor", "Multi-cursor"),
        Binding("ctrl+k", "toggle_split_view", "Split View"),
        Binding("ctrl+tab", "switch_editor", "Switch Editor"),
        Binding("ctrl+t", "toggle_terminal", "Terminal"),
        Binding("ctrl+p", "show_command_palette", "Command Palette"),
        Binding("ctrl+space", "code_completion", "AI Code Completion"),
        Binding("f9", "toggle_breakpoint", "Toggle Breakpoint"),
        Binding("f10", "debug_current_file", "Debug"),
        Binding("ctrl+b", "show_branch_visualization", "Git Branches"),
        Binding("ctrl+shift+p", "toggle_pair_programming", "AI Pair Programming"),
        Binding("ctrl+shift+r", "connect_remote", "Remote Development"),
        Binding("ctrl+shift+c", "start_collaboration", "Collaborate"),
    ]
    
    # Track application state
    current_file = reactive(None)
    git_repository = reactive(None)
    editor_theme = reactive("monokai")  # Rename to editor_theme to avoid conflict with built-in current_theme
    
    # Add these variables to your class
    _last_status_update_time = 0
    _status_update_debounce = 0.5  # seconds

    # Define screens
    SCREENS = {
        "commit": CommitDialog,
        "analysis": CodeAnalysisDialog,
        "command_palette": CommandPalette,
        "debugger": DebuggerScreen,
        "branch_visualization": BranchVisualizationScreen,
        "remote_connection": RemoteConnectionDialog,
        "remote_browser": RemoteFilesBrowser,
        "theme_selection": ThemeSelectionScreen,
        "diff_view": DiffViewScreen
    }

    def compose(self) -> ComposeResult:
        """Create the UI layout with resizable panels"""
        yield Header()
        
        # Main layout with resizable panels using the gutter parameter
        # This horizontal container will have resizable children
        with Horizontal(id="main-layout"):
            # Left sidebar with file explorer and git status - initially 20% width
            with Vertical(id="sidebar", classes="panel"):
                yield Label("File Explorer", classes="title")
                yield DirectoryTree(".", id="file-explorer")
                
                yield Label("Git Status", classes="title")
                with Vertical(id="git-status"):
                    yield Static("", id="git-output")
                    with Horizontal():
                        yield Button("Commit", id="commit-btn")
                        yield Button("Pull", id="pull-btn")
                        yield Button("Push", id="push-btn")
                        yield Button("Branches", id="branches-btn")
            
            # Resizer element between sidebar and editor
            yield Static("|", classes="gutter")
            
            # Center code editor and terminal - initially 60% width
            with Vertical(id="editor-container", classes="panel"):
                # Use TabPane directly instead of add_pane method
                with TabbedContent(id="editor-tabs"):
                    with TabPane("Editor", id="editor-tab-pane"):
                        yield Static(id="editor-tab")
                    with TabPane("Terminal", id="terminal-tab-pane"):
                        yield Static(id="terminal-tab")
                
                # Editor Tab
                with Container(id="editor-content"):
                    yield Label("Code Editor", classes="title")
                    with Horizontal(id="editor-split-view"):
                        # Check if we have syntax extras before trying to use code_editor
                        try:
                            import tree_sitter_languages
                            # If syntax highlighting is available, use code_editor
                            yield TextArea.code_editor(
                                language="python", 
                                theme="monokai", 
                                show_line_numbers=True,
                                tab_behavior="indent",
                                id="editor-primary"
                            )
                            yield TextArea.code_editor(
                                language="python", 
                                theme="monokai", 
                                show_line_numbers=True,
                                tab_behavior="indent",
                                id="editor-secondary", 
                                classes="hidden"
                            )
                        except ImportError:
                            # Fall back to standard TextArea if syntax highlighting isn't available
                            yield TextArea(
                                language="python", 
                                id="editor-primary"
                            )
                            yield TextArea(
                                language="python", 
                                id="editor-secondary", 
                                classes="hidden"
                            )
                    with Horizontal():
                        yield Button("Run", id="run-btn")
                        yield Button("Debug", id="debug-btn")
                        yield Button("Save", id="save-btn")
                        yield Button("Format", id="format-btn")
                        yield Button("Split View", id="split-view-btn")
                        yield Button("Theme", id="theme-btn")
                        yield Button("Pair Program", id="pair-program-btn", variant="primary")

                # Terminal Tab
                with Container(id="terminal-content", classes="hidden"):
                    yield Label("Integrated Terminal", classes="title")
                    yield TextArea(language="bash", id="terminal-output", read_only=True)
                    with Horizontal():
                        yield Input(placeholder="Enter terminal command...", id="terminal-input")
                        yield Button("Execute", id="terminal-execute-btn")
                        yield Button("Remote", id="open-remote-btn")
            
            # Resizer element between editor and AI panel
            yield Static("|", classes="gutter")
            
            # Right AI panel - initially 20% width
            with Vertical(id="ai-panel", classes="panel"):
                yield Label("AI Assistant", classes="title")
                yield Markdown("Welcome to Terminator v1! Ask me anything about your code.", id="ai-output")
                with Vertical(id="ai-input"):
                    yield Input(placeholder="Ask the AI...", id="ai-prompt")
                    yield Button("Submit", id="ai-submit")
        
        yield Footer()
    
    def get_language_from_extension(self, extension):
        """Map file extension to language for syntax highlighting"""
        extension_map = {
            ".py": "python",
            ".js": "javascript",
            ".html": "html",
            ".css": "css",
            ".json": "json",
            ".md": "markdown",
            ".markdown": "markdown",
            ".txt": "text",
            ".xml": "xml",
            ".yaml": "yaml",
            ".yml": "yaml",
            ".toml": "toml",
            ".rs": "rust",
            ".go": "go",
            ".sh": "bash",
            ".bash": "bash",
            ".sql": "sql",
            ".java": "java",
            # Add more mappings as needed
        }
        return extension_map.get(extension, "text")
        
    def set_editor_theme(self, theme_name: str) -> None:
        """Set the theme for all editors"""
        try:
            # Apply theme to primary editor
            primary_editor = self.query_one("#editor-primary")
            primary_editor.theme = theme_name
            
            # Apply theme to secondary editor if it exists
            try:
                secondary_editor = self.query_one("#editor-secondary")
                secondary_editor.theme = theme_name
            except Exception:
                pass  # Secondary editor might not exist yet
            
            # Update the reactive current_theme
            
            self.notify(f"Theme changed to: {theme_name}", severity="information")
        except Exception as e:
            self.notify(f"Error setting theme: {str(e)}", severity="error")
            
    def show_diff_view(self, original_content: str, modified_content: str, 
                    title: str = "Code Changes", language: str = "python",
                    original_title: str = "Original", modified_title: str = "Modified") -> None:
        """
        Show the diff view popup for comparing original and modified content
        
        Args:
            original_content: The original content
            modified_content: The modified content
            title: Title for the diff view
            language: Language for syntax highlighting
            original_title: Title for the original content panel
            modified_title: Title for the modified content panel
        """
        try:
            # Create a diff view screen with the provided content
            diff_screen = DiffViewScreen(
                original_content=original_content,
                modified_content=modified_content,
                title=title,
                original_title=original_title,
                modified_title=modified_title,
                highlight_language=language
            )
            
            # Push the screen
            self.push_screen(diff_screen)
            
        except Exception as e:
            self.notify(f"Error showing diff view: {str(e)}", severity="error")
            logging.error(f"Error showing diff view: {str(e)}", exc_info=True)
            
    def show_code_suggestion(self, original_content: str, new_content: str, title: str = "AI Suggested Changes") -> None:
        """
        Show code suggestions from the AI agent with a diff view
        
        Args:
            original_content: The original file content
            new_content: The suggested new content
            title: Title for the diff view popup
        """
        try:
            # Create a callback that will be called when the user clicks "Apply Changes"
            def on_apply_callback(content):
                self.apply_diff_changes(content)
            
            # Create a diff view screen with the provided content
            diff_screen = DiffViewScreen(
                original_content=original_content,
                modified_content=new_content,
                title=title,
                original_title="Current Code",
                modified_title="AI Suggestion",
                highlight_language=self._get_language_from_filename(self.current_file) if self.current_file else "python",
                on_apply_callback=on_apply_callback
            )
            
            # Push the screen
            self.push_screen(diff_screen)
            
            # Show a notification about the suggestion
            self.notify("AI has suggested changes. Review and apply if desired.", severity="information")
            
        except Exception as e:
            self.notify(f"Error showing code suggestion: {str(e)}", severity="error")
            logging.error(f"Error showing code suggestion: {str(e)}", exc_info=True)
    
    def _get_language_from_filename(self, filename: str) -> str:
        """
        Get the language for syntax highlighting based on file extension
        
        Args:
            filename: The filename to check
            
        Returns:
            Language identifier for syntax highlighting
        """
        if not filename:
            return "python"
            
        ext = os.path.splitext(filename)[1].lower()
        
        language_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".jsx": "javascript",
            ".tsx": "typescript",
            ".html": "html",
            ".css": "css",
            ".json": "json",
            ".md": "markdown",
            ".xml": "xml",
            ".yaml": "yaml",
            ".yml": "yaml",
            ".sh": "bash",
            ".c": "c",
            ".cpp": "cpp",
            ".java": "java",
            ".go": "go",
            ".rs": "rust"
        }
        
        return language_map.get(ext, "python")
            
    def apply_diff_changes(self, new_content: str) -> None:
        """
        Apply changes from diff view to the current file
        
        Args:
            new_content: The new content to apply
        """
        if not self.current_file:
            self.notify("No file selected to save changes to", severity="error")
            return
            
        try:
            # Get the active editor
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")
                
            # Update the editor content
            if hasattr(editor, "load_text"):
                editor.load_text(new_content)
            else:
                editor.text = new_content
                
            # Save the changes to the file
            with open(self.current_file, "w", encoding="utf-8") as file:
                file.write(new_content)
                
            self.notify(f"Changes applied and saved to {os.path.basename(self.current_file)}", severity="success")
            
            # Update git status if applicable
            if hasattr(self, "git_repository") and self.git_repository:
                asyncio.create_task(self.update_git_status())
                
        except Exception as e:
            self.notify(f"Error applying changes: {str(e)}", severity="error")
            logging.error(f"Error applying changes: {str(e)}", exc_info=True)
    
    def ensure_syntax_dependencies(self):
        """Ensure the syntax highlighting dependencies are installed"""
        try:
            # Try to import tree_sitter_languages, which is part of textual[syntax]
            import importlib
            importlib.import_module('tree_sitter_languages')
            return True
        except ImportError:
            # Don't try to highlight the word 'syntax' in the notification
            self.notify("Syntax highlighting requires textual[syntax] package", severity="warning")
            return False
    
    def initialize_agent_context(self):
        """Initialize the agent context with proper validation"""
        try:
            # Ensure current_directory is valid
            if not hasattr(self, "current_directory") or not self.current_directory:
                self.current_directory = os.getcwd()
                logging.info(f"Set default current directory to: {self.current_directory}")
            
            # Validate the directory exists
            if not os.path.exists(self.current_directory):
                logging.error(f"Current directory doesn't exist: {self.current_directory}")
                self.notify(f"Invalid working directory: {self.current_directory}", severity="error")
                self.current_directory = os.getcwd()
                logging.info(f"Falling back to current working directory: {self.current_directory}")
            
            # Create agent context with validated directory
            self.agent_context = AgentContext(current_dir=self.current_directory)
            logging.info(f"Agent context initialized with directory: {self.current_directory}")
        
            # Verify context was created correctly
            if not self.agent_context or not hasattr(self.agent_context, 'current_dir'):
                logging.error("Failed to create valid agent context")
                self.notify("Failed to initialize AI agent context", severity="error")
                return False
            
            return True
        
        except Exception as e:
            logging.error(f"Error initializing agent context: {str(e)}", exc_info=True)
            self.notify(f"Error initializing AI agent context: {str(e)}", severity="error")
            return False

    def on_mount(self):
        """Called when the app is mounted"""
        # Set up initial directory
        self.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        self.active_editor = "primary"
        self.split_view_active = False
        self.multi_cursor_positions = []
        self.active_tab = "editor"
        self.terminal_history = []
        
        # Initialize resizable panel tracking
        self.resizing = False
        self.resizing_panel = None
        self.start_x = 0
        self.current_widths = {
            "sidebar": 20,           # Default sidebar width 20%
            "editor-container": 60,  # Default editor width 60%
            "ai-panel": 20           # Default AI panel width 20%
        }
        
        # Check for syntax highlighting dependencies
        self.ensure_syntax_dependencies()
        
        # Initialize debugger state
        self.debug_session = None
        self.breakpoints = {}  # Format: {file_path: [line_numbers]}
        
        # Initialize AI pair programming state
        self.pair_programming_active = False
        self.pair_programming_timer = None
        self.last_edit_time = time.time()
        
        # Initialize remote development state
        self.remote_connected = False
        self.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # Initialize logging
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler("terminator.log"),
                logging.StreamHandler()
            ]
        )
        
        # Initialize the agent system
        self.notify("Initializing AI agents...")
        agent_initialized = initialize_agent_system()
        if agent_initialized:
            self.notify("AI agents ready", severity="information")
        else:
            self.notify("Failed to initialize AI agents. Check OpenAI API key.", severity="error")
        
        # Initialize agent context
        if self.initialize_agent_context():
            self.notify("AI agent context ready", severity="information")
        else:
            self.notify("Failed to initialize AI agent context - some AI features may not work correctly", severity="error")
        
        # Focus the file explorer by default
        self.query_one("#file-explorer").focus()
        
        # Check for git repository
        self.check_git_repository()
    
        # Initialize AI panel
        self.initialize_ai_panel()
        
        # Apply initial panel widths
        self._apply_panel_widths()

    async def on_input_submitted(self, event: Input.Submitted) -> None:
        """Handle input submission events for all inputs in the app"""
        input_id = event.input.id
        
        if input_id == "ai-prompt":
            # Execute AI request when Enter is pressed in the prompt input
            asyncio.create_task(self.action_ai_request())
        elif input_id == "terminal-input":
            # Execute terminal command when Enter is pressed
            asyncio.create_task(self.execute_terminal_command())
        elif input_id == "command-search":
            # Handle command palette search submission
            commands = self.screen.query_one("#command-list").query(Button)
            if commands:
                # Execute the first command in the filtered list
                command_id = commands[0].id
                if command_id.startswith("cmd-"):
                    command = command_id[4:]  # Remove "cmd-" prefix
                    self.action(command)
                    self.pop_screen()

    def check_git_repository(self):
        """Check if the current directory is a git repository"""
        is_repo, repo_root = GitManager.check_git_repo(self.current_directory)
        
        if is_repo:
            self.git_repository = repo_root
            self.update_git_status()
        else:
            self.git_repository = None
            git_output = self.query_one("#git-output")
            git_output.update("No Git repository found")
    
    def _apply_panel_widths(self):
        """Apply the current panel widths to the UI"""
        try:
            # Apply widths to each panel
            sidebar = self.query_one("#sidebar")
            editor = self.query_one("#editor-container")
            ai_panel = self.query_one("#ai-panel")
            
            sidebar.styles.width = f"{self.current_widths['sidebar']}%"
            editor.styles.width = f"{self.current_widths['editor-container']}%"
            ai_panel.styles.width = f"{self.current_widths['ai-panel']}%"
        except Exception as e:
            logging.error(f"Error applying panel widths: {str(e)}", exc_info=True)
    
    async def on_static_click(self, event) -> None:
        """Handle click events on static elements, including gutters"""
        if event.static.has_class("gutter"):
            # Determine which panel is being resized
            if event.static.query_one("#sidebar", default=None) is not None:
                self.resizing_panel = "sidebar"
            elif event.static.query_one("#editor-container", default=None) is not None:
                self.resizing_panel = "editor-container"
            else:
                self.resizing_panel = None
                return
            
            # Start resizing
            self.resizing = True
            self.start_x = event.screen_x
    
    async def on_mouse_down(self, event: MouseDown) -> None:
        """Handle mouse down events for gutter resizing"""
        # Check if we clicked on a gutter element
        target = self.get_widget_at(event.screen_x, event.screen_y)
        
        if target and isinstance(target, Static) and target.has_class("gutter"):
            # Find the adjacent panels for this gutter
            gutter_idx = list(self.query(".gutter")).index(target)
            
            if gutter_idx == 0:
                # First gutter - between sidebar and editor
                self.resizing_panel = "sidebar"
            elif gutter_idx == 1:
                # Second gutter - between editor and AI panel
                self.resizing_panel = "editor-container"
            
            # Start resizing
            self.resizing = True
            self.start_x = event.screen_x
            
            # Capture the mouse to receive events outside the gutter
            self.capture_mouse()
    
    async def on_mouse_up(self, event: MouseUp) -> None:
        """Handle mouse up events to stop resizing"""
        if self.resizing:
            self.resizing = False
            self.resizing_panel = None
            
            # Release the mouse capture
            self.release_mouse()
    
    async def on_mouse_move(self, event: MouseMove) -> None:
        """Handle mouse move events for panel resizing"""
        if not self.resizing or not self.resizing_panel:
            return
            
        # Calculate movement
        delta_x = event.screen_x - self.start_x
        if delta_x == 0:
            return
            
        # Convert to percentage of total width based on app width
        app_width = self.size.width
        delta_percent = (delta_x / app_width) * 100
        
        # Update panel widths with constraints
        if self.resizing_panel == "sidebar":
            # Resizing sidebar affects editor width
            new_sidebar_width = self.current_widths["sidebar"] + delta_percent
            new_editor_width = self.current_widths["editor-container"] - delta_percent
            
            # Apply constraints
            if 10 <= new_sidebar_width <= 40 and 30 <= new_editor_width <= 80:
                self.current_widths["sidebar"] = new_sidebar_width
                self.current_widths["editor-container"] = new_editor_width
                
                # Apply new widths
                sidebar = self.query_one("#sidebar")
                editor = self.query_one("#editor-container")
                sidebar.styles.width = f"{new_sidebar_width}%"
                editor.styles.width = f"{new_editor_width}%"
                
        elif self.resizing_panel == "editor-container":
            # Resizing editor affects AI panel width
            new_editor_width = self.current_widths["editor-container"] + delta_percent
            new_ai_width = self.current_widths["ai-panel"] - delta_percent
            
            # Apply constraints
            if 30 <= new_editor_width <= 80 and 15 <= new_ai_width <= 40:
                self.current_widths["editor-container"] = new_editor_width
                self.current_widths["ai-panel"] = new_ai_width
                
                # Apply new widths
                editor = self.query_one("#editor-container")
                ai_panel = self.query_one("#ai-panel")
                editor.styles.width = f"{new_editor_width}%"
                ai_panel.styles.width = f"{new_ai_width}%"
        
        # Update the start position for the next move
        self.start_x = event.screen_x
    
    def get_widget_at(self, x: int, y: int):
        """
        Get the widget at a specific screen coordinate
        
        Args:
            x: The x screen coordinate
            y: The y screen coordinate
            
        Returns:
            The widget at the given coordinates, or None if no widget is found
        """
        # Convert screen coordinates to app coordinates
        app_x = x 
        app_y = y
        
        # Find the widget at the given position
        for widget in self.query("*"):
        # Get widget's region
            region = widget.region
            if region and region.contains(app_x, app_y):
            # Calculate offset within the widget
                offset = (app_x - region.x, app_y - region.y)
                return widget, offset
    
        return None, (0, 0)

    def initialize_ai_panel(self):
        """Initialize AI panel elements"""
        try:
            ai_prompt = self.query_one("#ai-prompt")
            ai_submit = self.query_one("#ai-submit")
        
            # Make sure the input can receive focus
            ai_prompt.can_focus = True
            
            # Make sure the button can be clicked and has the right styling
            ai_submit.can_focus = True
            ai_submit.variant = "primary"
            
            # Log successful initialization
            logging.info("AI panel initialized successfully")
        except Exception as e:
            logging.error(f"Error initializing AI panel: {str(e)}", exc_info=True)
    
    @work
    async def update_git_status(self):
        """Update the git status display"""
        current_time = time.time()
        if current_time - self._last_status_update_time < self._status_update_debounce:
            return
        self._last_status_update_time = current_time
        if not self.git_repository:
            return
            
        git_output = self.query_one("#git-output")
        git_output.update("Checking Git status...")
        
        try:
            # Use GitManager to get status
            status = GitManager.get_git_status(self.git_repository)
            
            if "error" in status:
                git_output.update(f"Git error: {status['error']}")
                return
                
            if status.get("clean", False):
                git_output.update("Working tree clean")
                return
            
            # Format the status output
            status_text = ""
            
            if status.get("modified_files"):
                status_text += " Modified files:\n"
                for file in status["modified_files"]:
                    status_text += f"  {file}\n"
                    
            if status.get("untracked_files"):
                if status_text:
                    status_text += "\n"
                status_text += " Untracked files:\n"
                for file in status["untracked_files"]:
                    status_text += f"  {file}\n"
                    
            if status.get("staged_files"):
                if status_text:
                    status_text += "\n"
                status_text += " Staged files:\n"
                for file in status["staged_files"]:
                    status_text += f"  {file}\n"
                    
            git_output.update(status_text)
            
        except Exception as e:
            git_output.update(f"Error: {str(e)}")
            
    @work
    async def git_commit(self, message: str):
        """Commit changes to the Git repository"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return
            
        try:
            # First add all changes
            result = subprocess.run(
                ["git", "add", "."],
                capture_output=True,
                text=True,
                cwd=self.git_repository
            )
            
            if result.returncode != 0:
                self.notify(f"Git add failed: {result.stderr}", severity="error")
                return
                
            # Then commit with GitManager
            success, result_msg = GitManager.git_commit(self.git_repository, message)
            
            if success:
                self.notify("Changes committed successfully", severity="success")
                # Update status after commit
                self.update_git_status()
            else:
                self.notify(f"Commit failed: {result_msg}", severity="error")
                
        except Exception as e:
            self.notify(f"Error making commit: {str(e)}", severity="error")
    
    async def on_directory_tree_file_selected(self, event: DirectoryTree.FileSelected):
        """Handle file selection in the directory tree"""
        try:
            # Safely extract the path using proper error handling
            path = getattr(event, "path", None)
            if not path:
                # Try to access path through event.node.data (for newer Textual versions)
                node = getattr(event, "node", None)
                if node:
                    path = getattr(node, "data", None)
                    
            # If still no path, use event as fallback (some Textual versions pass path directly)
            if not path and isinstance(event, (str, Path)):
                path = str(event)
                
            # Last resort - try to get path from the message itself
            if not path and hasattr(event, "_Message__data"):
                data = getattr(event, "_Message__data", {})
                path = data.get("path", None)
                
            if not path:
                raise ValueError("Could not determine file path from event")
                
            # Store the path and update window title
            self.current_file = path
            self.title = f"Terminator - {path}"
            
            # Load the file content
            with open(path, "r", encoding="utf-8") as file:
                content = file.read()
                
            # Get file extension for language detection
            extension = os.path.splitext(path)[1].lower()
            language = self.get_language_from_extension(extension)
                
            # Update the active editor
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")
                
            # Set the language and content
            editor.language = language
            
            # Use load_text if available (newer Textual versions)
            if hasattr(editor, "load_text"):
                editor.load_text(content)
            else:
                # Fallback to direct text assignment
                editor.text = content
            
            # Apply the current theme
            if hasattr(self, "current_theme"):
                editor.theme = self.current_theme
            elif hasattr(self, "editor_theme"):
                editor.theme = self.editor_theme
            
            # Focus the editor
            editor.focus()
            
            # Add the file to recent files list if we maintain one
            if hasattr(self, "recent_files") and isinstance(self.recent_files, list):
                if path in self.recent_files:
                    self.recent_files.remove(path)
                self.recent_files.insert(0, path)
                # Keep list at reasonable size
                self.recent_files = self.recent_files[:10]
            
            # Notify about the detected language
            self.notify(f"File opened with {language} highlighting", severity="information")
            
        except Exception as e:
            error_msg = str(e)
            logging.error(f"Error opening file: {error_msg}", exc_info=True)
            self.notify(f"Error opening file: {error_msg}", severity="error")
            
    async def toggle_split_view(self):
        """Toggle split view mode"""
        editor_split = self.query_one("#editor-split-view")
        secondary_editor = self.query_one("#editor-secondary")
        
        if self.split_view_active:
            # Disable split view
            secondary_editor.add_class("hidden")
            editor_split.remove_class("split-view")
            self.split_view_active = False
            self.active_editor = "primary"
            self.notify("Split view disabled")
        else:
            # Enable split view
            secondary_editor.remove_class("hidden")
            editor_split.add_class("split-view")
            self.split_view_active = True
            
            # Copy content from primary to secondary if secondary is empty
            primary_editor = self.query_one("#editor-primary")
            if not secondary_editor.text:
                secondary_editor.language = primary_editor.language
                secondary_editor.text = primary_editor.text
                
            self.notify("Split view enabled")
    
    def get_language_from_extension(self, extension):
        """Map file extension to language for syntax highlighting"""
        extension_map = {
            ".py": "python",
            ".js": "javascript",
            ".html": "html",
            ".css": "css",
            ".json": "json",
            ".md": "markdown",
            ".txt": "text",
            # Add more as needed
        }
        return extension_map.get(extension, "text")
    
    @work(thread=True)
    async def action_save(self):
        """Save the current file asynchronously"""
        if not self.current_file:
            self.notify("No file selected to save", severity="warning")
            return
            
        try:
            # Get the active editor
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")
                
            # Get content before I/O operations to avoid UI blocking
            content = editor.text
            
            # Use run_in_thread to perform file I/O off the main thread
            def write_file():
                with open(self.current_file, "w", encoding="utf-8") as file:
                    file.write(content)
                    
            # Run the file write in a separate thread
            await self.run_worker(write_file)
                
            # Notify success after async operation completes
            self.notify(f"Saved {self.current_file}")
            
            # Schedule git status update as a separate task to avoid blocking
            asyncio.create_task(self.update_git_status())
            
        except Exception as e:
            self.notify(f"Error saving file: {str(e)}", severity="error")
    
    @work(thread=True)
    async def action_run(self):
        """Run the current Python file"""
        if not self.current_file:
            self.notify("No file selected to run", severity="warning")
            return
            
        if not self.current_file.endswith(".py"):
            self.notify("Only Python files can be executed", severity="warning")
            return
            
        # Save before running
        self.action_save()
        
        try:
            # Show running indicator
            ai_output = self.query_one("#ai-output")
            ai_output.update(f"Running {os.path.basename(self.current_file)}...\n\n")
            
            # Execute the Python file
            import subprocess
            result = subprocess.run(
                [sys.executable, self.current_file],
                capture_output=True,
                text=True,
                cwd=os.path.dirname(self.current_file)
            )
            
            # Display the output
            output = f"## Execution Results for {os.path.basename(self.current_file)}\n\n"
            
            if result.stdout:
                output += f"### Output:\n```\n{result.stdout}\n```\n\n"
                
            if result.stderr:
                output += f"### Errors:\n```\n{result.stderr}\n```\n\n"
                
            if result.returncode == 0:
                output += f" Program completed successfully (exit code: 0)"
            else:
                output += f" Program failed with exit code: {result.returncode}"
                
            ai_output.update(output)
            
        except Exception as e:
            self.notify(f"Error running file: {str(e)}", severity="error")
            ai_output = self.query_one("#ai-output")
            ai_output.update(f"## Execution Error\n\n```\n{str(e)}\n```")
    
    async def action_ai_request(self):
        """Process an AI request"""
        # Get the prompt from the input field
        prompt_input = self.query_one("#ai-prompt")
        prompt = prompt_input.value.strip()
        
        # For debugging, add a notification to confirm this method is called
        self.notify(f"Processing AI request: {prompt}", severity="information")
        
        if not prompt:
            self.notify("Please enter a prompt for the AI", severity="warning")
            return
        
        # Clear the input field
        prompt_input.value = ""
        
        # Get current code from active editor for context
        try:
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")
                
            code_context = editor.text
        except Exception as e:
            self.notify(f"Error getting editor context: {str(e)}", severity="error")
            code_context = ""
        
        # Update the AI output with the query
        try:
            ai_output = self.query_one("#ai-output")
            # Get current content as a string - Markdown widgets use str() in newer Textual
            current_content = str(ai_output)
            
            # Update the markdown with the query
            ai_output.update(f"{current_content}\n\n### Your Question:\n{prompt}\n\n### AI Assistant:\n*Thinking...*")
        except Exception as e:
            self.notify(f"Error updating AI output: {str(e)}", severity="error")
        
        # Call the AI agent
        try:
            self.call_ai_agent(prompt, code_context)
        except Exception as e:
            self.notify(f"Error calling AI agent: {str(e)}", severity="error")
            logging.error(f"AI agent error: {str(e)}", exc_info=True)
    
    def _prepare_agent_prompt(self, prompt, code_context):
        """Prepare the full prompt for the AI agent"""
        is_code_completion = False
        if prompt.lower().startswith("complete") or "autocomplete" in prompt.lower() or "finish this code" in prompt.lower():
            is_code_completion = True

        if code_context:
            if is_code_completion:
                return f"""Complete or suggest the next part of this code. 
                Analyze the code patterns and provide a detailed completion that follows the style and logic of the existing code.
                Return complete functions or code blocks, not just a single line.

                Code to complete:
                ```python
                {code_context}
                ```

                Provide the completed code only, without explanations."""
            else:
                return f"I'm working with this code:\n```python\n{code_context}\n```\n\nMy question: {prompt}"
        return prompt

    @work(thread=True)
    async def call_ai_agent(self, prompt, code_context):
        """Call the AI agent with the prompt and code context"""
        try:
            full_prompt = self._prepare_agent_prompt(prompt, code_context)
            result = await run_agent_query(full_prompt, self.agent_context)
            response = result.get("response", "I couldn't process that request.")
            self.call_after_refresh(self._update_ai_output_with_response, response)
        except Exception as e:
            error_message = f"An error occurred: {str(e)}"
            self.call_after_refresh(self._update_ai_output_with_response, error_message)

    def _update_ai_output_with_response(self, response):
        """Update the AI output widget with the response"""
        try:
            ai_output = self.query_one("#ai-output")
            # Get current content as a string - Markdown widgets use .update() in newer Textual
            current_content = str(ai_output)

            # Remove the "Thinking..." placeholder and add the real response
            if "*Thinking...*" in current_content:
                # Find everything before "Thinking..."
                thinking_pos = current_content.find("*Thinking...*")
                if thinking_pos > 0:
                    current_content = current_content[:thinking_pos]
                else:
                    # Just use a clean slate if we can't find the position
                    current_content = ""

            # Add the response and update the markdown
            ai_output.update(f"{current_content}{response}")
            
            # Check for code edits in the response
            self._check_for_code_suggestions(response)
            
        except Exception as e:
            self.notify(f"Error updating AI output: {str(e)}", severity="error")
            logging.error(f"Error updating AI output: {str(e)}", exc_info=True)
            

        
    async def on_mouse_down(self, event: MouseDown) -> None:
        """Handle mouse down events for gutter resizing"""
        # Check if we clicked on a gutter element
        target, _ = self.get_widget_at(event.screen_x, event.screen_y)
        
        if target and isinstance(target, Static) and target.has_class("gutter"):
            # Find the adjacent panels for this gutter
            gutter_idx = list(self.query(".gutter")).index(target)
            
            if gutter_idx == 0:
                # First gutter - between sidebar and editor
                self.resizing_panel = "sidebar"
            elif gutter_idx == 1:
                # Second gutter - between editor and AI panel
                self.resizing_panel = "editor-container"
            
            # Start resizing
            self.resizing = True
            self.start_x = event.screen_x
            
            # Capture the mouse to receive events outside the gutter
            self.capture_mouse()
                
            # Set the cursor to indicate resizing
            event.prevent_default()
            
    async def on_mouse_up(self, event) -> None:
        """Handle mouse up events to end panel resizing"""
        if self.resizing:
            self.resizing = False
            self.resizing_panel = None
            
    async def on_mouse_move(self, event) -> None:
        """Handle mouse move events for panel resizing"""
        if not self.resizing or not self.resizing_panel:
            return
            
        # Calculate movement
        delta_x = event.screen_x - self.start_x
        if delta_x == 0:
            return
            
        # Convert to percentage of total width based on app width
        app_width = self.size.width
        delta_percent = (delta_x / app_width) * 100
        
        # Update panel widths with constraints
        if self.resizing_panel == "sidebar":
            # Resizing sidebar affects editor width
            new_sidebar_width = self.current_widths["sidebar"] + delta_percent
            new_editor_width = self.current_widths["editor-container"] - delta_percent
            
            # Apply constraints
            if 10 <= new_sidebar_width <= 40 and 30 <= new_editor_width <= 80:
                self.current_widths["sidebar"] = new_sidebar_width
                self.current_widths["editor-container"] = new_editor_width
                
                # Apply new widths
                sidebar = self.query_one("#sidebar")
                editor = self.query_one("#editor-container")
                sidebar.styles.width = f"{new_sidebar_width}%"
                editor.styles.width = f"{new_editor_width}%"
                
        elif self.resizing_panel == "editor-container":
            # Resizing editor affects AI panel width
            new_editor_width = self.current_widths["editor-container"] + delta_percent
            new_ai_width = self.current_widths["ai-panel"] - delta_percent
            
            # Apply constraints
            if 30 <= new_editor_width <= 80 and 10 <= new_ai_width <= 40:
                self.current_widths["editor-container"] = new_editor_width
                self.current_widths["ai-panel"] = new_ai_width
                
                # Apply new widths
                editor = self.query_one("#editor-container")
                ai_panel = self.query_one("#ai-panel")
                editor.styles.width = f"{new_editor_width}%"
                ai_panel.styles.width = f"{new_ai_width}%"
                
        # Update start position for next movement
        self.start_x = event.screen_x
            
    def _check_for_code_suggestions(self, response):
        """
        Check if the AI response contains code suggestions and offer to show diff
        
        Args:
            response: The AI response text
        """
        try:
            # Only proceed if we have an active file
            if not self.current_file:
                return
                
            # Get the active editor content for comparison
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")
                
            current_content = editor.text
            
            # Check if the response contains code blocks
            code_blocks = re.findall(r'```(\w*)\n(.*?)```', response, re.DOTALL)
            
            if code_blocks:
                # Find the first Python code block that's a significant edit
                for lang, code in code_blocks:
                    # Skip if not Python code or if it's just a short snippet
                    if lang.lower() not in ['python', 'py'] or len(code.strip()) < 10:
                        continue
                        
                    # Skip if the code is too different from the current file
                    # This is a simple heuristic to avoid comparing unrelated code
                    if len(current_content) > 0 and len(code) > 0:
                        # If the suggested code is less than 20% similar to current content,
                        # it's probably unrelated
                        similarity = difflib.SequenceMatcher(None, current_content, code).ratio()
                        if similarity < 0.2:
                            continue
                    
                    # Calculate how different the code is
                    diff = CodeAnalyzer.create_diff(current_content, code)
                    
                    # If there are actual differences, offer to preview and apply them
                    if diff and '+' in diff and '-' in diff:
                        # Start a background task to show a notification with action buttons
                        asyncio.create_task(self._show_code_suggestion_notification(current_content, code))
                        break
                        
        except Exception as e:
            logging.error(f"Error checking for code suggestions: {str(e)}", exc_info=True)
            
    async def _show_code_suggestion_notification(self, current_content, suggested_code):
        """
        Show notification for code suggestions with action buttons
        
        Args:
            current_content: Current file content
            suggested_code: Suggested code from AI
        """
        # Slight delay to ensure notification appears after the main response
        await asyncio.sleep(0.5)
        
        # Create a notification with action buttons
        extension = os.path.splitext(self.current_file)[1].lower()
        language = self.get_language_from_extension(extension)
        
        from textual.notifications import Notification
        
        class CodeSuggestionNotification(Notification):
            def __init__(self, app, current, suggested):
                self.app = app
                self.current = current
                self.suggested = suggested
                super().__init__(
                    title="AI Code Suggestion",
                    message="The AI has suggested code changes. Would you like to view them?",
                    timeout=20
                )
                
            def on_button_pressed(self, event):
                button_id = event.button.id
                if button_id == "view-diff":
                    self.app.show_diff_view(
                        self.current, 
                        self.suggested,
                        title="AI Suggested Changes", 
                        language=language
                    )
                elif button_id == "apply-directly":
                    self.app.apply_diff_changes(self.suggested)
                    
            def compose(self):
                yield from super().compose()
                with self.content:
                    yield Button("View Changes", id="view-diff", variant="primary")
                    yield Button("Apply Directly", id="apply-directly", variant="warning")
                    
        # Show the custom notification
        self.notify(
            CodeSuggestionNotification(self, current_content, suggested_code),
            severity="information"
        )

    # Handle tab changes
    async def on_tabbed_content_tab_activated(self, event: TabbedContent.TabActivated) -> None:
        """Handle tab activation"""
        tab_id = event.tab.id
        
        self.notify(f"Tab activated: {tab_id}", severity="information")
        if tab_id == "editor-tab-pane":
            self.active_tab = "editor"
            self.query_one("#editor-content").remove_class("hidden")
            self.query_one("#terminal-content").add_class("hidden")
            # Focus the active editor
            if self.active_editor == "primary":
                self.query_one("#editor-primary").focus()
            else:
                self.query_one("#editor-secondary").focus()
        elif tab_id == "terminal-tab-pane":
            self.active_tab = "terminal"
            self.query_one("#terminal-content").remove_class("hidden")
            self.query_one("#editor-content").add_class("hidden")
            # Focus the terminal input
            self.query_one("#terminal-input").focus()
    
    # Handle button events
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button press events"""
        button_id = event.button.id
    
        if button_id == "commit-btn":
            self.action_git_commit()
        elif button_id == "pull-btn":
            self.action_git_pull()
        elif button_id == "push-btn":
            self.action_git_push()
        elif button_id == "branches-btn":
            self.action_show_branch_visualization()
        elif button_id == "run-btn":
            self.action_run()
        elif button_id == "debug-btn":
            self.action_debug_current_file()
        elif button_id == "save-btn":
            self.action_save()
        elif button_id == "format-btn":
            self.action_format_code()
        elif button_id == "split-view-btn":
            self.toggle_split_view()
        elif button_id == "theme-btn":
            self.push_screen(ThemeSelectionScreen())
        elif button_id == "pair-program-btn":
            self.action_toggle_pair_programming()
        elif button_id == "ai-submit":
            # Create a task to run the async method
            asyncio.create_task(self.action_ai_request())
        elif button_id == "terminal-execute-btn":
            self.execute_terminal_command()
        elif button_id == "open-remote-btn":
            self.action_connect_remote()
    
    async def action_show_branch_visualization(self) -> None:
        """Show the branch visualization screen"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return
            
        self.push_screen("branch_visualization")
    
    async def action_git_commit(self) -> None:
        """Show the commit dialog screen"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return
            
        self.push_screen(CommitDialog())
    
    @work
    async def action_git_pull(self) -> None:
        """Pull changes from remote repository"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return
            
        git_output = self.query_one("#git-output")
        git_output.update("Pulling changes...")
        
        try:
            # Use GitManager to pull changes
            success, result = GitManager.git_pull(self.git_repository)
            
            if success:
                self.notify("Changes pulled successfully", severity="success")
                git_output.update(f"Pull successful:\n{result}")
                # Update status after pull
                self.update_git_status()
            else:
                self.notify(f"Pull failed", severity="error")
                git_output.update(f"Pull failed:\n{result}")
                
        except Exception as e:
            self.notify(f"Error pulling changes: {str(e)}", severity="error")
            git_output.update(f"Error: {str(e)}")
    
    @work
    async def action_git_push(self) -> None:
        """Push changes to remote repository"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return
            
        git_output = self.query_one("#git-output")
        git_output.update("Pushing changes...")
        
        try:
            # Use GitManager to push changes
            success, result = GitManager.git_push(self.git_repository)
            
            if success:
                self.notify("Changes pushed successfully", severity="success")
                git_output.update(f"Push successful:\n{result}")
            else:
                self.notify(f"Push failed", severity="error")
                git_output.update(f"Push failed:\n{result}")
                
        except Exception as e:
            self.notify(f"Error pushing changes: {str(e)}", severity="error")
            git_output.update(f"Error: {str(e)}")
    
    @work
    async def action_format_code(self) -> None:
        """Format the current code using autopep8 or black if available"""
        if not self.current_file:
            self.notify("No file selected for formatting", severity="warning")
            return
            
        if not self.current_file.endswith('.py'):
            self.notify("Only Python files can be formatted", severity="warning")
            return
            
        # Get the active editor content
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")
            
        code = editor.text
        
        # Use an agent query to format the code
        if hasattr(self, "agent_context"):
            self.notify("Formatting code with AI...", severity="information")
            
            try:
                # Format code query
                result = await run_agent_query(
                    "Format this Python code according to PEP 8 without changing functionality:\n```python\n" + 
                    code + "\n```", 
                    self.agent_context
                )
                
                response = result.get("response", "")
                
                # Extract the code from markdown code blocks
                code_block_pattern = r'```python\s*\n(.*?)```'
                matches = re.findall(code_block_pattern, response, re.DOTALL)
                
                if matches:
                    formatted_code = matches[0].strip()
                    editor.text = formatted_code
                    self.notify("Code formatted successfully", severity="success")
                else:
                    # Try without language specifier
                    code_block_pattern = r'```\s*\n(.*?)```'
                    matches = re.findall(code_block_pattern, response, re.DOTALL)
                    if matches:
                        formatted_code = matches[0].strip()
                        editor.text = formatted_code
                        self.notify("Code formatted successfully", severity="success")
                    else:
                        self.notify("Failed to format code", severity="error")
                        
            except Exception as e:
                self.notify(f"Error formatting code: {str(e)}", severity="error")
        else:
            self.notify("AI agent not initialized, cannot format code", severity="error")
    
    @work
    async def action_analyze_code(self) -> None:
        """Analyze the current code for issues"""
        if not self.current_file:
            self.notify("No file selected for analysis", severity="warning")
            return
            
        if not self.current_file.endswith('.py'):
            self.notify("Only Python files can be analyzed", severity="warning")
            return
            
        # Get the active editor content
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")
            
        code = editor.text
        
        # Show the analysis dialog
        self.push_screen(CodeAnalysisDialog())
        
        try:
            # Use CodeAnalyzer
            analysis = CodeAnalyzer.analyze_python_code(code)
            
            # Format the result as markdown
            result_md = f"# Analysis of {os.path.basename(self.current_file)}\n\n"
            
            # Add issues
            if analysis.get("issues"):
                result_md += f"## Issues Found ({len(analysis['issues'])})\n\n"
                for issue in analysis["issues"]:
                    result_md += f"- **Line {issue.get('line', '?')}**: {issue.get('message', 'Unknown issue')} ({issue.get('type', 'unknown')})\n"
            else:
                result_md += "## No Issues Found\n\n"
                
            # Add recommendations
            if analysis.get("recommendations"):
                result_md += f"\n## Recommendations\n\n"
                for rec in analysis["recommendations"]:
                    result_md += f"- {rec}\n"
                    
            # Also add code stats
            stats = CodeAnalyzer.count_code_lines(code)
            result_md += f"\n## Code Statistics\n\n"
            result_md += f"- Total lines: {stats.get('total_lines', 0)}\n"
            result_md += f"- Code lines: {stats.get('code_lines', 0)}\n"
            result_md += f"- Comment lines: {stats.get('comment_lines', 0)}\n"
            result_md += f"- Blank lines: {stats.get('blank_lines', 0)}\n"
            
            # Post the completion event with the result
            self.post_message(self.CodeAnalysisComplete(result_md))
            
        except Exception as e:
            error_msg = f"# Error During Analysis\n\n{str(e)}"
            self.post_message(self.CodeAnalysisComplete(error_msg))
    
    async def on_code_analysis_complete(self, event: CodeAnalysisComplete) -> None:
        """Handle code analysis completion"""
        # Update the analysis result in the dialog
        analysis_dialog = self.query_one(CodeAnalysisDialog)
        analysis_result = analysis_dialog.query_one("#analysis-result")
        analysis_result.query_one(Markdown).update(event.analysis_result)


    def action_toggle_pair_programming(self) -> None:
        """Toggle AI pair programming mode"""
        # Toggle the pair programming mode
        self.pair_programming_active = not self.pair_programming_active
        
        # Update the button visual state
        pair_btn = self.query_one("#pair-program-btn")
        
        if self.pair_programming_active:
            # Start pair programming mode
            self.notify("AI Pair Programming mode activated", severity="success")
            pair_btn.variant = "error"  # Red button when active
            pair_btn.label = "Stop Pair Programming"
            
            # Create a timer to check for inactivity and provide suggestions
            if self.pair_programming_timer:
                self.pair_programming_timer.stop()
                
            # Start a timer that checks for inactivity every 5 seconds
            self.pair_programming_timer = self.set_interval(5, self.check_for_pair_programming_suggestion)
            
            # Start tracking edit time
            self.last_edit_time = time.time()
            
            # Add event handlers for text editing
            self.watch_text_area()
        else:
            # Stop pair programming mode
            self.notify("AI Pair Programming mode deactivated", severity="information")
            pair_btn.variant = "primary"  # Normal button when inactive
            pair_btn.label = "Pair Program"
            
            # Stop the timer
            if self.pair_programming_timer:
                self.pair_programming_timer.stop()
                self.pair_programming_timer = None
    
    def watch_text_area(self) -> None:
        """Start watching text area for changes in pair programming mode"""
        # In a full implementation, we would set up event handlers to track edits
        # We'll simulate this by updating the last_edit_time in certain methods
        pass
        
    def check_for_pair_programming_suggestion(self) -> None:
        """Check if it's time to provide a pair programming suggestion"""
        if not self.pair_programming_active:
            return
            
        # Check if the user has been inactive for more than 5 seconds
        current_time = time.time()
        if current_time - self.last_edit_time >= 5:
            # Time to generate a suggestion
            self.generate_pair_programming_suggestion()
            
    @work
    async def generate_pair_programming_suggestion(self) -> None:
        """Generate a pair programming suggestion based on current code"""
        if not self.current_file:
            return
            
        # Get the current code
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")
            
        code_context = editor.text
        
        if not code_context:
            return
            
        self.notify("AI suggesting improvements...", severity="information")
        
        # Create a specific prompt for pair programming suggestions
        prompt = """As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements."""
        
        # Call the AI with the pair programming prompt
        await self.call_ai_agent(prompt, code_context)
        
        # Reset the timer to avoid constant suggestions
        self.last_edit_time = time.time()
    
    async def on_text_area_changed(self, event: TextArea.Changed) -> None:
        """Handle text area changes"""
        # Update the last edit time when the user makes changes
        self.last_edit_time = time.time()
        
    def action_connect_remote(self) -> None:
        """Show the remote connection dialog"""
        self.push_screen("remote_connection")
        
    def configure_remote(self, **config) -> None:
        """Configure the remote connection"""
        # Update the remote config
        for key, value in config.items():
            if key in self.remote_config:
                self.remote_config[key] = value
                
        # Simulate connection
        self.notify(f"Connecting to {self.remote_config['host']}...", severity="information")
        
        # In a real implementation, we would actually connect
        # For now, we'll just simulate a successful connection
        self.remote_connected = True
        
        # Show connection success message
        self.notify(
            f"Connected to {self.remote_config['host']} as {self.remote_config['username']}", 
            severity="success"
        )
        
        # Show the remote files browser
        self.push_screen("remote_browser")
        
    def disconnect_remote(self) -> None:
        """Disconnect from the remote server"""
        if not self.remote_connected:
            self.notify("Not connected to a remote server", severity="warning")
            return
            
        # Simulate disconnection
        self.notify("Disconnecting from remote server...", severity="information")
        
        # Reset connection state
        self.remote_connected = False
        
        # Show disconnection message
        self.notify("Disconnected from remote server", severity="success")
        
    def action_upload_to_remote(self) -> None:
        """Upload a file to the remote server"""
        if not self.remote_connected:
            self.notify("Not connected to a remote server", severity="warning")
            return
            
        if not self.current_file:
            self.notify("No file selected for upload", severity="warning")
            return
            
        # Simulate file upload
        self.notify(f"Uploading {self.current_file} to remote server...", severity="information")
        
        # In a real implementation, we would actually upload the file
        # For now, we'll just simulate a successful upload
        self.notify(f"Uploaded {os.path.basename(self.current_file)} to {self.remote_config['remote_path']}", severity="success")
    
    def action_switch_editor(self) -> None:
        """Switch focus between primary and secondary editors in split view"""
        if not self.split_view_active:
            self.notify("Split view is not active", severity="warning")
            return
            
        if self.active_editor == "primary":
            self.active_editor = "secondary"
            self.query_one("#editor-secondary").focus()
            self.notify("Switched to secondary editor")
        else:
            self.active_editor = "primary"
            self.query_one("#editor-primary").focus()
            self.notify("Switched to primary editor")
            
    def action_add_multi_cursor(self) -> None:
        """Add a cursor at the current position or word"""
        # Get the active editor
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")
            
        # Get the current cursor position
        cursor_position = editor.cursor_position
        cursor_row, cursor_column = cursor_position
        
        # In a real implementation, this would add a cursor
        # For this example, we'll just notify about the cursor position
        self.notify(f"Multi-cursor added at row {cursor_row+1}, column {cursor_column+1}", severity="information")
        
        # Track multi-cursor positions for future implementation
        self.multi_cursor_positions.append(cursor_position)
        
        # Add multi-cursor class to the editor
        editor.add_class("multi-cursor")
    
    async def action_toggle_split_view(self) -> None:
        """Toggle split view mode using keyboard shortcut"""
        self.toggle_split_view()
        
    async def action_show_command_palette(self) -> None:
        """Show the command palette for quick access to commands"""
        self.push_screen("command_palette")
        
    async def action_code_completion(self) -> None:
        """Trigger AI code completion on the current cursor position"""
        if not self.current_file:
            self.notify("No file open for code completion", severity="warning")
            return
            
        # Get the active editor
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")
            
        # Get the code context
        code_context = editor.text
        
        # Show a notification
        self.notify("Generating code completion...", severity="information")
        
        # Request code completion from AI
        await self.call_ai_agent("Complete this code", code_context)
        
    def action_toggle_breakpoint(self) -> None:
        """Toggle a breakpoint at the current line in the editor"""
        if not self.current_file:
            self.notify("No file open for setting breakpoints", severity="warning")
            return
            
        if not self.current_file.endswith('.py'):
            self.notify("Breakpoints can only be set in Python files", severity="warning")
            return
            
        # Get the active editor
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")
            
        # Get the current cursor position
        cursor_row, _ = editor.cursor_position
        line_number = cursor_row + 1  # Convert to 1-based line number
        
        # Initialize breakpoints for this file if not already present
        if self.current_file not in self.breakpoints:
            self.breakpoints[self.current_file] = []
            
        # Toggle the breakpoint
        if line_number in self.breakpoints[self.current_file]:
            self.breakpoints[self.current_file].remove(line_number)
            self.notify(f"Breakpoint removed at line {line_number}", severity="information")
        else:
            # Verify that we can set a breakpoint at this line
            success, message = PythonDebugger.set_breakpoint(self.current_file, line_number)
            
            if success:
                self.breakpoints[self.current_file].append(line_number)
                self.notify(f"Breakpoint set at line {line_number}", severity="information")
            else:
                self.notify(f"Cannot set breakpoint: {message}", severity="error")
                
    def action_debug_current_file(self) -> None:
        """Start debugging the current file"""
        if not self.current_file:
            self.notify("No file open for debugging", severity="warning")
            return
            
        if not self.current_file.endswith('.py'):
            self.notify("Only Python files can be debugged", severity="warning")
            return
            
        # Save the file before debugging
        self.action_save()
        
        # Start a debugging session
        success, debug_session = PythonDebugger.start_debugging(self.current_file)
        
        if not success:
            error = debug_session.get("error", "Unknown error starting debugger")
            self.notify(f"Debug failed: {error}", severity="error")
            return
            
        # Store the debug session
        self.debug_session = debug_session
        
        # Add any existing breakpoints
        if self.current_file in self.breakpoints:
            self.debug_session["breakpoints"] = self.breakpoints[self.current_file]
            
        # Switch to the debugger screen
        self.push_screen("debugger")
        
        # Update the debugger UI
        self.update_debugger_ui()
        
    def update_debugger_ui(self) -> None:
        """Update the debugger UI with the current debug session info"""
        if not self.debug_session or not self.app.screen_stack[-1].id == "debugger":
            return
            
        # Update code view
        debug_code = self.app.screen_stack[-1].query_one("#debug-code")
        debug_code.text = self.debug_session.get("code", "")
        
        # Highlight current line
        current_line = self.debug_session.get("current_line", 1)
        
        # This would require extending TextArea to support line highlighting
        # For now, we'd just notify about the current line position
        self.notify(f"Debugging at line {current_line}", severity="information")
        
        # Update variables table
        variables_table = self.app.screen_stack[-1].query_one("#debug-variables")
        variables_table.clear()
        
        for var_name, var_info in self.debug_session.get("variables", {}).items():
            variables_table.add_row(var_name, var_info.get("type", "unknown"), var_info.get("value", ""))
            
        # Update call stack
        stack_table = self.app.screen_stack[-1].query_one("#debug-stack")
        stack_table.clear()
        
        for frame in self.debug_session.get("call_stack", []):
            stack_table.add_row(
                str(frame.get("frame", "")), 
                frame.get("function", ""),
                frame.get("file", ""),
                str(frame.get("line", ""))
            )
            
        # Update output
        debug_output = self.app.screen_stack[-1].query_one("#debug-output")
        debug_output.text = self.debug_session.get("output", "")
        
    def debug_step_over(self) -> None:
        """Execute a step over command in the debugger"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return
            
        if not self.debug_session.get("active", False):
            self.notify("Debugging session has ended", severity="information")
            return
            
        # Execute step over
        self.debug_session = PythonDebugger.debug_step(self.debug_session, "step_over")
        
        # Update the UI
        self.update_debugger_ui()
        
    def debug_step_into(self) -> None:
        """Execute a step into command in the debugger"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return
            
        if not self.debug_session.get("active", False):
            self.notify("Debugging session has ended", severity="information")
            return
            
        # Execute step into
        self.debug_session = PythonDebugger.debug_step(self.debug_session, "step_into")
        
        # Update the UI
        self.update_debugger_ui()
        
    def debug_step_out(self) -> None:
        """Execute a step out command in the debugger"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return
            
        if not self.debug_session.get("active", False):
            self.notify("Debugging session has ended", severity="information")
            return
            
        # Execute step out
        self.debug_session = PythonDebugger.debug_step(self.debug_session, "step_out")
        
        # Update the UI
        self.update_debugger_ui()
        
    def debug_continue(self) -> None:
        """Execute a continue command in the debugger"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return
            
        if not self.debug_session.get("active", False):
            self.notify("Debugging session has ended", severity="information")
            return
            
        # Execute continue
        self.debug_session = PythonDebugger.debug_step(self.debug_session, "continue")
        
        # Update the UI
        self.update_debugger_ui()
        
    def debug_stop(self) -> None:
        """Stop the current debugging session"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return
            
        # Stop debugging
        self.debug_session = PythonDebugger.stop_debugging(self.debug_session)
        
        # Update the UI
        self.update_debugger_ui()
        
        # Return to the main screen if debugging has ended
        if not self.debug_session.get("active", False):
            self.pop_screen()
            self.notify("Debugging session terminated", severity="information")
    
    async def action_toggle_terminal(self) -> None:
        """Toggle between editor and terminal tabs"""
        editor_tabs = self.query_one("#editor-tabs")
        
        if self.active_tab == "editor":
            # Activate terminal tab
            editor_tabs.active = "terminal-tab"
            self.active_tab = "terminal"
            self.query_one("#terminal-content").remove_class("hidden")
            self.query_one("#editor-content").add_class("hidden")
            self.query_one("#terminal-input").focus()
        else:
            # Activate editor tab
            editor_tabs.active = "editor-tab"
            self.active_tab = "editor"
            self.query_one("#editor-content").remove_class("hidden")
            self.query_one("#terminal-content").add_class("hidden")
            if self.active_editor == "primary":
                self.query_one("#editor-primary").focus()
            else:
                self.query_one("#editor-secondary").focus()
            
    async def action_start_collaboration(self) -> None:
        """Start or join a real-time collaboration session"""
        # Show collaboration dialog
        self.push_screen(CollaborationSessionDialog(), callback=self.handle_collaboration_dialog)
    
    def handle_collaboration_dialog(self, result: Optional[Dict[str, Any]]) -> None:
        """
        Handle the result from the collaboration dialog
        
        Args:
            result: Dialog result
        """
        if result is None:
            # Dialog was canceled
            return
            
        action = result.get("action")
        
        if action == "create":
            # Create a new session
            username = result.get("username", "User")
            # Generate a random session ID
            import random
            session_id = ''.join(random.choices('ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789', k=6))
            
            session_details = {
                "username": username,
                "session_id": session_id,
                "is_host": True
            }
            
            # Start the collaboration server in a background task
            self.start_collaboration_server()
            
            # Open the collaboration screen
            self.push_screen(CollaborationScreen(session_details))
            
        elif action == "join":
            # Join an existing session
            username = result.get("username", "User")
            session_id = result.get("session_id", "")
            
            session_details = {
                "username": username,
                "session_id": session_id,
                "is_host": False
            }
            
            # Open the collaboration screen
            self.push_screen(CollaborationScreen(session_details))
    
    @work
    async def start_collaboration_server(self) -> None:
        """Start the WebSocket server for real-time collaboration"""
        try:
            # Initialize collaboration manager if not already created
            if not hasattr(self, "collaboration_manager"):
                self.collaboration_manager = CollaborationManager()
                
            # Start the server
            await self.collaboration_manager.start_server()
            self.notify("Collaboration server started", severity="information")
            
        except Exception as e:
            self.notify(f"Failed to start collaboration server: {str(e)}", severity="error")
    
    async def stop_collaboration_server(self) -> None:
        """Stop the WebSocket server for real-time collaboration"""
        if hasattr(self, "collaboration_manager") and self.collaboration_manager.running:
            await self.collaboration_manager.stop_server()
            self.notify("Collaboration server stopped", severity="information")
            
    async def on_text_area_cursor_moved(self, event) -> None:
        """
        Handle cursor movement events for collaborative editing
        
        Args:
            event: Cursor moved event
        """
        # Only process if we're in a collaboration session
        if not self.app.is_screen_active(CollaborationScreen):
            return
            
        # Get current position and forward to collaboration session
        text_area = event.text_area
        position = (text_area.cursor_location.row, text_area.cursor_location.column)
        
        # In a real implementation, this would send the cursor position to other users
        # For now, we just log it
        if hasattr(self, "last_cursor_position") and self.last_cursor_position == position:
            return
            
        self.last_cursor_position = position
        self.query_one("#terminal-content").add_class("hidden")
        if self.active_editor == "primary":
            self.query_one("#editor-primary").focus()
        else:
            self.query_one("#editor-secondary").focus()
    
    @work
    async def execute_terminal_command(self) -> None:
        """Execute command in the terminal"""
        terminal_input = self.query_one("#terminal-input")
        command = terminal_input.value.strip()
        
        if not command:
            self.notify("Please enter a command", severity="warning")
            return
            
        # Clear the input
        terminal_input.value = ""
        
        # Add command to terminal output
        terminal_output = self.query_one("#terminal-output")
        current_output = terminal_output.text
        terminal_output.text = f"{current_output}\n$ {command}\n"
        
        # Add to history
        self.terminal_history.append(command)
        
        try:
            # Execute the command in the current directory
            import subprocess
            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                cwd=self.current_directory
            )
            
            # Add output to terminal
            output = ""
            if result.stdout:
                output += result.stdout
            if result.stderr:
                output += result.stderr
                
            # Update terminal with output
            terminal_output.text = f"{terminal_output.text}{output}"
            
            # Auto-scroll to bottom
            terminal_output.scroll_to_line(len(terminal_output.text.splitlines()) - 1)
            
        except Exception as e:
            # Show error
            terminal_output.text = f"{terminal_output.text}Error: {str(e)}\n"
            
        # Focus input for next command
        terminal_input.focus()


if __name__ == "__main__":
    app = TerminatorApp()
    app.run()
```

My question: Help me fix this code please.
2025-04-01 18:31:31,267 - terminator_agents - INFO - Added 11616 tokens for query, total: 11637
2025-04-01 18:31:31,281 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: list.*file
2025-04-01 18:31:31,304 - openai._base_client - INFO - Retrying request to /responses in 0.478036 seconds
2025-04-01 18:31:36,162 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 18:41:31,871 - openai._base_client - INFO - Retrying request to /responses in 0.901690 seconds
2025-04-01 18:51:32,855 - openai.agents - ERROR - Error getting response: Request timed out.. (request_id: None)
2025-04-01 18:51:32,859 - terminator_agents - ERROR - Error in agent query: Request timed out.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection.py", line 103, in handle_async_request
    return await self._connection.handle_async_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 136, in handle_async_request
    raise exc
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 106, in handle_async_request
    ) = await self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 177, in _receive_response_headers
    event = await self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 217, in _receive_event
    data = await self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_backends/anyio.py", line 32, in read
    with map_exceptions(exc_map):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1500, in _request
    response = await self._client.send(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 393, in handle_async_request
    with map_httpcore_exceptions():
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1809, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 215, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 739, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 896, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 75, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 234, in _fetch_response
    return await self._client.responses.create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/resources/responses/responses.py", line 1415, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1461, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1524, in _request
    return await self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in _retry_request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1509, in _request
    return await self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in _retry_request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1519, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2025-04-01 18:51:38,187 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 18:56:07,635 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:28:33,829 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 22:28:33,842 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 22:28:33,842 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 22:28:33,843 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 22:28:33,847 - root - INFO - AI panel initialized successfully
2025-04-01 22:28:42,286 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:28:49,053 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:28:49,848 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:28:56,476 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:29:02,321 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:29:08,142 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:29:26,763 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:29:41,109 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:29:51,459 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:30:27,471 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:30:42,558 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:30:54,891 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:31:23,465 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:31:26,902 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:32:14,450 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:32:15,744 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:32:28,547 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:32:34,438 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:32:41,419 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:32:47,268 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:32:53,553 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:32:59,401 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:33:05,584 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:33:58,095 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:34:18,720 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="o3-mini",
    tools=[get_weather],
)

## Context

@dataclass
class UserContext:
    uid: str
    is_pro_user: bool

    async def fetch_purchases() -> list[Purchase]:
        return ...

agent = Agent[UserContext](
    ...,
)


## output types:

from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)


## handoffs

from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    name="Triage agent",
    instructions=(
        "Help the user with their questions."
        "If they ask about booking, handoff to the booking agent."
        "If they ask about refunds, handoff to the refund agent."
    ),
    handoffs=[booking_agent, refund_agent],
)

## dynamic instructions

def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)

## Cloning copying agents

pirate_agent = Agent(
    name="Pirate",
    instructions="Write like a pirate",
    model="o3-mini",
)

robot_agent = pirate_agent.clone(
    name="Robot",
    instructions="Write like a robot",
)

## running agents

from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant")

    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance.
    
#The agent loop

#When you use the run method in Runner, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.

#The runner then runs a loop:

#We call the LLM for the current agent, with the current input.
#The LLM produces its output.
#If the LLM returns a final_output, the loop ends and we return the result.
#If the LLM does a handoff, we update the current agent and input, and re-run the loop.
#If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.
#If we exceed the max_turns passed, we raise a MaxTurnsExceeded exception.

# Streaming

#Streaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the RunResultStreaming will contain the complete information about the run, including all the new outputs produces. You can call .stream_events() for the streaming events. Read more in the streaming guide.
#Streaming

#Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.

#To stream, you can call Runner.run_streamed(), which will give you a RunResultStreaming. Calling result.stream_events() gives you an async stream of StreamEvent objects, which are described below.

#Raw response events

#RawResponsesStreamEvent are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like response.created, response.output_text.delta, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.

#For example, this will output the text generated by the LLM token-by-token.

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
    
# Run item events and agent events
import asyncio
import random
from agents import Agent, ItemHelpers, Runner, function_tool

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
    
#Run config

#The run_config parameter lets you configure some global settings for the agent run:

#model: Allows setting a global LLM model to use, irrespective of what model each Agent has.
#model_provider: A model provider for looking up model names, which defaults to OpenAI.
#model_settings: Overrides agent-specific settings. For example, you can set a global temperature or top_p.
#input_guardrails, output_guardrails: A list of input or output guardrails to include on all runs.
#handoff_input_filter: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in Handoff.input_filter for more details.
#tracing_disabled: Allows you to disable tracing for the entire run.
#trace_include_sensitive_data: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.
#workflow_name, trace_id, group_id: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting workflow_name. The session ID is an optional field that lets you link traces across multiple runs.
#trace_metadata: Metadata to include on all traces.

# Conversations/chat threads
async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print(result.final_output)
        # San Francisco

        # Second turn
        new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
        result = await Runner.run(agent, new_input)
        print(result.final_output)
        # California
        
#Exceptions

#The SDK raises exceptions in certain cases. The full list is in agents.exceptions. As an overview:

#AgentsException is the base class for all exceptions raised in the SDK.
#MaxTurnsExceeded is raised when the run exceeds the max_turns passed to the run methods.
#ModelBehaviorError is raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools.
#UserError is raised when you (the person writing code using the SDK) make an error using the SDK.
#InputGuardrailTripwireTriggered, OutputGuardrailTripwireTriggered is raised when a guardrail is tripped.

##Results
#When you call the Runner.run methods, you either get a:
#RunResult if you call run or run_sync
#RunResultStreaming if you call run_streamed
#Both of these inherit from RunResultBase, which is where most useful information is present.
#Final output
#The final_output property contains the final output of the last agent that ran. This is either:
#a str, if the last agent didn't have an output_type defined
#an object of type last_agent.output_type, if the agent had an output type defined.
#Note
#final_output is of type Any. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types.
#Inputs for the next turn
#You can use result.to_input_list() to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.
#Last agent
#The last_agent property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.
#New items
#The new_items property contains the new items generated during the run. The items are RunItems. A run item wraps the raw item generated by the LLM.
#MessageOutputItem indicates a message from the LLM. The raw item is the message generated.
#HandoffCallItem indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.
#HandoffOutputItem indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.
#ToolCallItem indicates that the LLM invoked a tool.
#ToolCallOutputItem indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.
#ReasoningItem indicates a reasoning item from the LLM. The raw item is the reasoning generated.
#Other information
#Guardrail results
#The input_guardrail_results and output_guardrail_results properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you.
#Raw responses
#The raw_responses property contains the ModelResponses generated by the LLM.
#Original input
#The input property contains the original input you provided to the run method. In most cases you won't need this, but it's available in case you do.

# Function tools
import json

from typing_extensions import TypedDict, Any

from agents import Agent, FunctionTool, RunContextWrapper, function_tool


class Location(TypedDict):
    lat: float
    long: float

@function_tool  
async def fetch_weather(location: Location) -> str:
    
    """Fetch the weather for a given location.

    Args:
        location: The location to fetch the weather for.
    """
    # In real life, we'd fetch the weather from a weather API
    return "sunny"


@function_tool(name_override="fetch_data")  
def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:
    """Read the contents of a file.

    Args:
        path: The path to the file to read.
        directory: The directory to read the file from.
    """
    # In real life, we'd read the file from the file system
    return "<file contents>"


agent = Agent(
    name="Assistant",
    tools=[fetch_weather, read_file],  
)

for tool in agent.tools:
    if isinstance(tool, FunctionTool):
        print(tool.name)
        print(tool.description)
        print(json.dumps(tool.params_json_schema, indent=2))
        print()
        
## Custom function tools
from typing import Any

from pydantic import BaseModel

from agents import RunContextWrapper, FunctionTool



def do_some_work(data: str) -> str:
    return "done"


class FunctionArgs(BaseModel):
    username: str
    age: int


async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:
    parsed = FunctionArgs.model_validate_json(args)
    return do_some_work(data=f"{parsed.username} is {parsed.age} years old")


tool = FunctionTool(
    name="process_user",
    description="Processes extracted user data",
    params_json_schema=FunctionArgs.model_json_schema(),
    on_invoke_tool=run_function,
)

## Agents as tools
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)

french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)

async def main():
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(result.final_output)
    
## creating a handoff
from agents import Agent, handoff

billing_agent = Agent(name="Billing agent")
refund_agent = Agent(name="Refund agent")


triage_agent = Agent(name="Triage agent", handoffs=[billing_agent, handoff(refund_agent)])

## Customizing handoffs via the handoff() function
from agents import Agent, handoff, RunContextWrapper

def on_handoff(ctx: RunContextWrapper[None]):
    print("Handoff called")

agent = Agent(name="My agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    tool_name_override="custom_handoff_tool",
    tool_description_override="Custom description",
)

## Handoff inputs

from pydantic import BaseModel

from agents import Agent, handoff, RunContextWrapper

class EscalationData(BaseModel):
    reason: str

async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):
    print(f"Escalation agent called with reason: {input_data.reason}")

agent = Agent(name="Escalation agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    input_type=EscalationData,
)

## Input filters

from agents import Agent, handoff
from agents.extensions import handoff_filters

agent = Agent(name="FAQ agent")

handoff_obj = handoff(
    agent=agent,
    input_filter=handoff_filters.remove_all_tools, 
)

## Recommended prompts

from agents import Agent
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX

billing_agent = Agent(
    name="Billing agent",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    <Fill in the rest of your prompt here>.""",
)

## Tracing

#The Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the Traces dashboard, you can debug, visualize, and monitor your workflows during development and in production.

## Default tracing
#By default, the SDK traces the following:
#The entire Runner.{run, run_sync, run_streamed}() is wrapped in a trace().
#Each time an agent runs, it is wrapped in agent_span()
#LLM generations are wrapped in generation_span()
#Function tool calls are each wrapped in function_span()
#Guardrails are wrapped in guardrail_span()
#Handoffs are wrapped in handoff_span()
#By default, the trace is named "Agent trace". You can set this name if you use trace, or you can can configure the name and other properties with the RunConfig.
#In addition, you can set up custom trace processors to push traces to other destinations (as a replacement, or secondary destination).

## Higher level traces
from agents import Agent, Runner, trace

async def main():
    agent = Agent(name="Joke generator", instructions="Tell funny jokes.")

    with trace("Joke workflow"): 
        first_result = await Runner.run(agent, "Tell me a joke")
        second_result = await Runner.run(agent, f"Rate this joke: {first_result.final_output}")
        print(f"Joke: {first_result.final_output}")
        print(f"Rating: {second_result.final_output}")
        
## Context management
#Context is an overloaded term. There are two main classes of context you might care about:
#Context available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like on_handoff, in lifecycle hooks, etc.
#Context available to LLMs: this is data the LLM sees when generating a response.
#Local context
#This is represented via the RunContextWrapper class and the context property within it. The way this works is:
#You create any Python object you want. A common pattern is to use a dataclass or a Pydantic object.
#You pass that object to the various run methods (e.g. Runner.run(..., **context=whatever**)).
#All your tool calls, lifecycle hooks etc will be passed a wrapper object, RunContextWrapper[T], where T represents your context object type which you can access via wrapper.context.
#The most important thing to be aware of: every agent, tool function, lifecycle etc for a given agent run must use the same type of context.
#You can use the context for things like:
#Contextual data for your run (e.g. things like a username/uid or other information about the user)
#Dependencies (e.g. logger objects, data fetchers, etc)
#Helper functions

## example
import asyncio
from dataclasses import dataclass

from agents import Agent, RunContextWrapper, Runner, function_tool

@dataclass
class UserInfo:  
    name: str
    uid: int

@function_tool
async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:  
    return f"User {wrapper.context.name} is 47 years old"

async def main():
    user_info = UserInfo(name="John", uid=123)  

    agent = Agent[UserInfo](  
        name="Assistant",
        tools=[fetch_user_age],
    )

    result = await Runner.run(
        starting_agent=agent,
        input="What is the age of the user?",
        context=user_info,
    )

    print(result.final_output)  
    # The user John is 47 years old.

if __name__ == "__main__":
    asyncio.run(main())
    
##Agent/LLM context

#When an LLM is called, the only data it can see is from the conversation history. This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this:
#You can add it to the Agent instructions. This is also known as a "system prompt" or "developer message". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date).
#Add it to the input when calling the Runner.run functions. This is similar to the instructions tactic, but allows you to have messages that are lower in the chain of command.
#Expose it via function tools. This is useful for on-demand context - the LLM decides when it needs some data, and can call the tool to fetch that data.
#Use retrieval or web search. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for "grounding" the response in relevant contextual data.


## Implementing a guardrail

from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    TResponseInputItem,
    input_guardrail,
)

class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str

guardrail_agent = Agent( 
    name="Guardrail check",
    instructions="Check if the user is asking you to do their math homework.",
    output_type=MathHomeworkOutput,
)


@input_guardrail
async def math_guardrail( 
    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output, 
        tripwire_triggered=result.final_output.is_math_homework,
    )


agent = Agent(  
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    input_guardrails=[math_guardrail],
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped")
        
## output guardrails

from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    OutputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    output_guardrail,
)
class MessageOutput(BaseModel): 
    response: str

class MathOutput(BaseModel): 
    is_math: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the output includes any math.",
    output_type=MathOutput,
)

@output_guardrail
async def math_guardrail(  
    ctx: RunContextWrapper, agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output,
        tripwire_triggered=result.final_output.is_math,
    )

agent = Agent( 
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    output_guardrails=[math_guardrail],
    output_type=MessageOutput,
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except OutputGuardrailTripwireTriggered:
        print("Math output guardrail tripped")
        
## Orchestrating multiple agents

#Orchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents:

#Allowing the LLM to make decisions: this uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that.
#Orchestrating via code: determining the flow of agents via your code.
#You can mix and match these patterns. Each has their own tradeoffs, described below.

#Orchestrating via LLM

#An agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents. For example, a research agent could be equipped with tools like:

#Web search to find information online
#File search and retrieval to search through proprietary data and connections
#Computer use to take actions on a computer
#Code execution to do data analysis
#Handoffs to specialized agents that are great at planning, report writing and more.
#This pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM. The most important tactics here are:

#Invest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within.
#Monitor your app and iterate on it. See where things go wrong, and iterate on your prompts.
#Allow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve.
#Have specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything.
#Invest in evals. This lets you train your agents to improve and get better at tasks.
#Orchestrating via code

#While orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance. Common patterns here are:

#Using structured outputs to generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category.
#Chaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it.
#Running the agent that performs the task in a while loop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria.
#Running multiple agents in parallel, e.g. via Python primitives like asyncio.gather. This is useful for speed when you have multiple tasks that don't depend on each other.

#Models

#The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:

#Recommended: the OpenAIResponsesModel, which calls OpenAI APIs using the new Responses API.
#The OpenAIChatCompletionsModel, which calls OpenAI APIs using the Chat Completions API.
#Mixing and matching models

#Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an Agent, you can select a specific model by either:

#Passing the name of an OpenAI model.
#Passing any model name + a ModelProvider that can map that name to a Model instance.
#Directly providing a Model implementation.

## example

from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
    model="o3-mini", 
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model=OpenAIChatCompletionsModel( 
        model="gpt-4o",
        openai_client=AsyncOpenAI()
    ),
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    model="gpt-3.5-turbo",
)

async def main():
    result = await Runner.run(triage_agent, input="Hola, cmo ests?")
    print(result.final_output)
    
## API keys and clients
from agents import set_default_openai_key

set_default_openai_key("sk-...")

## alternative
from openai import AsyncOpenAI
from agents import set_default_openai_client

custom_client = AsyncOpenAI(base_url="...", api_key="...")
set_default_openai_client(custom_client)

#Finally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the set_default_openai_api() function.
from agents import set_default_openai_api

set_default_openai_api("chat_completions")

## Debug logging
from agents import enable_verbose_stdout_logging

enable_verbose_stdout_logging()

## Custom logging
import logging

logger =  logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())

## simple agent
from agents import Agent, Runner

agent = Agent(
    name="Assistant",
    instructions="You are a helpful assistant."
)

async def main():
    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

## Example of Creating an Agent with a Tool:

from agents import Agent, function_tool, Runner

# Define a custom tool using the function_tool decorator
@function_tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

# Create an agent with specific instructions and the custom tool
agent = Agent(
    name="CalculatorAgent",
    instructions="You are a calculator agent that adds two numbers.",
    tools=[add_numbers],
)

# Run the agent with a sample input
async def main():
    result = await Runner.run(agent, "Add 5 and 7.")
    print(result.final_output)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
    
## How to Run Agents:
#Agents in OpenAIs SDK are executed using the Runner class, which provides three primary methods:
#Runner.run()  Standard async run (basic usage)
#Runner.run_streamed() for receiving streaming results
#Runner.run_sync() for synchronous agent execution (blocking call)

## Streaming execution
from agents import Agent, Runner
import asyncio

async def main():
    agent = Agent(name="Streaming Assistant", instructions="Answer concisely.")
    
    # Run agent with streaming
    async for event in Runner.run_streamed(agent, "Tell me about recursion."):
        print(event)

if __name__ == "__main__":
    asyncio.run(main())
    
## Running Agents in Conversational Mode
from agents import Agent, Runner, trace
import asyncio

async def main():
    thread_id = "conversation_12345"  # Unique ID for conversation

    agent = Agent(
        name="Concise Assistant",
        instructions="Reply very concisely."
    )

    with trace(workflow_name="Chat Session", group_id=thread_id):
        # First turn
        result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print("Assistant:", result1.final_output)

        # Second turn, using previous output as context
        next_input = result1.final_output
        result2 = await Runner.run(agent, f"Provide one fun fact about {result1.final_output}.")
        print(result1.final_output)
        print(result2.final_output)from agents import Agent, ModelSettings, function_tool

@function_tool
def get_weather(city: str) -> str:
    return f"The weather in {city} is sunny"

agent = Agent(
    name="Haiku agent",
    instructions="Always respond in haiku form",
    model="o3-mini",
    tools=[get_weather],
)

## Context

@dataclass
class UserContext:
    uid: str
    is_pro_user: bool

    async def fetch_purchases() -> list[Purchase]:
        return ...

agent = Agent[UserContext](
    ...,
)


## output types:

from pydantic import BaseModel
from agents import Agent


class CalendarEvent(BaseModel):
    name: str
    date: str
    participants: list[str]

agent = Agent(
    name="Calendar extractor",
    instructions="Extract calendar events from text",
    output_type=CalendarEvent,
)


## handoffs

from agents import Agent

booking_agent = Agent(...)
refund_agent = Agent(...)

triage_agent = Agent(
    name="Triage agent",
    instructions=(
        "Help the user with their questions."
        "If they ask about booking, handoff to the booking agent."
        "If they ask about refunds, handoff to the refund agent."
    ),
    handoffs=[booking_agent, refund_agent],
)

## dynamic instructions

def dynamic_instructions(
    context: RunContextWrapper[UserContext], agent: Agent[UserContext]
) -> str:
    return f"The user's name is {context.context.name}. Help them with their questions."


agent = Agent[UserContext](
    name="Triage agent",
    instructions=dynamic_instructions,
)

## Cloning copying agents

pirate_agent = Agent(
    name="Pirate",
    instructions="Write like a pirate",
    model="o3-mini",
)

robot_agent = pirate_agent.clone(
    name="Robot",
    instructions="Write like a robot",
)

## running agents

from agents import Agent, Runner

async def main():
    agent = Agent(name="Assistant", instructions="You are a helpful assistant")

    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)
    # Code within the code,
    # Functions calling themselves,
    # Infinite loop's dance.
    
#The agent loop

#When you use the run method in Runner, you pass in a starting agent and input. The input can either be a string (which is considered a user message), or a list of input items, which are the items in the OpenAI Responses API.

#The runner then runs a loop:

#We call the LLM for the current agent, with the current input.
#The LLM produces its output.
#If the LLM returns a final_output, the loop ends and we return the result.
#If the LLM does a handoff, we update the current agent and input, and re-run the loop.
#If the LLM produces tool calls, we run those tool calls, append the results, and re-run the loop.
#If we exceed the max_turns passed, we raise a MaxTurnsExceeded exception.

# Streaming

#Streaming allows you to additionally receive streaming events as the LLM runs. Once the stream is done, the RunResultStreaming will contain the complete information about the run, including all the new outputs produces. You can call .stream_events() for the streaming events. Read more in the streaming guide.
#Streaming

#Streaming lets you subscribe to updates of the agent run as it proceeds. This can be useful for showing the end-user progress updates and partial responses.

#To stream, you can call Runner.run_streamed(), which will give you a RunResultStreaming. Calling result.stream_events() gives you an async stream of StreamEvent objects, which are described below.

#Raw response events

#RawResponsesStreamEvent are raw events passed directly from the LLM. They are in OpenAI Responses API format, which means each event has a type (like response.created, response.output_text.delta, etc) and data. These events are useful if you want to stream response messages to the user as soon as they are generated.

#For example, this will output the text generated by the LLM token-by-token.

import asyncio
from openai.types.responses import ResponseTextDeltaEvent
from agents import Agent, Runner

async def main():
    agent = Agent(
        name="Joker",
        instructions="You are a helpful assistant.",
    )

    result = Runner.run_streamed(agent, input="Please tell me 5 jokes.")
    async for event in result.stream_events():
        if event.type == "raw_response_event" and isinstance(event.data, ResponseTextDeltaEvent):
            print(event.data.delta, end="", flush=True)


if __name__ == "__main__":
    asyncio.run(main())
    
# Run item events and agent events
import asyncio
import random
from agents import Agent, ItemHelpers, Runner, function_tool

@function_tool
def how_many_jokes() -> int:
    return random.randint(1, 10)


async def main():
    agent = Agent(
        name="Joker",
        instructions="First call the `how_many_jokes` tool, then tell that many jokes.",
        tools=[how_many_jokes],
    )

    result = Runner.run_streamed(
        agent,
        input="Hello",
    )
    print("=== Run starting ===")

    async for event in result.stream_events():
        # We'll ignore the raw responses event deltas
        if event.type == "raw_response_event":
            continue
        # When the agent updates, print that
        elif event.type == "agent_updated_stream_event":
            print(f"Agent updated: {event.new_agent.name}")
            continue
        # When items are generated, print them
        elif event.type == "run_item_stream_event":
            if event.item.type == "tool_call_item":
                print("-- Tool was called")
            elif event.item.type == "tool_call_output_item":
                print(f"-- Tool output: {event.item.output}")
            elif event.item.type == "message_output_item":
                print(f"-- Message output:\n {ItemHelpers.text_message_output(event.item)}")
            else:
                pass  # Ignore other event types

    print("=== Run complete ===")


if __name__ == "__main__":
    asyncio.run(main())
    
#Run config

#The run_config parameter lets you configure some global settings for the agent run:

#model: Allows setting a global LLM model to use, irrespective of what model each Agent has.
#model_provider: A model provider for looking up model names, which defaults to OpenAI.
#model_settings: Overrides agent-specific settings. For example, you can set a global temperature or top_p.
#input_guardrails, output_guardrails: A list of input or output guardrails to include on all runs.
#handoff_input_filter: A global input filter to apply to all handoffs, if the handoff doesn't already have one. The input filter allows you to edit the inputs that are sent to the new agent. See the documentation in Handoff.input_filter for more details.
#tracing_disabled: Allows you to disable tracing for the entire run.
#trace_include_sensitive_data: Configures whether traces will include potentially sensitive data, such as LLM and tool call inputs/outputs.
#workflow_name, trace_id, group_id: Sets the tracing workflow name, trace ID and trace group ID for the run. We recommend at least setting workflow_name. The session ID is an optional field that lets you link traces across multiple runs.
#trace_metadata: Metadata to include on all traces.

# Conversations/chat threads
async def main():
    agent = Agent(name="Assistant", instructions="Reply very concisely.")

    with trace(workflow_name="Conversation", group_id=thread_id):
        # First turn
        result = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print(result.final_output)
        # San Francisco

        # Second turn
        new_input = result.to_input_list() + [{"role": "user", "content": "What state is it in?"}]
        result = await Runner.run(agent, new_input)
        print(result.final_output)
        # California
        
#Exceptions

#The SDK raises exceptions in certain cases. The full list is in agents.exceptions. As an overview:

#AgentsException is the base class for all exceptions raised in the SDK.
#MaxTurnsExceeded is raised when the run exceeds the max_turns passed to the run methods.
#ModelBehaviorError is raised when the model produces invalid outputs, e.g. malformed JSON or using non-existent tools.
#UserError is raised when you (the person writing code using the SDK) make an error using the SDK.
#InputGuardrailTripwireTriggered, OutputGuardrailTripwireTriggered is raised when a guardrail is tripped.

##Results
#When you call the Runner.run methods, you either get a:
#RunResult if you call run or run_sync
#RunResultStreaming if you call run_streamed
#Both of these inherit from RunResultBase, which is where most useful information is present.
#Final output
#The final_output property contains the final output of the last agent that ran. This is either:
#a str, if the last agent didn't have an output_type defined
#an object of type last_agent.output_type, if the agent had an output type defined.
#Note
#final_output is of type Any. We can't statically type this, because of handoffs. If handoffs occur, that means any Agent might be the last agent, so we don't statically know the set of possible output types.
#Inputs for the next turn
#You can use result.to_input_list() to turn the result into an input list that concatenates the original input you provided, to the items generated during the agent run. This makes it convenient to take the outputs of one agent run and pass them into another run, or to run it in a loop and append new user inputs each time.
#Last agent
#The last_agent property contains the last agent that ran. Depending on your application, this is often useful for the next time the user inputs something. For example, if you have a frontline triage agent that hands off to a language-specific agent, you can store the last agent, and re-use it the next time the user messages the agent.
#New items
#The new_items property contains the new items generated during the run. The items are RunItems. A run item wraps the raw item generated by the LLM.
#MessageOutputItem indicates a message from the LLM. The raw item is the message generated.
#HandoffCallItem indicates that the LLM called the handoff tool. The raw item is the tool call item from the LLM.
#HandoffOutputItem indicates that a handoff occurred. The raw item is the tool response to the handoff tool call. You can also access the source/target agents from the item.
#ToolCallItem indicates that the LLM invoked a tool.
#ToolCallOutputItem indicates that a tool was called. The raw item is the tool response. You can also access the tool output from the item.
#ReasoningItem indicates a reasoning item from the LLM. The raw item is the reasoning generated.
#Other information
#Guardrail results
#The input_guardrail_results and output_guardrail_results properties contain the results of the guardrails, if any. Guardrail results can sometimes contain useful information you want to log or store, so we make these available to you.
#Raw responses
#The raw_responses property contains the ModelResponses generated by the LLM.
#Original input
#The input property contains the original input you provided to the run method. In most cases you won't need this, but it's available in case you do.

# Function tools
import json

from typing_extensions import TypedDict, Any

from agents import Agent, FunctionTool, RunContextWrapper, function_tool


class Location(TypedDict):
    lat: float
    long: float

@function_tool  
async def fetch_weather(location: Location) -> str:
    
    """Fetch the weather for a given location.

    Args:
        location: The location to fetch the weather for.
    """
    # In real life, we'd fetch the weather from a weather API
    return "sunny"


@function_tool(name_override="fetch_data")  
def read_file(ctx: RunContextWrapper[Any], path: str, directory: str | None = None) -> str:
    """Read the contents of a file.

    Args:
        path: The path to the file to read.
        directory: The directory to read the file from.
    """
    # In real life, we'd read the file from the file system
    return "<file contents>"


agent = Agent(
    name="Assistant",
    tools=[fetch_weather, read_file],  
)

for tool in agent.tools:
    if isinstance(tool, FunctionTool):
        print(tool.name)
        print(tool.description)
        print(json.dumps(tool.params_json_schema, indent=2))
        print()
        
## Custom function tools
from typing import Any

from pydantic import BaseModel

from agents import RunContextWrapper, FunctionTool



def do_some_work(data: str) -> str:
    return "done"


class FunctionArgs(BaseModel):
    username: str
    age: int


async def run_function(ctx: RunContextWrapper[Any], args: str) -> str:
    parsed = FunctionArgs.model_validate_json(args)
    return do_some_work(data=f"{parsed.username} is {parsed.age} years old")


tool = FunctionTool(
    name="process_user",
    description="Processes extracted user data",
    params_json_schema=FunctionArgs.model_json_schema(),
    on_invoke_tool=run_function,
)

## Agents as tools
from agents import Agent, Runner
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You translate the user's message to Spanish",
)

french_agent = Agent(
    name="French agent",
    instructions="You translate the user's message to French",
)

orchestrator_agent = Agent(
    name="orchestrator_agent",
    instructions=(
        "You are a translation agent. You use the tools given to you to translate."
        "If asked for multiple translations, you call the relevant tools."
    ),
    tools=[
        spanish_agent.as_tool(
            tool_name="translate_to_spanish",
            tool_description="Translate the user's message to Spanish",
        ),
        french_agent.as_tool(
            tool_name="translate_to_french",
            tool_description="Translate the user's message to French",
        ),
    ],
)

async def main():
    result = await Runner.run(orchestrator_agent, input="Say 'Hello, how are you?' in Spanish.")
    print(result.final_output)
    
## creating a handoff
from agents import Agent, handoff

billing_agent = Agent(name="Billing agent")
refund_agent = Agent(name="Refund agent")


triage_agent = Agent(name="Triage agent", handoffs=[billing_agent, handoff(refund_agent)])

## Customizing handoffs via the handoff() function
from agents import Agent, handoff, RunContextWrapper

def on_handoff(ctx: RunContextWrapper[None]):
    print("Handoff called")

agent = Agent(name="My agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    tool_name_override="custom_handoff_tool",
    tool_description_override="Custom description",
)

## Handoff inputs

from pydantic import BaseModel

from agents import Agent, handoff, RunContextWrapper

class EscalationData(BaseModel):
    reason: str

async def on_handoff(ctx: RunContextWrapper[None], input_data: EscalationData):
    print(f"Escalation agent called with reason: {input_data.reason}")

agent = Agent(name="Escalation agent")

handoff_obj = handoff(
    agent=agent,
    on_handoff=on_handoff,
    input_type=EscalationData,
)

## Input filters

from agents import Agent, handoff
from agents.extensions import handoff_filters

agent = Agent(name="FAQ agent")

handoff_obj = handoff(
    agent=agent,
    input_filter=handoff_filters.remove_all_tools, 
)

## Recommended prompts

from agents import Agent
from agents.extensions.handoff_prompt import RECOMMENDED_PROMPT_PREFIX

billing_agent = Agent(
    name="Billing agent",
    instructions=f"""{RECOMMENDED_PROMPT_PREFIX}
    <Fill in the rest of your prompt here>.""",
)

## Tracing

#The Agents SDK includes built-in tracing, collecting a comprehensive record of events during an agent run: LLM generations, tool calls, handoffs, guardrails, and even custom events that occur. Using the Traces dashboard, you can debug, visualize, and monitor your workflows during development and in production.

## Default tracing
#By default, the SDK traces the following:
#The entire Runner.{run, run_sync, run_streamed}() is wrapped in a trace().
#Each time an agent runs, it is wrapped in agent_span()
#LLM generations are wrapped in generation_span()
#Function tool calls are each wrapped in function_span()
#Guardrails are wrapped in guardrail_span()
#Handoffs are wrapped in handoff_span()
#By default, the trace is named "Agent trace". You can set this name if you use trace, or you can can configure the name and other properties with the RunConfig.
#In addition, you can set up custom trace processors to push traces to other destinations (as a replacement, or secondary destination).

## Higher level traces
from agents import Agent, Runner, trace

async def main():
    agent = Agent(name="Joke generator", instructions="Tell funny jokes.")

    with trace("Joke workflow"): 
        first_result = await Runner.run(agent, "Tell me a joke")
        second_result = await Runner.run(agent, f"Rate this joke: {first_result.final_output}")
        print(f"Joke: {first_result.final_output}")
        print(f"Rating: {second_result.final_output}")
        
## Context management
#Context is an overloaded term. There are two main classes of context you might care about:
#Context available locally to your code: this is data and dependencies you might need when tool functions run, during callbacks like on_handoff, in lifecycle hooks, etc.
#Context available to LLMs: this is data the LLM sees when generating a response.
#Local context
#This is represented via the RunContextWrapper class and the context property within it. The way this works is:
#You create any Python object you want. A common pattern is to use a dataclass or a Pydantic object.
#You pass that object to the various run methods (e.g. Runner.run(..., **context=whatever**)).
#All your tool calls, lifecycle hooks etc will be passed a wrapper object, RunContextWrapper[T], where T represents your context object type which you can access via wrapper.context.
#The most important thing to be aware of: every agent, tool function, lifecycle etc for a given agent run must use the same type of context.
#You can use the context for things like:
#Contextual data for your run (e.g. things like a username/uid or other information about the user)
#Dependencies (e.g. logger objects, data fetchers, etc)
#Helper functions

## example
import asyncio
from dataclasses import dataclass

from agents import Agent, RunContextWrapper, Runner, function_tool

@dataclass
class UserInfo:  
    name: str
    uid: int

@function_tool
async def fetch_user_age(wrapper: RunContextWrapper[UserInfo]) -> str:  
    return f"User {wrapper.context.name} is 47 years old"

async def main():
    user_info = UserInfo(name="John", uid=123)  

    agent = Agent[UserInfo](  
        name="Assistant",
        tools=[fetch_user_age],
    )

    result = await Runner.run(
        starting_agent=agent,
        input="What is the age of the user?",
        context=user_info,
    )

    print(result.final_output)  
    # The user John is 47 years old.

if __name__ == "__main__":
    asyncio.run(main())
    
##Agent/LLM context

#When an LLM is called, the only data it can see is from the conversation history. This means that if you want to make some new data available to the LLM, you must do it in a way that makes it available in that history. There are a few ways to do this:
#You can add it to the Agent instructions. This is also known as a "system prompt" or "developer message". System prompts can be static strings, or they can be dynamic functions that receive the context and output a string. This is a common tactic for information that is always useful (for example, the user's name or the current date).
#Add it to the input when calling the Runner.run functions. This is similar to the instructions tactic, but allows you to have messages that are lower in the chain of command.
#Expose it via function tools. This is useful for on-demand context - the LLM decides when it needs some data, and can call the tool to fetch that data.
#Use retrieval or web search. These are special tools that are able to fetch relevant data from files or databases (retrieval), or from the web (web search). This is useful for "grounding" the response in relevant contextual data.


## Implementing a guardrail

from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    InputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    TResponseInputItem,
    input_guardrail,
)

class MathHomeworkOutput(BaseModel):
    is_math_homework: bool
    reasoning: str

guardrail_agent = Agent( 
    name="Guardrail check",
    instructions="Check if the user is asking you to do their math homework.",
    output_type=MathHomeworkOutput,
)


@input_guardrail
async def math_guardrail( 
    ctx: RunContextWrapper[None], agent: Agent, input: str | list[TResponseInputItem]
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, input, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output, 
        tripwire_triggered=result.final_output.is_math_homework,
    )


agent = Agent(  
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    input_guardrails=[math_guardrail],
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except InputGuardrailTripwireTriggered:
        print("Math homework guardrail tripped")
        
## output guardrails

from pydantic import BaseModel
from agents import (
    Agent,
    GuardrailFunctionOutput,
    OutputGuardrailTripwireTriggered,
    RunContextWrapper,
    Runner,
    output_guardrail,
)
class MessageOutput(BaseModel): 
    response: str

class MathOutput(BaseModel): 
    is_math: bool
    reasoning: str

guardrail_agent = Agent(
    name="Guardrail check",
    instructions="Check if the output includes any math.",
    output_type=MathOutput,
)

@output_guardrail
async def math_guardrail(  
    ctx: RunContextWrapper, agent: Agent, output: MessageOutput
) -> GuardrailFunctionOutput:
    result = await Runner.run(guardrail_agent, output.response, context=ctx.context)

    return GuardrailFunctionOutput(
        output_info=result.final_output,
        tripwire_triggered=result.final_output.is_math,
    )

agent = Agent( 
    name="Customer support agent",
    instructions="You are a customer support agent. You help customers with their questions.",
    output_guardrails=[math_guardrail],
    output_type=MessageOutput,
)

async def main():
    # This should trip the guardrail
    try:
        await Runner.run(agent, "Hello, can you help me solve for x: 2x + 3 = 11?")
        print("Guardrail didn't trip - this is unexpected")

    except OutputGuardrailTripwireTriggered:
        print("Math output guardrail tripped")
        
## Orchestrating multiple agents

#Orchestration refers to the flow of agents in your app. Which agents run, in what order, and how do they decide what happens next? There are two main ways to orchestrate agents:

#Allowing the LLM to make decisions: this uses the intelligence of an LLM to plan, reason, and decide on what steps to take based on that.
#Orchestrating via code: determining the flow of agents via your code.
#You can mix and match these patterns. Each has their own tradeoffs, described below.

#Orchestrating via LLM

#An agent is an LLM equipped with instructions, tools and handoffs. This means that given an open-ended task, the LLM can autonomously plan how it will tackle the task, using tools to take actions and acquire data, and using handoffs to delegate tasks to sub-agents. For example, a research agent could be equipped with tools like:

#Web search to find information online
#File search and retrieval to search through proprietary data and connections
#Computer use to take actions on a computer
#Code execution to do data analysis
#Handoffs to specialized agents that are great at planning, report writing and more.
#This pattern is great when the task is open-ended and you want to rely on the intelligence of an LLM. The most important tactics here are:

#Invest in good prompts. Make it clear what tools are available, how to use them, and what parameters it must operate within.
#Monitor your app and iterate on it. See where things go wrong, and iterate on your prompts.
#Allow the agent to introspect and improve. For example, run it in a loop, and let it critique itself; or, provide error messages and let it improve.
#Have specialized agents that excel in one task, rather than having a general purpose agent that is expected to be good at anything.
#Invest in evals. This lets you train your agents to improve and get better at tasks.
#Orchestrating via code

#While orchestrating via LLM is powerful, orchestrating via code makes tasks more deterministic and predictable, in terms of speed, cost and performance. Common patterns here are:

#Using structured outputs to generate well formed data that you can inspect with your code. For example, you might ask an agent to classify the task into a few categories, and then pick the next agent based on the category.
#Chaining multiple agents by transforming the output of one into the input of the next. You can decompose a task like writing a blog post into a series of steps - do research, write an outline, write the blog post, critique it, and then improve it.
#Running the agent that performs the task in a while loop with an agent that evaluates and provides feedback, until the evaluator says the output passes certain criteria.
#Running multiple agents in parallel, e.g. via Python primitives like asyncio.gather. This is useful for speed when you have multiple tasks that don't depend on each other.

#Models

#The Agents SDK comes with out-of-the-box support for OpenAI models in two flavors:

#Recommended: the OpenAIResponsesModel, which calls OpenAI APIs using the new Responses API.
#The OpenAIChatCompletionsModel, which calls OpenAI APIs using the Chat Completions API.
#Mixing and matching models

#Within a single workflow, you may want to use different models for each agent. For example, you could use a smaller, faster model for triage, while using a larger, more capable model for complex tasks. When configuring an Agent, you can select a specific model by either:

#Passing the name of an OpenAI model.
#Passing any model name + a ModelProvider that can map that name to a Model instance.
#Directly providing a Model implementation.

## example

from agents import Agent, Runner, AsyncOpenAI, OpenAIChatCompletionsModel
import asyncio

spanish_agent = Agent(
    name="Spanish agent",
    instructions="You only speak Spanish.",
    model="o3-mini", 
)

english_agent = Agent(
    name="English agent",
    instructions="You only speak English",
    model=OpenAIChatCompletionsModel( 
        model="gpt-4o",
        openai_client=AsyncOpenAI()
    ),
)

triage_agent = Agent(
    name="Triage agent",
    instructions="Handoff to the appropriate agent based on the language of the request.",
    handoffs=[spanish_agent, english_agent],
    model="gpt-3.5-turbo",
)

async def main():
    result = await Runner.run(triage_agent, input="Hola, cmo ests?")
    print(result.final_output)
    
## API keys and clients
from agents import set_default_openai_key

set_default_openai_key("sk-...")

## alternative
from openai import AsyncOpenAI
from agents import set_default_openai_client

custom_client = AsyncOpenAI(base_url="...", api_key="...")
set_default_openai_client(custom_client)

#Finally, you can also customize the OpenAI API that is used. By default, we use the OpenAI Responses API. You can override this to use the Chat Completions API by using the set_default_openai_api() function.
from agents import set_default_openai_api

set_default_openai_api("chat_completions")

## Debug logging
from agents import enable_verbose_stdout_logging

enable_verbose_stdout_logging()

## Custom logging
import logging

logger =  logging.getLogger("openai.agents") # or openai.agents.tracing for the Tracing logger

# To make all logs show up
logger.setLevel(logging.DEBUG)
# To make info and above show up
logger.setLevel(logging.INFO)
# To make warning and above show up
logger.setLevel(logging.WARNING)
# etc

# You can customize this as needed, but this will output to `stderr` by default
logger.addHandler(logging.StreamHandler())

## simple agent
from agents import Agent, Runner

agent = Agent(
    name="Assistant",
    instructions="You are a helpful assistant."
)

async def main():
    result = await Runner.run(agent, "Write a haiku about recursion in programming.")
    print(result.final_output)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

## Example of Creating an Agent with a Tool:

from agents import Agent, function_tool, Runner

# Define a custom tool using the function_tool decorator
@function_tool
def add_numbers(a: int, b: int) -> int:
    """Add two numbers."""
    return a + b

# Create an agent with specific instructions and the custom tool
agent = Agent(
    name="CalculatorAgent",
    instructions="You are a calculator agent that adds two numbers.",
    tools=[add_numbers],
)

# Run the agent with a sample input
async def main():
    result = await Runner.run(agent, "Add 5 and 7.")
    print(result.final_output)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
    
## How to Run Agents:
#Agents in OpenAIs SDK are executed using the Runner class, which provides three primary methods:
#Runner.run()  Standard async run (basic usage)
#Runner.run_streamed() for receiving streaming results
#Runner.run_sync() for synchronous agent execution (blocking call)

## Streaming execution
from agents import Agent, Runner
import asyncio

async def main():
    agent = Agent(name="Streaming Assistant", instructions="Answer concisely.")
    
    # Run agent with streaming
    async for event in Runner.run_streamed(agent, "Tell me about recursion."):
        print(event)

if __name__ == "__main__":
    asyncio.run(main())
    
## Running Agents in Conversational Mode
from agents import Agent, Runner, trace
import asyncio

async def main():
    thread_id = "conversation_12345"  # Unique ID for conversation

    agent = Agent(
        name="Concise Assistant",
        instructions="Reply very concisely."
    )

    with trace(workflow_name="Chat Session", group_id=thread_id):
        # First turn
        result1 = await Runner.run(agent, "What city is the Golden Gate Bridge in?")
        print("Assistant:", result1.final_output)

        # Second turn, using previous output as context
        next_input = result1.final_output
        result2 = await Runner.run(agent, f"Provide one fun fact about {result1.final_output}.")
        print(result1.final_output)
        print(result2.final_output)
```

My question: Hi there, Can you see what is currently opened in the editor?
2025-04-01 22:34:18,734 - terminator_agents - INFO - Added 7944 tokens for query, total: 7944
2025-04-01 22:34:18,736 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: search.*file
2025-04-01 22:34:20,660 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:34:28,802 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:34:28,977 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 22:34:31,343 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:36:31,835 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 22:36:31,844 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 22:36:31,844 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 22:36:31,845 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 22:36:31,846 - root - INFO - AI panel initialized successfully
2025-04-01 22:36:35,351 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:37:12,075 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 22:37:12,088 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 22:37:12,089 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 22:37:12,089 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 22:37:12,092 - root - INFO - AI panel initialized successfully
2025-04-01 22:37:14,328 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:37:26,663 - root - ERROR - Error opening file: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2682, in on_directory_tree_file_selected
    editor.language = language
    ^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 759, in _watch_language
    self._set_document(self.document.text, language)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 923, in _set_document
    raise LanguageDoesNotExist(
textual.widgets._text_area.LanguageDoesNotExist: tree-sitter is available, but no built-in or user-registered language called 'text'.
Ensure the language is installed (e.g. `pip install tree-sitter-ruby`)
Falling back to plain text.
2025-04-01 22:37:57,577 - terminator_agents - INFO - Processing agent query: This is a test.
2025-04-01 22:37:57,587 - terminator_agents - INFO - Added 4 tokens for query, total: 4
2025-04-01 22:38:01,924 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:38:02,885 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:38:04,435 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:38:08,750 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:38:09,924 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:38:14,513 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:38:15,580 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:38:15,737 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 22:38:20,235 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:43:46,518 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 22:43:46,530 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 22:43:46,530 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 22:43:46,531 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 22:43:46,535 - root - INFO - AI panel initialized successfully
2025-04-01 22:44:02,164 - terminator_agents - INFO - Processing agent query: Can you give me some python code?
2025-04-01 22:44:02,168 - terminator_agents - INFO - Added 7 tokens for query, total: 7
2025-04-01 22:44:06,297 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:44:09,349 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:44:11,934 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:44:12,380 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:44:12,541 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 22:44:18,075 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:46:20,099 - terminator_agents - INFO - Processing agent query: Can you search the file count_words.py ?
2025-04-01 22:46:20,121 - terminator_agents - INFO - Added 7 tokens for query, total: 14
2025-04-01 22:46:20,140 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: search.*file
2025-04-01 22:46:20,207 - openai._base_client - INFO - Retrying request to /responses in 0.398374 seconds
2025-04-01 22:46:21,039 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:46:25,534 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:46:25,686 - terminator_agents - INFO - Searching for file: 'count_words.py', starting from: '/Users/kevinvanosch/Documents/TextualAgents/Textual', recursive: True
2025-04-01 22:46:26,739 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:46:29,505 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:46:29,657 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 22:46:32,470 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:47:39,583 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:48:25,073 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2693, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 22:48:55,325 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
"""
TerminatorV1 Tools - Utility tools for the Terminator IDE
Provides file operations, code analysis, git integration, and real-time collaboration tools
"""

import os
import re
import sys
import json
import subprocess
import difflib
import tempfile
import asyncio
import uuid
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Callable
from dataclasses import dataclass
import websockets

# File System Utilities
class FileSystem:
    """File system utilities for Terminator"""
    
    @staticmethod
    def read_file(file_path: str, max_size_mb: int = 10) -> Tuple[bool, str]:
        """
        Read a file safely
        
        Args:
            file_path: Path to the file
            max_size_mb: Maximum file size in MB
            
        Returns:
            Tuple of (success, content/error_message)
        """
        try:
            file_path = os.path.expanduser(file_path)
            
            if not os.path.exists(file_path):
                return False, f"File not found: {file_path}"
                
            if os.path.isdir(file_path):
                return False, f"Path is a directory, not a file: {file_path}"
                
            # Check file size
            file_size = os.path.getsize(file_path)
            if file_size > max_size_mb * 1024 * 1024:
                return False, f"File too large: {file_size / (1024*1024):.2f} MB (max: {max_size_mb} MB)"
                
            with open(file_path, 'r', encoding='utf-8', errors='replace') as file:
                content = file.read()
                
            return True, content
            
        except Exception as e:
            return False, f"Error reading file: {str(e)}"
    
    @staticmethod
    def write_file(file_path: str, content: str, create_dirs: bool = True) -> Tuple[bool, str]:
        """
        Write content to a file
        
        Args:
            file_path: Path to the file
            content: Content to write
            create_dirs: Whether to create parent directories
            
        Returns:
            Tuple of (success, message)
        """
        try:
            file_path = os.path.expanduser(file_path)
            
            # Create parent directories if they don't exist
            if create_dirs:
                os.makedirs(os.path.dirname(os.path.abspath(file_path)), exist_ok=True)
                
            with open(file_path, 'w', encoding='utf-8') as file:
                file.write(content)
                
            return True, f"File saved: {file_path}"
            
        except Exception as e:
            return False, f"Error writing file: {str(e)}"
    
    @staticmethod
    def get_directory_structure(path: str, max_depth: int = 3) -> Dict[str, Any]:
        """
        Get a nested dictionary representing the directory structure
        
        Args:
            path: Directory path
            max_depth: Maximum recursion depth
            
        Returns:
            Directory structure as a nested dictionary
        """
        result = {"dirs": {}, "files": []}
        
        try:
            path = os.path.expanduser(path)
            
            if not os.path.isdir(path):
                return {"error": f"Not a directory: {path}"}
                
            def scan_directory(dir_path, current_depth=0):
                if current_depth > max_depth:
                    return {"dirs": {}, "files": ["... (max depth reached)"]}
                    
                result = {"dirs": {}, "files": []}
                
                try:
                    for entry in os.scandir(dir_path):
                        if entry.is_dir():
                            result["dirs"][entry.name] = scan_directory(
                                entry.path, current_depth + 1
                            )
                        else:
                            result["files"].append(entry.name)
                except PermissionError:
                    return {"error": "Permission denied"}
                    
                return result
                
            return scan_directory(path)
            
        except Exception as e:
            return {"error": str(e)}

# Code Analysis Utilities
class CodeAnalyzer:
    """Code analysis utilities"""
    
    @staticmethod
    def analyze_python_code(code: str) -> Dict[str, Any]:
        """
        Analyze Python code for common issues
        
        Args:
            code: Python code to analyze
            
        Returns:
            Analysis results
        """
        issues = []
        
        # Check for simple issues
        lines = code.splitlines()
        
        # Check for long lines
        for i, line in enumerate(lines):
            if len(line) > 100:
                issues.append({
                    "line": i + 1,
                    "type": "style",
                    "message": f"Line too long ({len(line)} > 100 characters)"
                })
        
        # Check for missing docstrings
        if len(lines) > 1 and not any(line.strip().startswith('"""') for line in lines[:5]):
            issues.append({
                "line": 1,
                "type": "style",
                "message": "Missing module docstring"
            })
        
        # Check for function definitions without docstrings
        function_pattern = r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\((.*?)\)(?:\s*->.*?)?:'
        for i, line in enumerate(lines):
            match = re.search(function_pattern, line)
            if match:
                func_name = match.group(1)
                
                # Check next lines for docstring
                has_docstring = False
                for j in range(i+1, min(i+4, len(lines))):
                    if lines[j].strip().startswith('"""'):
                        has_docstring = True
                        break
                
                if not has_docstring:
                    issues.append({
                        "line": i + 1,
                        "type": "style",
                        "message": f"Missing docstring for function '{func_name}'"
                    })
        
        # Try to use pylint if available
        try:
            with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as temp:
                temp_path = temp.name
                temp.write(code.encode('utf-8'))
            
            try:
                result = subprocess.run(
                    ['pylint', '--output-format=json', '--exit-zero', temp_path],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                
                if result.stdout:
                    try:
                        pylint_issues = json.loads(result.stdout)
                        for issue in pylint_issues:
                            issues.append({
                                "line": issue.get("line", 0),
                                "type": issue.get("type", "unknown"),
                                "message": issue.get("message", "Unknown issue")
                            })
                    except json.JSONDecodeError:
                        pass
                        
            except (subprocess.TimeoutExpired, subprocess.SubprocessError):
                pass
                
            finally:
                os.unlink(temp_path)
                
        except Exception:
            pass
        
        return {
            "issues": issues,
            "issue_count": len(issues),
            "recommendations": [
                "Add docstrings to all functions and modules",
                "Keep lines under 100 characters",
                "Use meaningful variable names"
            ] if issues else []
        }
    
    @staticmethod
    def create_diff(original: str, modified: str) -> str:
        """
        Create a unified diff between original and modified text
        
        Args:
            original: Original text
            modified: Modified text
            
        Returns:
            Unified diff as a string
        """
        original_lines = original.splitlines(True)
        modified_lines = modified.splitlines(True)
        
        diff = difflib.unified_diff(
            original_lines,
            modified_lines,
            fromfile="Original",
            tofile="Modified",
            n=3
        )
        
        return ''.join(diff)
    
    @staticmethod
    def count_code_lines(code: str) -> Dict[str, int]:
        """
        Count lines of code, comments and blank lines
        
        Args:
            code: Code to analyze
            
        Returns:
            Counts of different line types
        """
        lines = code.splitlines()
        
        blank_lines = 0
        comment_lines = 0
        code_lines = 0
        
        for line in lines:
            stripped = line.strip()
            if not stripped:
                blank_lines += 1
            elif stripped.startswith('#'):
                comment_lines += 1
            else:
                code_lines += 1
                
        return {
            "total_lines": len(lines),
            "code_lines": code_lines,
            "comment_lines": comment_lines,
            "blank_lines": blank_lines
        }

# Git Utilities
class GitManager:
    """Git integration utilities"""
    
    @staticmethod
    def check_git_repo(path: str) -> Tuple[bool, Optional[str]]:
        """
        Check if a directory is a Git repository
        
        Args:
            path: Directory path to check
            
        Returns:
            Tuple of (is_repo, repo_root)
        """
        try:
            # Try to find .git directory
            current_path = os.path.abspath(path)
            while current_path != os.path.dirname(current_path):  # Stop at filesystem root
                if os.path.exists(os.path.join(current_path, '.git')):
                    return True, current_path
                current_path = os.path.dirname(current_path)
                
            return False, None
            
        except Exception:
            return False, None
            
    @staticmethod
    def get_branches(repo_path: str) -> Dict[str, Any]:
        """
        Get list of branches and the current branch
        
        Args:
            repo_path: Path to Git repository
            
        Returns:
            Dictionary with branch information
        """
        try:
            # Get current branch
            result = subprocess.run(
                ['git', 'branch', '--show-current'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return {"error": f"Git error: {result.stderr}"}
                
            current_branch = result.stdout.strip()
            
            # Get all branches
            result = subprocess.run(
                ['git', 'branch', '-a'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return {"error": f"Git error: {result.stderr}"}
                
            all_branches = []
            remote_branches = []
            
            for line in result.stdout.splitlines():
                branch_name = line.strip()
                if branch_name.startswith('*'):
                    # Current branch, already captured
                    continue
                    
                if branch_name.startswith('remotes/'):
                    # Remote branch
                    remote_branch = branch_name.strip().replace('remotes/', '', 1)
                    remote_branches.append(remote_branch)
                else:
                    # Local branch
                    all_branches.append(branch_name.strip())
            
            return {
                "current_branch": current_branch,
                "local_branches": all_branches,
                "remote_branches": remote_branches
            }
            
        except Exception as e:
            return {"error": str(e)}
            
    @staticmethod
    def get_branch_graph(repo_path: str, max_commits: int = 20) -> Dict[str, Any]:
        """
        Get branch graph information for visualization
        
        Args:
            repo_path: Path to Git repository
            max_commits: Maximum number of commits to include
            
        Returns:
            Dictionary with branch graph data
        """
        try:
            # Get graph data using git log with graph format
            result = subprocess.run(
                ['git', 'log', '--graph', '--oneline', '--decorate', '--all', f'-n{max_commits}'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return {"error": f"Git error: {result.stderr}"}
                
            # Get commit data for more details
            commit_result = subprocess.run(
                ['git', 'log', '--pretty=format:%H|%an|%ad|%s', '--date=short', f'-n{max_commits}'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if commit_result.returncode != 0:
                return {"error": f"Git error: {commit_result.stderr}"}
                
            # Parse commit data
            commits = []
            for line in commit_result.stdout.splitlines():
                parts = line.split('|', 3)
                if len(parts) >= 4:
                    commit_hash, author, date, message = parts
                    commits.append({
                        "hash": commit_hash,
                        "short_hash": commit_hash[:7],
                        "author": author,
                        "date": date,
                        "message": message
                    })
            
            # Get branch structure
            branch_data = GitManager.get_branches(repo_path)
            
            return {
                "graph_output": result.stdout,
                "commits": commits,
                "branches": branch_data
            }
            
        except Exception as e:
            return {"error": str(e)}
            
    @staticmethod
    def switch_branch(repo_path: str, branch_name: str) -> Tuple[bool, str]:
        """
        Switch to a different branch
        
        Args:
            repo_path: Path to Git repository
            branch_name: Name of the branch to switch to
            
        Returns:
            Tuple of (success, message)
        """
        try:
            # Check if branch exists
            result = subprocess.run(
                ['git', 'show-ref', '--verify', '--quiet', f'refs/heads/{branch_name}'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            branch_exists = result.returncode == 0
            
            if branch_exists:
                # Checkout existing branch
                result = subprocess.run(
                    ['git', 'checkout', branch_name],
                    capture_output=True,
                    text=True,
                    cwd=repo_path
                )
            else:
                # Check if it's a remote branch
                result = subprocess.run(
                    ['git', 'show-ref', '--verify', '--quiet', f'refs/remotes/origin/{branch_name}'],
                    capture_output=True,
                    text=True,
                    cwd=repo_path
                )
                
                remote_exists = result.returncode == 0
                
                if remote_exists:
                    # Checkout remote branch
                    result = subprocess.run(
                        ['git', 'checkout', '-b', branch_name, f'origin/{branch_name}'],
                        capture_output=True,
                        text=True,
                        cwd=repo_path
                    )
                else:
                    # Create and checkout new branch
                    result = subprocess.run(
                        ['git', 'checkout', '-b', branch_name],
                        capture_output=True,
                        text=True,
                        cwd=repo_path
                    )
            
            if result.returncode != 0:
                return False, f"Failed to switch branch: {result.stderr}"
                
            return True, f"Switched to branch '{branch_name}'"
            
        except Exception as e:
            return False, f"Error switching branch: {str(e)}"
            
    @staticmethod
    def create_branch(repo_path: str, branch_name: str) -> Tuple[bool, str]:
        """
        Create a new branch
        
        Args:
            repo_path: Path to Git repository
            branch_name: Name of the new branch
            
        Returns:
            Tuple of (success, message)
        """
        try:
            # Create and checkout new branch
            result = subprocess.run(
                ['git', 'checkout', '-b', branch_name],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return False, f"Failed to create branch: {result.stderr}"
                
            return True, f"Created and switched to branch '{branch_name}'"
            
        except Exception as e:
            return False, f"Error creating branch: {str(e)}"
    
    @staticmethod
    def get_git_status(repo_path: str) -> Dict[str, Any]:
        """
        Get Git repository status
        
        Args:
            repo_path: Path to Git repository
            
        Returns:
            Git status information
        """
        try:
            result = subprocess.run(
                ['git', 'status', '--porcelain'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return {"error": f"Git error: {result.stderr}"}
                
            modified_files = []
            untracked_files = []
            staged_files = []
            
            for line in result.stdout.splitlines():
                status = line[:2]
                filename = line[3:]
                
                if status.startswith('M'):
                    modified_files.append(filename)
                elif status.startswith('A'):
                    staged_files.append(filename)
                elif status.startswith('??'):
                    untracked_files.append(filename)
                else:
                    # Other statuses (deleted, renamed, etc.)
                    staged_files.append(filename)
                    
            return {
                "clean": len(result.stdout.strip()) == 0,
                "modified_files": modified_files,
                "untracked_files": untracked_files,
                "staged_files": staged_files
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    @staticmethod
    def git_commit(repo_path: str, message: str) -> Tuple[bool, str]:
        """
        Commit changes to Git repository
        
        Args:
            repo_path: Path to Git repository
            message: Commit message
            
        Returns:
            Tuple of (success, message)
        """
        try:
            result = subprocess.run(
                ['git', 'commit', '-m', message],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return False, f"Commit failed: {result.stderr}"
                
            return True, result.stdout
            
        except Exception as e:
            return False, f"Commit error: {str(e)}"
            
    @staticmethod
    def git_push(repo_path: str) -> Tuple[bool, str]:
        """
        Push changes to remote repository
        
        Args:
            repo_path: Path to Git repository
            
        Returns:
            Tuple of (success, message)
        """
        try:
            result = subprocess.run(
                ['git', 'push'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return False, f"Push failed: {result.stderr}"
                
            return True, result.stdout
            
        except Exception as e:
            return False, f"Push error: {str(e)}"
            
    @staticmethod
    def git_pull(repo_path: str) -> Tuple[bool, str]:
        """
        Pull changes from remote repository
        
        Args:
            repo_path: Path to Git repository
            
        Returns:
            Tuple of (success, message)
        """
        try:
            result = subprocess.run(
                ['git', 'pull'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return False, f"Pull failed: {result.stderr}"
                
            return True, result.stdout
            
        except Exception as e:
            return False, f"Pull error: {str(e)}"

# Debugger Utilities
class PythonDebugger:
    """Python debugger utilities for code debugging and inspection"""
    
    @staticmethod
    def start_debugging(file_path: str) -> Tuple[bool, Dict[str, Any]]:
        """
        Start a debugging session for a Python file
        
        Args:
            file_path: Path to the Python file to debug
            
        Returns:
            Tuple of (success, session_info)
        """
        try:
            if not file_path.endswith('.py'):
                return False, {"error": "Only Python files can be debugged"}
                
            if not os.path.exists(file_path):
                return False, {"error": f"File not found: {file_path}"}
                
            # Setup debugging session using pdb
            import pdb
            import sys
            import io
            import threading
            import importlib.util
            from types import ModuleType
            
            # Create a debug info object
            debug_session = {
                "file_path": file_path,
                "active": True,
                "current_line": 1,
                "variables": {},
                "output": "",
                "call_stack": [],
                "breakpoints": []
            }
            
            # In a real implementation, we would launch a proper
            # debug session with a subprocess using pdb or similar
            
            # For demonstration, we'll simulate a debug session
            with open(file_path, 'r', encoding='utf-8') as f:
                code_lines = f.readlines()
                
            debug_session["code"] = "".join(code_lines)
            debug_session["line_count"] = len(code_lines)
            
            # Add some sample data for demonstration
            debug_session["variables"] = {
                "x": {"type": "int", "value": "10"},
                "y": {"type": "str", "value": '"hello"'},
                "my_list": {"type": "list", "value": "[1, 2, 3]"}
            }
            
            debug_session["call_stack"] = [
                {"frame": 0, "function": "main", "file": os.path.basename(file_path), "line": 1},
                {"frame": 1, "function": "<module>", "file": os.path.basename(file_path), "line": 1}
            ]
            
            return True, debug_session
            
        except Exception as e:
            return False, {"error": f"Error starting debugger: {str(e)}"}
    
    @staticmethod
    def set_breakpoint(file_path: str, line: int) -> Tuple[bool, str]:
        """
        Set a breakpoint at a specific line
        
        Args:
            file_path: Path to the Python file
            line: Line number for the breakpoint
            
        Returns:
            Tuple of (success, message)
        """
        try:
            if not os.path.exists(file_path):
                return False, f"File not found: {file_path}"
                
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            if line < 1 or line > len(lines):
                return False, f"Line number out of range: {line}"
                
            # In a real implementation, we'd actually set the breakpoint
            # using pdb or a proper debugger
            
            return True, f"Breakpoint set at line {line}"
            
        except Exception as e:
            return False, f"Error setting breakpoint: {str(e)}"
    
    @staticmethod
    def debug_step(debug_session: Dict[str, Any], command: str) -> Dict[str, Any]:
        """
        Execute a debug step command
        
        Args:
            debug_session: The current debug session info
            command: Debug command ('step', 'next', 'continue', etc.)
            
        Returns:
            Updated debug session info
        """
        # Simulate stepping through code
        if not debug_session.get("active", False):
            debug_session["error"] = "No active debugging session"
            return debug_session
            
        current_line = debug_session.get("current_line", 1)
        line_count = debug_session.get("line_count", 1)
        
        if command == "step_over" or command == "next":
            # Move to the next line
            debug_session["current_line"] = min(current_line + 1, line_count)
            
        elif command == "step_into" or command == "step":
            # Simulate stepping into a function
            debug_session["current_line"] = min(current_line + 1, line_count)
            
            # Add a frame to the call stack if appropriate
            if current_line + 1 <= line_count and "def " in debug_session.get("code", "").splitlines()[current_line]:
                func_name = debug_session.get("code", "").splitlines()[current_line].strip().split("def ")[1].split("(")[0]
                debug_session["call_stack"].insert(0, {
                    "frame": 0,
                    "function": func_name,
                    "file": os.path.basename(debug_session.get("file_path", "")),
                    "line": current_line + 1
                })
                
                # Increment other frames
                for i in range(1, len(debug_session["call_stack"])):
                    debug_session["call_stack"][i]["frame"] = i
                
        elif command == "step_out" or command == "return":
            # Simulate stepping out of a function
            debug_session["current_line"] = min(current_line + 1, line_count)
            
            # Remove a frame from the call stack if there are multiple frames
            if len(debug_session.get("call_stack", [])) > 1:
                debug_session["call_stack"] = debug_session["call_stack"][1:]
                
                # Update frame numbers
                for i in range(len(debug_session["call_stack"])):
                    debug_session["call_stack"][i]["frame"] = i
                    
        elif command == "continue":
            # Simulate continuing to the next breakpoint (or end)
            debug_session["current_line"] = line_count
            
        # Update variables to simulate program execution
        if debug_session["current_line"] % 3 == 0:
            # Change some variables periodically to simulate program flow
            debug_session["variables"]["x"] = {"type": "int", "value": str(int(debug_session["variables"]["x"]["value"]) + 1)}
            
        # Check if we've reached the end
        if debug_session["current_line"] >= line_count:
            debug_session["active"] = False
            debug_session["output"] += "\nProgram execution completed."
            
        return debug_session
    
    @staticmethod
    def stop_debugging(debug_session: Dict[str, Any]) -> Dict[str, Any]:
        """
        Stop a debugging session
        
        Args:
            debug_session: The current debug session info
            
        Returns:
            Final debug session info with active=False
        """
        debug_session["active"] = False
        debug_session["output"] += "\nDebugging session terminated."
        return debug_session
    
    @staticmethod
    def evaluate_expression(debug_session: Dict[str, Any], expression: str) -> Tuple[bool, Any]:
        """
        Evaluate an expression in the current debug context
        
        Args:
            debug_session: The current debug session info
            expression: The expression to evaluate
            
        Returns:
            Tuple of (success, result/error)
        """
        try:
            # In a real implementation, we'd use the actual debugger's evaluation
            # For demonstration, simulate simple expression evaluation
            
            if expression in debug_session.get("variables", {}):
                var_info = debug_session["variables"][expression]
                return True, f"{var_info['value']} ({var_info['type']})"
            
            if expression == "len(my_list)":
                return True, "3"
                
            return False, f"Cannot evaluate '{expression}' in current context"
            
        except Exception as e:
            return False, f"Error evaluating expression: {str(e)}"

# Python Execution
class PythonRunner:
    """Python code execution utilities"""
    
    @staticmethod
    def run_code(code: str, timeout: int = 10) -> Dict[str, Any]:
        """
        Run Python code safely
        
        Args:
            code: Python code to run
            timeout: Maximum execution time in seconds
            
        Returns:
            Execution results
        """
        try:
            # Create a temporary file
            with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as temp:
                temp_path = temp.name
                temp.write(code.encode('utf-8'))
            
            try:
                # Run the code with timeout
                result = subprocess.run(
                    [sys.executable, temp_path],
                    capture_output=True,
                    text=True,
                    timeout=timeout
                )
                
                return {
                    "success": result.returncode == 0,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "returncode": result.returncode
                }
                
            except subprocess.TimeoutExpired:
                return {
                    "success": False,
                    "error": f"Execution timed out after {timeout} seconds"
                }
                
            finally:
                # Clean up
                os.unlink(temp_path)
                
        except Exception as e:
            return {
                "success": False,
                "error": f"Execution error: {str(e)}"
            }


# Real-time Collaboration Manager
# Using enhanced implementation from terminator/collaboration module
from terminator.collaboration.adapter import CollaborationManager, CollaborationSession

# These classes provide:
# - Operational Transform (OT) for conflict-free editing
# - Document chunking for large files (>1MB)
# - Connection pooling for better performance
# - Reliable message delivery with acknowledgments
# - Performance optimizations for multiple users
```

My question: Help me debug the code that is in the editor?
2025-04-01 22:48:55,331 - terminator_agents - INFO - Added 2547 tokens for query, total: 2561
2025-04-01 22:48:55,337 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-01 22:48:55,368 - openai._base_client - INFO - Retrying request to /responses in 0.407471 seconds
2025-04-01 22:48:56,186 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:49:16,028 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:49:17,204 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:49:34,638 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 22:49:34,794 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 22:49:38,075 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 22:53:20,935 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
"""
TerminatorV1 Tools - Utility tools for the Terminator IDE
Provides file operations, code analysis, git integration, and real-time collaboration tools
"""

import os
import re
import sys
import json
import subprocess
import difflib
import tempfile
import asyncio
import uuid
import time
from pathlib import Path
from typing import List, Dict, Any, Optional, Union, Tuple, Set, Callable
from dataclasses import dataclass
import websockets

# File System Utilities
class FileSystem:
    """File system utilities for Terminator"""
    
    @staticmethod
    def read_file(file_path: str, max_size_mb: int = 10) -> Tuple[bool, str]:
        """
        Read a file safely
        
        Args:
            file_path: Path to the file
            max_size_mb: Maximum file size in MB
            
        Returns:
            Tuple of (success, content/error_message)
        """
        try:
            file_path = os.path.expanduser(file_path)
            
            if not os.path.exists(file_path):
                return False, f"File not found: {file_path}"
                
            if os.path.isdir(file_path):
                return False, f"Path is a directory, not a file: {file_path}"
                
            # Check file size
            file_size = os.path.getsize(file_path)
            if file_size > max_size_mb * 1024 * 1024:
                return False, f"File too large: {file_size / (1024*1024):.2f} MB (max: {max_size_mb} MB)"
                
            with open(file_path, 'r', encoding='utf-8', errors='replace') as file:
                content = file.read()
                
            return True, content
            
        except Exception as e:
            return False, f"Error reading file: {str(e)}"
    
    @staticmethod
    def write_file(file_path: str, content: str, create_dirs: bool = True) -> Tuple[bool, str]:
        """
        Write content to a file
        
        Args:
            file_path: Path to the file
            content: Content to write
            create_dirs: Whether to create parent directories
            
        Returns:
            Tuple of (success, message)
        """
        try:
            file_path = os.path.expanduser(file_path)
            
            # Create parent directories if they don't exist
            if create_dirs:
                os.makedirs(os.path.dirname(os.path.abspath(file_path)), exist_ok=True)
                
            with open(file_path, 'w', encoding='utf-8') as file:
                file.write(content)
                
            return True, f"File saved: {file_path}"
            
        except Exception as e:
            return False, f"Error writing file: {str(e)}"
    
    @staticmethod
    def get_directory_structure(path: str, max_depth: int = 3) -> Dict[str, Any]:
        """
        Get a nested dictionary representing the directory structure
        
        Args:
            path: Directory path
            max_depth: Maximum recursion depth
            
        Returns:
            Directory structure as a nested dictionary
        """
        result = {"dirs": {}, "files": []}
        
        try:
            path = os.path.expanduser(path)
            
            if not os.path.isdir(path):
                return {"error": f"Not a directory: {path}"}
                
            def scan_directory(dir_path, current_depth=0):
                if current_depth > max_depth:
                    return {"dirs": {}, "files": ["... (max depth reached)"]}
                    
                result = {"dirs": {}, "files": []}
                
                try:
                    for entry in os.scandir(dir_path):
                        if entry.is_dir():
                            result["dirs"][entry.name] = scan_directory(
                                entry.path, current_depth + 1
                            )
                        else:
                            result["files"].append(entry.name)
                except PermissionError:
                    return {"error": "Permission denied"}
                    
                return result
                
            return scan_directory(path)
            
        except Exception as e:
            return {"error": str(e)}

# Code Analysis Utilities
class CodeAnalyzer:
    """Code analysis utilities"""
    
    @staticmethod
    def analyze_python_code(code: str) -> Dict[str, Any]:
        """
        Analyze Python code for common issues
        
        Args:
            code: Python code to analyze
            
        Returns:
            Analysis results
        """
        issues = []
        
        # Check for simple issues
        lines = code.splitlines()
        
        # Check for long lines
        for i, line in enumerate(lines):
            if len(line) > 100:
                issues.append({
                    "line": i + 1,
                    "type": "style",
                    "message": f"Line too long ({len(line)} > 100 characters)"
                })
        
        # Check for missing docstrings
        if len(lines) > 1 and not any(line.strip().startswith('"""') for line in lines[:5]):
            issues.append({
                "line": 1,
                "type": "style",
                "message": "Missing module docstring"
            })
        
        # Check for function definitions without docstrings
        function_pattern = r'def\s+([a-zA-Z_][a-zA-Z0-9_]*)\s*\((.*?)\)(?:\s*->.*?)?:'
        for i, line in enumerate(lines):
            match = re.search(function_pattern, line)
            if match:
                func_name = match.group(1)
                
                # Check next lines for docstring
                has_docstring = False
                for j in range(i+1, min(i+4, len(lines))):
                    if lines[j].strip().startswith('"""'):
                        has_docstring = True
                        break
                
                if not has_docstring:
                    issues.append({
                        "line": i + 1,
                        "type": "style",
                        "message": f"Missing docstring for function '{func_name}'"
                    })
        
        # Try to use pylint if available
        try:
            with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as temp:
                temp_path = temp.name
                temp.write(code.encode('utf-8'))
            
            try:
                result = subprocess.run(
                    ['pylint', '--output-format=json', '--exit-zero', temp_path],
                    capture_output=True,
                    text=True,
                    timeout=5
                )
                
                if result.stdout:
                    try:
                        pylint_issues = json.loads(result.stdout)
                        for issue in pylint_issues:
                            issues.append({
                                "line": issue.get("line", 0),
                                "type": issue.get("type", "unknown"),
                                "message": issue.get("message", "Unknown issue")
                            })
                    except json.JSONDecodeError:
                        pass
                        
            except (subprocess.TimeoutExpired, subprocess.SubprocessError):
                pass
                
            finally:
                os.unlink(temp_path)
                
        except Exception:
            pass
        
        return {
            "issues": issues,
            "issue_count": len(issues),
            "recommendations": [
                "Add docstrings to all functions and modules",
                "Keep lines under 100 characters",
                "Use meaningful variable names"
            ] if issues else []
        }
    
    @staticmethod
    def create_diff(original: str, modified: str) -> str:
        """
        Create a unified diff between original and modified text
        
        Args:
            original: Original text
            modified: Modified text
            
        Returns:
            Unified diff as a string
        """
        original_lines = original.splitlines(True)
        modified_lines = modified.splitlines(True)
        
        diff = difflib.unified_diff(
            original_lines,
            modified_lines,
            fromfile="Original",
            tofile="Modified",
            n=3
        )
        
        return ''.join(diff)
    
    @staticmethod
    def count_code_lines(code: str) -> Dict[str, int]:
        """
        Count lines of code, comments and blank lines
        
        Args:
            code: Code to analyze
            
        Returns:
            Counts of different line types
        """
        lines = code.splitlines()
        
        blank_lines = 0
        comment_lines = 0
        code_lines = 0
        
        for line in lines:
            stripped = line.strip()
            if not stripped:
                blank_lines += 1
            elif stripped.startswith('#'):
                comment_lines += 1
            else:
                code_lines += 1
                
        return {
            "total_lines": len(lines),
            "code_lines": code_lines,
            "comment_lines": comment_lines,
            "blank_lines": blank_lines
        }

# Git Utilities
class GitManager:
    """Git integration utilities"""
    
    @staticmethod
    def check_git_repo(path: str) -> Tuple[bool, Optional[str]]:
        """
        Check if a directory is a Git repository
        
        Args:
            path: Directory path to check
            
        Returns:
            Tuple of (is_repo, repo_root)
        """
        try:
            # Try to find .git directory
            current_path = os.path.abspath(path)
            while current_path != os.path.dirname(current_path):  # Stop at filesystem root
                if os.path.exists(os.path.join(current_path, '.git')):
                    return True, current_path
                current_path = os.path.dirname(current_path)
                
            return False, None
            
        except Exception:
            return False, None
            
    @staticmethod
    def get_branches(repo_path: str) -> Dict[str, Any]:
        """
        Get list of branches and the current branch
        
        Args:
            repo_path: Path to Git repository
            
        Returns:
            Dictionary with branch information
        """
        try:
            # Get current branch
            result = subprocess.run(
                ['git', 'branch', '--show-current'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return {"error": f"Git error: {result.stderr}"}
                
            current_branch = result.stdout.strip()
            
            # Get all branches
            result = subprocess.run(
                ['git', 'branch', '-a'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return {"error": f"Git error: {result.stderr}"}
                
            all_branches = []
            remote_branches = []
            
            for line in result.stdout.splitlines():
                branch_name = line.strip()
                if branch_name.startswith('*'):
                    # Current branch, already captured
                    continue
                    
                if branch_name.startswith('remotes/'):
                    # Remote branch
                    remote_branch = branch_name.strip().replace('remotes/', '', 1)
                    remote_branches.append(remote_branch)
                else:
                    # Local branch
                    all_branches.append(branch_name.strip())
            
            return {
                "current_branch": current_branch,
                "local_branches": all_branches,
                "remote_branches": remote_branches
            }
            
        except Exception as e:
            return {"error": str(e)}
            
    @staticmethod
    def get_branch_graph(repo_path: str, max_commits: int = 20) -> Dict[str, Any]:
        """
        Get branch graph information for visualization
        
        Args:
            repo_path: Path to Git repository
            max_commits: Maximum number of commits to include
            
        Returns:
            Dictionary with branch graph data
        """
        try:
            # Get graph data using git log with graph format
            result = subprocess.run(
                ['git', 'log', '--graph', '--oneline', '--decorate', '--all', f'-n{max_commits}'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return {"error": f"Git error: {result.stderr}"}
                
            # Get commit data for more details
            commit_result = subprocess.run(
                ['git', 'log', '--pretty=format:%H|%an|%ad|%s', '--date=short', f'-n{max_commits}'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if commit_result.returncode != 0:
                return {"error": f"Git error: {commit_result.stderr}"}
                
            # Parse commit data
            commits = []
            for line in commit_result.stdout.splitlines():
                parts = line.split('|', 3)
                if len(parts) >= 4:
                    commit_hash, author, date, message = parts
                    commits.append({
                        "hash": commit_hash,
                        "short_hash": commit_hash[:7],
                        "author": author,
                        "date": date,
                        "message": message
                    })
            
            # Get branch structure
            branch_data = GitManager.get_branches(repo_path)
            
            return {
                "graph_output": result.stdout,
                "commits": commits,
                "branches": branch_data
            }
            
        except Exception as e:
            return {"error": str(e)}
            
    @staticmethod
    def switch_branch(repo_path: str, branch_name: str) -> Tuple[bool, str]:
        """
        Switch to a different branch
        
        Args:
            repo_path: Path to Git repository
            branch_name: Name of the branch to switch to
            
        Returns:
            Tuple of (success, message)
        """
        try:
            # Check if branch exists
            result = subprocess.run(
                ['git', 'show-ref', '--verify', '--quiet', f'refs/heads/{branch_name}'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            branch_exists = result.returncode == 0
            
            if branch_exists:
                # Checkout existing branch
                result = subprocess.run(
                    ['git', 'checkout', branch_name],
                    capture_output=True,
                    text=True,
                    cwd=repo_path
                )
            else:
                # Check if it's a remote branch
                result = subprocess.run(
                    ['git', 'show-ref', '--verify', '--quiet', f'refs/remotes/origin/{branch_name}'],
                    capture_output=True,
                    text=True,
                    cwd=repo_path
                )
                
                remote_exists = result.returncode == 0
                
                if remote_exists:
                    # Checkout remote branch
                    result = subprocess.run(
                        ['git', 'checkout', '-b', branch_name, f'origin/{branch_name}'],
                        capture_output=True,
                        text=True,
                        cwd=repo_path
                    )
                else:
                    # Create and checkout new branch
                    result = subprocess.run(
                        ['git', 'checkout', '-b', branch_name],
                        capture_output=True,
                        text=True,
                        cwd=repo_path
                    )
            
            if result.returncode != 0:
                return False, f"Failed to switch branch: {result.stderr}"
                
            return True, f"Switched to branch '{branch_name}'"
            
        except Exception as e:
            return False, f"Error switching branch: {str(e)}"
            
    @staticmethod
    def create_branch(repo_path: str, branch_name: str) -> Tuple[bool, str]:
        """
        Create a new branch
        
        Args:
            repo_path: Path to Git repository
            branch_name: Name of the new branch
            
        Returns:
            Tuple of (success, message)
        """
        try:
            # Create and checkout new branch
            result = subprocess.run(
                ['git', 'checkout', '-b', branch_name],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return False, f"Failed to create branch: {result.stderr}"
                
            return True, f"Created and switched to branch '{branch_name}'"
            
        except Exception as e:
            return False, f"Error creating branch: {str(e)}"
    
    @staticmethod
    def get_git_status(repo_path: str) -> Dict[str, Any]:
        """
        Get Git repository status
        
        Args:
            repo_path: Path to Git repository
            
        Returns:
            Git status information
        """
        try:
            result = subprocess.run(
                ['git', 'status', '--porcelain'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return {"error": f"Git error: {result.stderr}"}
                
            modified_files = []
            untracked_files = []
            staged_files = []
            
            for line in result.stdout.splitlines():
                status = line[:2]
                filename = line[3:]
                
                if status.startswith('M'):
                    modified_files.append(filename)
                elif status.startswith('A'):
                    staged_files.append(filename)
                elif status.startswith('??'):
                    untracked_files.append(filename)
                else:
                    # Other statuses (deleted, renamed, etc.)
                    staged_files.append(filename)
                    
            return {
                "clean": len(result.stdout.strip()) == 0,
                "modified_files": modified_files,
                "untracked_files": untracked_files,
                "staged_files": staged_files
            }
            
        except Exception as e:
            return {"error": str(e)}
    
    @staticmethod
    def git_commit(repo_path: str, message: str) -> Tuple[bool, str]:
        """
        Commit changes to Git repository
        
        Args:
            repo_path: Path to Git repository
            message: Commit message
            
        Returns:
            Tuple of (success, message)
        """
        try:
            result = subprocess.run(
                ['git', 'commit', '-m', message],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return False, f"Commit failed: {result.stderr}"
                
            return True, result.stdout
            
        except Exception as e:
            return False, f"Commit error: {str(e)}"
            
    @staticmethod
    def git_push(repo_path: str) -> Tuple[bool, str]:
        """
        Push changes to remote repository
        
        Args:
            repo_path: Path to Git repository
            
        Returns:
            Tuple of (success, message)
        """
        try:
            result = subprocess.run(
                ['git', 'push'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return False, f"Push failed: {result.stderr}"
                
            return True, result.stdout
            
        except Exception as e:
            return False, f"Push error: {str(e)}"
            
    @staticmethod
    def git_pull(repo_path: str) -> Tuple[bool, str]:
        """
        Pull changes from remote repository
        
        Args:
            repo_path: Path to Git repository
            
        Returns:
            Tuple of (success, message)
        """
        try:
            result = subprocess.run(
                ['git', 'pull'],
                capture_output=True,
                text=True,
                cwd=repo_path
            )
            
            if result.returncode != 0:
                return False, f"Pull failed: {result.stderr}"
                
            return True, result.stdout
            
        except Exception as e:
            return False, f"Pull error: {str(e)}"

# Debugger Utilities
class PythonDebugger:
    """Python debugger utilities for code debugging and inspection"""
    
    @staticmethod
    def start_debugging(file_path: str) -> Tuple[bool, Dict[str, Any]]:
        """
        Start a debugging session for a Python file
        
        Args:
            file_path: Path to the Python file to debug
            
        Returns:
            Tuple of (success, session_info)
        """
        try:
            if not file_path.endswith('.py'):
                return False, {"error": "Only Python files can be debugged"}
                
            if not os.path.exists(file_path):
                return False, {"error": f"File not found: {file_path}"}
                
            # Setup debugging session using pdb
            import pdb
            import sys
            import io
            import threading
            import importlib.util
            from types import ModuleType
            
            # Create a debug info object
            debug_session = {
                "file_path": file_path,
                "active": True,
                "current_line": 1,
                "variables": {},
                "output": "",
                "call_stack": [],
                "breakpoints": []
            }
            
            # In a real implementation, we would launch a proper
            # debug session with a subprocess using pdb or similar
            
            # For demonstration, we'll simulate a debug session
            with open(file_path, 'r', encoding='utf-8') as f:
                code_lines = f.readlines()
                
            debug_session["code"] = "".join(code_lines)
            debug_session["line_count"] = len(code_lines)
            
            # Add some sample data for demonstration
            debug_session["variables"] = {
                "x": {"type": "int", "value": "10"},
                "y": {"type": "str", "value": '"hello"'},
                "my_list": {"type": "list", "value": "[1, 2, 3]"}
            }
            
            debug_session["call_stack"] = [
                {"frame": 0, "function": "main", "file": os.path.basename(file_path), "line": 1},
                {"frame": 1, "function": "<module>", "file": os.path.basename(file_path), "line": 1}
            ]
            
            return True, debug_session
            
        except Exception as e:
            return False, {"error": f"Error starting debugger: {str(e)}"}
    
    @staticmethod
    def set_breakpoint(file_path: str, line: int) -> Tuple[bool, str]:
        """
        Set a breakpoint at a specific line
        
        Args:
            file_path: Path to the Python file
            line: Line number for the breakpoint
            
        Returns:
            Tuple of (success, message)
        """
        try:
            if not os.path.exists(file_path):
                return False, f"File not found: {file_path}"
                
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                
            if line < 1 or line > len(lines):
                return False, f"Line number out of range: {line}"
                
            # In a real implementation, we'd actually set the breakpoint
            # using pdb or a proper debugger
            
            return True, f"Breakpoint set at line {line}"
            
        except Exception as e:
            return False, f"Error setting breakpoint: {str(e)}"
    
    @staticmethod
    def debug_step(debug_session: Dict[str, Any], command: str) -> Dict[str, Any]:
        """
        Execute a debug step command
        
        Args:
            debug_session: The current debug session info
            command: Debug command ('step', 'next', 'continue', etc.)
            
        Returns:
            Updated debug session info
        """
        # Simulate stepping through code
        if not debug_session.get("active", False):
            debug_session["error"] = "No active debugging session"
            return debug_session
            
        current_line = debug_session.get("current_line", 1)
        line_count = debug_session.get("line_count", 1)
        
        if command == "step_over" or command == "next":
            # Move to the next line
            debug_session["current_line"] = min(current_line + 1, line_count)
            
        elif command == "step_into" or command == "step":
            # Simulate stepping into a function
            debug_session["current_line"] = min(current_line + 1, line_count)
            
            # Add a frame to the call stack if appropriate
            if current_line + 1 <= line_count and "def " in debug_session.get("code", "").splitlines()[current_line]:
                func_name = debug_session.get("code", "").splitlines()[current_line].strip().split("def ")[1].split("(")[0]
                debug_session["call_stack"].insert(0, {
                    "frame": 0,
                    "function": func_name,
                    "file": os.path.basename(debug_session.get("file_path", "")),
                    "line": current_line + 1
                })
                
                # Increment other frames
                for i in range(1, len(debug_session["call_stack"])):
                    debug_session["call_stack"][i]["frame"] = i
                
        elif command == "step_out" or command == "return":
            # Simulate stepping out of a function
            debug_session["current_line"] = min(current_line + 1, line_count)
            
            # Remove a frame from the call stack if there are multiple frames
            if len(debug_session.get("call_stack", [])) > 1:
                debug_session["call_stack"] = debug_session["call_stack"][1:]
                
                # Update frame numbers
                for i in range(len(debug_session["call_stack"])):
                    debug_session["call_stack"][i]["frame"] = i
                    
        elif command == "continue":
            # Simulate continuing to the next breakpoint (or end)
            debug_session["current_line"] = line_count
            
        # Update variables to simulate program execution
        if debug_session["current_line"] % 3 == 0:
            # Change some variables periodically to simulate program flow
            debug_session["variables"]["x"] = {"type": "int", "value": str(int(debug_session["variables"]["x"]["value"]) + 1)}
            
        # Check if we've reached the end
        if debug_session["current_line"] >= line_count:
            debug_session["active"] = False
            debug_session["output"] += "\nProgram execution completed."
            
        return debug_session
    
    @staticmethod
    def stop_debugging(debug_session: Dict[str, Any]) -> Dict[str, Any]:
        """
        Stop a debugging session
        
        Args:
            debug_session: The current debug session info
            
        Returns:
            Final debug session info with active=False
        """
        debug_session["active"] = False
        debug_session["output"] += "\nDebugging session terminated."
        return debug_session
    
    @staticmethod
    def evaluate_expression(debug_session: Dict[str, Any], expression: str) -> Tuple[bool, Any]:
        """
        Evaluate an expression in the current debug context
        
        Args:
            debug_session: The current debug session info
            expression: The expression to evaluate
            
        Returns:
            Tuple of (success, result/error)
        """
        try:
            # In a real implementation, we'd use the actual debugger's evaluation
            # For demonstration, simulate simple expression evaluation
            
            if expression in debug_session.get("variables", {}):
                var_info = debug_session["variables"][expression]
                return True, f"{var_info['value']} ({var_info['type']})"
            
            if expression == "len(my_list)":
                return True, "3"
                
            return False, f"Cannot evaluate '{expression}' in current context"
            
        except Exception as e:
            return False, f"Error evaluating expression: {str(e)}"

# Python Execution
class PythonRunner:
    """Python code execution utilities"""
    
    @staticmethod
    def run_code(code: str, timeout: int = 10) -> Dict[str, Any]:
        """
        Run Python code safely
        
        Args:
            code: Python code to run
            timeout: Maximum execution time in seconds
            
        Returns:
            Execution results
        """
        try:
            # Create a temporary file
            with tempfile.NamedTemporaryFile(suffix='.py', delete=False) as temp:
                temp_path = temp.name
                temp.write(code.encode('utf-8'))
            
            try:
                # Run the code with timeout
                result = subprocess.run(
                    [sys.executable, temp_path],
                    capture_output=True,
                    text=True,
                    timeout=timeout
                )
                
                return {
                    "success": result.returncode == 0,
                    "stdout": result.stdout,
                    "stderr": result.stderr,
                    "returncode": result.returncode
                }
                
            except subprocess.TimeoutExpired:
                return {
                    "success": False,
                    "error": f"Execution timed out after {timeout} seconds"
                }
                
            finally:
                # Clean up
                os.unlink(temp_path)
                
        except Exception as e:
            return {
                "success": False,
                "error": f"Execution error: {str(e)}"
            }


# Real-time Collaboration Manager
# Using enhanced implementation from terminator/collaboration module
from terminator.collaboration.adapter import CollaborationManager, CollaborationSession

# These classes provide:
# - Operational Transform (OT) for conflict-free editing
# - Document chunking for large files (>1MB)
# - Connection pooling for better performance
# - Reliable message delivery with acknowledgments
# - Performance optimizations for multiple users
```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-01 22:53:20,938 - terminator_agents - INFO - Added 2572 tokens for query, total: 5133
2025-04-01 22:53:20,941 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-01 22:53:20,950 - openai._base_client - INFO - Retrying request to /responses in 0.410962 seconds
2025-04-01 22:53:23,185 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 23:03:21,426 - openai._base_client - INFO - Retrying request to /responses in 0.947279 seconds
2025-04-01 23:13:22,423 - openai.agents - ERROR - Error getting response: Request timed out.. (request_id: None)
2025-04-01 23:13:22,424 - terminator_agents - ERROR - Error in agent query: Request timed out.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection.py", line 103, in handle_async_request
    return await self._connection.handle_async_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 136, in handle_async_request
    raise exc
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 106, in handle_async_request
    ) = await self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 177, in _receive_response_headers
    event = await self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 217, in _receive_event
    data = await self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_backends/anyio.py", line 32, in read
    with map_exceptions(exc_map):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1500, in _request
    response = await self._client.send(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 393, in handle_async_request
    with map_httpcore_exceptions():
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1809, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 215, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 739, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 896, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 75, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 234, in _fetch_response
    return await self._client.responses.create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/resources/responses/responses.py", line 1415, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1461, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1524, in _request
    return await self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in _retry_request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1509, in _request
    return await self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in _retry_request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1519, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2025-04-01 23:13:23,166 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 23:49:52,391 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 23:49:52,405 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 23:49:52,405 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 23:49:52,406 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 23:49:52,409 - root - INFO - AI panel initialized successfully
2025-04-01 23:52:47,161 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2891, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-01 23:57:25,311 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-01 23:57:25,323 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-01 23:57:25,323 - terminator_agents - INFO - Agent system initialized successfully
2025-04-01 23:57:25,324 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-01 23:57:25,326 - root - INFO - AI panel initialized successfully
2025-04-01 23:57:32,968 - terminator_agents - INFO - Processing agent query: test
2025-04-01 23:57:32,978 - terminator_agents - INFO - Added 1 tokens for query, total: 1
2025-04-01 23:57:34,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 23:57:37,483 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 23:57:40,592 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 23:57:47,931 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 23:57:51,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-01 23:58:12,484 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-01 23:58:12,633 - terminator_agents - INFO - Successfully completed agent query
2025-04-01 23:58:13,280 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 12:34:55,019 - terminator_agents - ERROR - OPENAI_API_KEY not set in environment variables
2025-04-03 12:34:55,021 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 12:34:55,021 - root - INFO - AI panel initialized successfully
2025-04-03 12:35:29,091 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-03 12:35:29,099 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-03 12:35:29,099 - terminator_agents - INFO - Agent system initialized successfully
2025-04-03 12:35:29,100 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 12:35:29,102 - root - INFO - AI panel initialized successfully
2025-04-03 12:35:40,022 - terminator_agents - INFO - Processing agent query: This is a test.
2025-04-03 12:35:40,031 - terminator_agents - INFO - Added 4 tokens for query, total: 4
2025-04-03 12:35:43,002 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 12:35:44,123 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 12:35:52,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 12:35:52,456 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 12:35:54,880 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 12:37:16,858 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2904, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-03 12:37:28,484 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2904, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-03 12:37:31,700 - root - ERROR - Error opening file: unhashable type: 'Theme'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2904, in on_directory_tree_file_selected
    editor.theme = self.current_theme
    ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 336, in __set__
    self._set(obj, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 317, in _set
    self._check_watchers(obj, name, current_value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 354, in _check_watchers
    invoke_watcher(obj, private_watch_function, old_value, value)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/reactive.py", line 91, in invoke_watcher
    watch_result = cast(WatchCallbackNewValueType, watch_function)(value)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 784, in _watch_theme
    self._set_theme(theme)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_text_area.py", line 794, in _set_theme
    theme_object = self._themes[theme]
                   ~~~~~~~~~~~~^^^^^^^
TypeError: unhashable type: 'Theme'
2025-04-03 13:00:54,204 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-03 13:00:54,215 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-03 13:00:54,215 - terminator_agents - INFO - Agent system initialized successfully
2025-04-03 13:00:54,216 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 13:00:54,219 - root - INFO - AI panel initialized successfully
2025-04-03 13:01:28,641 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
#!/usr/bin/env python3
"""
Terminator v1 - A terminal-based Python IDE with integrated AI assistant
Combines code editing, file management, Git integration, and Claude AI assistance
"""

import os
import sys
import asyncio
import aiofiles
import logging
import time
import json
import subprocess
from pathlib import Path
from typing import Optional, Dict, Any, List
# Textual imports
from textual.app import App, ComposeResult
from textual.containers import Container, Horizontal, Vertical, ScrollableContainer
from textual.events import MouseDown, MouseUp, MouseMove
from textual.widgets import (
    Header,
    Footer,
    Static,
    Button,
    Input,
    TextArea,
    Tree,
    DirectoryTree,
    Label,
    Markdown,
    LoadingIndicator,
    TabbedContent,
    TabPane,
)
from textual.widgets.tree import TreeNode
from textual.screen import Screen, ModalScreen
from textual.suggester import SuggestFromList
from textual.widgets import DataTable
from textual.binding import Binding
from textual.reactive import reactive
from textual import events, work
from textual.message import Message
from textual.timer import Timer


# For older Textual versions
from textual.theme import Theme
import re

# Import agent and tools modules
from TerminatorV1_agents import initialize_agent_system, run_agent_query, AgentContext
from TerminatorV1_tools import (
    FileSystem,
    CodeAnalyzer,
    GitManager,
    PythonRunner,
    PythonDebugger,
    CollaborationManager,
    CollaborationSession,
)


# Git Commit Dialog Screen
class CommitDialog(ModalScreen):
    """Git commit dialog screen with Escape key support"""

    # Add key bindings for the dialog
    BINDINGS = [
        Binding("escape", "cancel", "Cancel"),
    ]

    def compose(self) -> ComposeResult:
        """Create the dialog layout"""
        with Container(id="commit-dialog"):
            with Horizontal(id="commit-header"):
                yield Label("Commit Message", classes="title")
                yield Label("Press ESC to cancel", classes="escape-hint")
            # Use None instead of "text" for language to avoid tree-sitter error
            yield TextArea(language=None, id="commit-message")
            with Horizontal():
                yield Button("Cancel", id="cancel-commit", variant="error")
                yield Button("Commit", id="confirm-commit", variant="success")

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        if event.button.id == "cancel-commit":
            self.app.pop_screen()
        elif event.button.id == "confirm-commit":
            # Get the commit message
            commit_message = self.query_one("#commit-message").text
            if not commit_message:
                self.notify("Please enter a commit message", severity="error")
                return

            # Trigger the commit and close the dialog
            self.app.git_commit(commit_message)
            self.app.pop_screen()

    async def action_cancel(self) -> None:
        """Cancel the commit dialog (called when ESC is pressed)"""
        self.app.pop_screen()

    async def on_key(self, event) -> None:
        """Handle key presses in the dialog"""
        # If the Escape key was already handled by bindings, we don't need to do anything
        # This is a fallback in case the binding doesn't work
        if event.key == "escape":
            await self.action_cancel()


# Code Analysis Dialog
class CodeAnalysisDialog(ModalScreen):
    """Code analysis results screen"""

    def compose(self) -> ComposeResult:
        """Create the dialog layout"""
        with Container(id="analysis-dialog"):
            yield Label("Code Analysis Results", classes="title")
            yield ScrollableContainer(
                Markdown("Analyzing code..."), id="analysis-result"
            )
            yield Button("Close", id="close-analysis")

    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        if event.button.id == "close-analysis":
            self.app.pop_screen()


# Theme Selection Screen
class ThemeSelectionScreen(ModalScreen):
    """Screen for selecting a TextArea theme"""

    def compose(self) -> ComposeResult:
        """Create the theme selection layout"""
        with Container(id="theme-dialog"):
            yield Label("Select Theme", classes="title")

            # Use a predefined list of known themes
            available_themes = [
                "css",
                "monokai",
                "dracula",
                "github_light",
                "vscode_dark",
            ]

            with ScrollableContainer(id="theme-list"):
                for theme in sorted(available_themes):
                    yield Button(theme, id=f"theme-{theme}", classes="theme-button")

            yield Button("Cancel", id="cancel-theme", variant="error")

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id

        if button_id == "cancel-theme":
            self.app.pop_screen()
        elif button_id.startswith("theme-"):
            theme_name = button_id[6:]  # Remove "theme-" prefix
            # Call the async method
            await self.app.set_editor_theme(theme_name)
            self.app.pop_screen()


class DiffViewScreen(ModalScreen):
    """Modal screen for displaying code diffs"""

    def __init__(
        self,
        original_content: str,
        modified_content: str,
        title: str = "Code Changes",
        original_title: str = "Original",
        modified_title: str = "Modified",
        highlight_language: str = "python",
    ):
        """
        Initialize the diff view screen

        Args:
            original_content: Original content
            modified_content: Modified content
            title: Title of the diff view
            original_title: Title for the original content panel
            modified_title: Title for the modified content panel
            highlight_language: Language for syntax highlighting
        """
        super().__init__()
        self.original_content = original_content
        self.modified_content = modified_content
        self.screen_title = title
        self.original_title = original_title
        self.modified_title = modified_title
        self.language = highlight_language

        # Calculate the diff
        self.unified_diff = CodeAnalyzer.create_diff(original_content, modified_content)

        # Extract line changes from diff
        self.changed_lines = self._extract_line_changes(self.unified_diff)

    def _extract_line_changes(self, diff_text):
        """
        Extract line numbers that were added or removed in the diff

        Args:
            diff_text: The unified diff text

        Returns:
            Dictionary with original and modified line numbers that changed
        """
        original_changes = set()
        modified_changes = set()

        current_original_line = 0
        current_modified_line = 0

        for line in diff_text.splitlines():
            # Check if this is a hunk header line (e.g., @@ -1,7 +1,9 @@)
            if line.startswith("@@"):
                # Extract line numbers from the hunk header
                # Format is @@ -original_start,original_count +modified_start,modified_count @@
                match = re.search(r"@@ -(\d+)(?:,\d+)? \+(\d+)(?:,\d+)? @@", line)
                if match:
                    current_original_line = (
                        int(match.group(1)) - 1
                    )  # Adjust to 0-based indexing
                    current_modified_line = (
                        int(match.group(2)) - 1
                    )  # Adjust to 0-based indexing
            elif line.startswith("-"):
                original_changes.add(current_original_line)
                current_original_line += 1
            elif line.startswith("+"):
                modified_changes.add(current_modified_line)
                current_modified_line += 1
            else:
                # Context line or other (moves both counters)
                current_original_line += 1
                current_modified_line += 1

        return {"original": original_changes, "modified": modified_changes}

    def compose(self) -> ComposeResult:
        """Create the diff view layout"""
        yield Label(self.screen_title, id="diff-title", classes="title")

        with Container(id="diff-view-container"):
            with Horizontal(id="diff-split-view"):
                # Left panel: Original code
                with Vertical(id="diff-original-panel"):
                    yield Label(self.original_title, classes="subtitle")
                    # Use TextArea with line numbers for original content
                    original_editor = yield TextArea(
                        language=self.language,
                        theme="monokai",
                        show_line_numbers=True,
                        read_only=True,
                        id="diff-original-content",
                    )
                    original_editor.text = self.original_content

                # Right panel: Modified code
                with Vertical(id="diff-modified-panel"):
                    yield Label(self.modified_title, classes="subtitle")
                    # Use TextArea with line numbers for modified content
                    modified_editor = yield TextArea(
                        language=self.language,
                        theme="monokai",
                        show_line_numbers=True,
                        read_only=True,
                        id="diff-modified-content",
                    )
                    modified_editor.text = self.modified_content

            # Bottom panel: Unified diff view (optional, can be toggled)
            with Vertical(id="unified-diff-panel", classes="hidden"):
                yield Label("Unified Diff View", classes="subtitle")
                diff_editor = yield TextArea(
                    language="diff",
                    theme="monokai",
                    show_line_numbers=True,
                    read_only=True,
                    id="unified-diff-content",
                )
                diff_editor.text = self.unified_diff

            with Horizontal(id="diff-buttons"):
                yield Button("Apply Changes", id="apply-diff", variant="success")
                yield Button("Toggle Unified View", id="toggle-unified-view")
                yield Button("Close", id="close-diff", variant="error")

    def on_mount(self) -> None:
        """Called when the screen is mounted"""
        # Apply custom CSS classes to highlight changed lines
        self._highlight_changes()

    def _highlight_changes(self) -> None:
        """Highlight the lines that have changed in both panels"""
        # This is a basic implementation - for a production app,
        # you would use proper CSS styling to highlight changes
        try:
            # Create CSS for highlighting original lines that were changed
            original_editor = self.query_one("#diff-original-content")
            for line_num in self.changed_lines["original"]:
                # Apply highlighting to the lines that were changed in the original
                pass  # This would require custom rendering in a full implementation

            # Create CSS for highlighting modified lines that were changed
            modified_editor = self.query_one("#diff-modified-content")
            for line_num in self.changed_lines["modified"]:
                # Apply highlighting to the lines that were changed in the modified
                pass  # This would require custom rendering in a full implementation

        except Exception as e:
            logging.error(f"Error highlighting changes: {str(e)}", exc_info=True)

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id

        if button_id == "close-diff":
            # Close without applying changes
            self.app.pop_screen()
        elif button_id == "apply-diff":
            # Apply the changes and close
            self.app.apply_diff_changes(self.modified_content)
            self.app.pop_screen()
        elif button_id == "toggle-unified-view":
            # Toggle visibility of unified diff panel
            unified_panel = self.query_one("#unified-diff-panel")
            if "hidden" in unified_panel.classes:
                unified_panel.remove_class("hidden")
            else:
                unified_panel.add_class("hidden")


# Debugger Screen
class DebuggerScreen(Screen):
    """Debugger interface screen"""

    def compose(self) -> ComposeResult:
        """Create the debugger layout"""
        yield Header()

        with Horizontal():
            # Left panel: Code with breakpoints
            with Vertical(id="debug-code-panel"):
                yield Label("Source Code", classes="title")
                yield TextArea(language="python", id="debug-code", read_only=True)
                with Horizontal():
                    yield Button("Step Over", id="debug-step-over-btn")
                    yield Button("Step Into", id="debug-step-into-btn")
                    yield Button("Step Out", id="debug-step-out-btn")
                    yield Button("Continue", id="debug-continue-btn")
                    yield Button("Stop", id="debug-stop-btn", variant="error")

            # Right panel: Variable inspector and output
            with Vertical(id="debug-info-panel"):
                yield Label("Variables", classes="title")
                yield DataTable(id="debug-variables")

                yield Label("Call Stack", classes="title")
                yield DataTable(id="debug-stack")

                yield Label("Output", classes="title")
                yield TextArea(id="debug-output", read_only=True)

    def on_mount(self):
        """Initialize the debugger UI"""
        # Set up variables table
        variables_table = self.query_one("#debug-variables")
        variables_table.add_columns("Name", "Type", "Value")

        # Set up call stack table
        stack_table = self.query_one("#debug-stack")
        stack_table.add_columns("Frame", "Function", "File", "Line")

    def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle debug control buttons"""
        button_id = event.button.id

        if button_id == "debug-step-over-btn":
            self.app.debug_step_over()
        elif button_id == "debug-step-into-btn":
            self.app.debug_step_into()
        elif button_id == "debug-step-out-btn":
            self.app.debug_step_out()
        elif button_id == "debug-continue-btn":
            self.app.debug_continue()
        elif button_id == "debug-stop-btn":
            self.app.debug_stop()


# Git Branch Visualization Screen
class BranchVisualizationScreen(Screen):
    """Screen for visualizing Git branches and history"""

    def compose(self) -> ComposeResult:
        """Create the branch visualization layout"""
        yield Header()

        with Horizontal():
            # Left panel: Branch tree
            with Vertical(id="branch-tree-panel"):
                yield Label("Branch Structure", classes="title")
                yield TextArea(language="git", id="branch-graph", read_only=True)
                with Horizontal():
                    yield Button("Refresh", id="refresh-branches-btn")
                    yield Button("Back", id="back-to-main-btn", variant="primary")

            # Right panel: Branch and commit info
            with Vertical(id="branch-info-panel"):
                yield Label("Current Branch", classes="title")
                yield Static("", id="current-branch-info")

                yield Label("All Branches", classes="title")
                yield ScrollableContainer(id="all-branches-container")

                yield Label("Recent Commits", classes="title")
                yield ScrollableContainer(id="recent-commits-container")

                with Horizontal():
                    yield Input(placeholder="New branch name...", id="new-branch-input")
                    yield Button("Create Branch", id="create-branch-btn")

    def on_mount(self):
        """Set up the branch visualization screen"""
        # Load branches and commit data
        self.load_branch_data()

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id

        if button_id == "refresh-branches-btn":
            self.load_branch_data()
        elif button_id == "back-to-main-btn":
            self.app.pop_screen()
        elif button_id == "create-branch-btn":
            self.create_new_branch()

    def load_branch_data(self):
        """Load and display branch and commit data"""
        if not self.app.git_repository:
            self.app.notify("Not a Git repository", severity="error")
            return

        # Get branch graph data
        branch_data = GitManager.get_branch_graph(self.app.git_repository)

        if "error" in branch_data:
            self.app.notify(
                f"Failed to load branch data: {branch_data['error']}", severity="error"
            )
            return

        # Update branch graph visualization
        branch_graph = self.query_one("#branch-graph")
        branch_graph.text = branch_data.get("graph_output", "")

        # Update current branch info
        current_branch_info = self.query_one("#current-branch-info")
        current_branch = branch_data.get("branches", {}).get(
            "current_branch", "unknown"
        )
        current_branch_info.update(f"Current branch: [bold]{current_branch}[/bold]")

        # Update all branches list
        all_branches_container = self.query_one("#all-branches-container")
        all_branches_container.remove_children()

        local_branches = branch_data.get("branches", {}).get("local_branches", [])
        remote_branches = branch_data.get("branches", {}).get("remote_branches", [])

        if local_branches:
            all_branches_container.mount(Label("Local Branches:"))
            for branch in local_branches:
                # Create a button for each branch to switch to it
                all_branches_container.mount(
                    Button(
                        branch, id=f"switch-branch-{branch}", classes="branch-button"
                    )
                )

        if remote_branches:
            all_branches_container.mount(Label("Remote Branches:"))
            for branch in remote_branches:
                # Create a button for each remote branch to check it out
                all_branches_container.mount(
                    Button(
                        branch,
                        id=f"checkout-remote-{branch}",
                        classes="remote-branch-button",
                    )
                )

        # Update recent commits list
        commits_container = self.query_one("#recent-commits-container")
        commits_container.remove_children()

        commits = branch_data.get("commits", [])
        for commit in commits:
            commit_message = commit.get("message", "")
            short_hash = commit.get("short_hash", "")
            author = commit.get("author", "")
            date = commit.get("date", "")

            # Create a button for the commit
            commit_label = f"{short_hash} ({date}) - {commit_message}"
            commits_container.mount(
                Button(
                    commit_label,
                    id=f"view-commit-{short_hash}",
                    classes="commit-button",
                )
            )

    def create_new_branch(self):
        """Create a new branch"""
        if not self.app.git_repository:
            self.app.notify("Not a Git repository", severity="error")
            return

        # Get the branch name
        branch_input = self.query_one("#new-branch-input")
        branch_name = branch_input.value.strip()

        if not branch_name:
            self.app.notify("Please enter a branch name", severity="warning")
            return

        # Create the branch
        success, message = GitManager.create_branch(
            self.app.git_repository, branch_name
        )

        if success:
            self.app.notify(message, severity="success")
            # Refresh the branch data
            self.load_branch_data()
            # Clear the input
            branch_input.value = ""
        else:
            self.app.notify(message, severity="error")

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id

        if button_id == "refresh-branches-btn":
            self.load_branch_data()
        elif button_id == "back-to-main-btn":
            self.app.pop_screen()
        elif button_id == "create-branch-btn":
            self.create_new_branch()
        elif button_id.startswith("switch-branch-"):
            # Switch to local branch
            branch_name = button_id[14:]  # Remove "switch-branch-" prefix
            self.switch_branch(branch_name)
        elif button_id.startswith("checkout-remote-"):
            # Checkout remote branch
            branch_name = button_id[16:]  # Remove "checkout-remote-" prefix
            self.switch_branch(branch_name)

    def switch_branch(self, branch_name: str):
        """Switch to a different branch"""
        if not self.app.git_repository:
            self.app.notify("Not a Git repository", severity="error")
            return

        # Switch branch
        success, message = GitManager.switch_branch(
            self.app.git_repository, branch_name
        )

        if success:
            self.app.notify(message, severity="success")
            # Refresh the branch data
            self.load_branch_data()
            # Update Git status in main app
            self.app.update_git_status()
        else:
            self.app.notify(message, severity="error")


# Remote Connection Dialog
class RemoteConnectionDialog(ModalScreen):
    """Dialog for setting up a remote development connection"""

    def compose(self) -> ComposeResult:
        """Create the dialog layout"""
        with Container(id="remote-dialog"):
            yield Label("Remote Connection", classes="title")

            yield Label("Connection Type:")
            with Horizontal():
                yield Button("SSH", id="connection-ssh", variant="primary")
                yield Button("SFTP", id="connection-sftp")

            yield Label("Server Details:")
            yield Input(placeholder="hostname or IP", id="remote-host")
            yield Input(placeholder="username", id="remote-username")
            yield Input(placeholder="port (default: 22)", id="remote-port", value="22")
            yield Input(
                placeholder="password (leave empty for key auth)",
                id="remote-password",
                password=True,
            )

            yield Label("Remote Directory:")
            yield Input(placeholder="/path/to/project", id="remote-path")

            with Horizontal():
                yield Button("Cancel", id="cancel-remote", variant="error")
                yield Button("Connect", id="confirm-remote", variant="success")

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id

        if button_id == "cancel-remote":
            self.app.pop_screen()
        elif button_id == "confirm-remote":
            self.setup_remote_connection()
        elif button_id == "connection-ssh":
            # Highlight SSH button
            self.query_one("#connection-ssh").variant = "primary"
            self.query_one("#connection-sftp").variant = "default"
        elif button_id == "connection-sftp":
            # Highlight SFTP button
            self.query_one("#connection-sftp").variant = "primary"
            self.query_one("#connection-ssh").variant = "default"

    def setup_remote_connection(self):
        """Set up the remote connection"""
        # Get connection details
        host = self.query_one("#remote-host").value
        username = self.query_one("#remote-username").value
        port = self.query_one("#remote-port").value
        password = self.query_one("#remote-password").value
        remote_path = self.query_one("#remote-path").value

        # Validate inputs
        if not host or not username or not remote_path:
            self.app.notify("Please fill in all required fields", severity="error")
            return

        # Determine connection type
        connection_type = "ssh"
        if self.query_one("#connection-sftp").variant == "primary":
            connection_type = "sftp"

        # Configure the remote connection
        self.app.configure_remote(
            connection_type=connection_type,
            host=host,
            username=username,
            port=int(port) if port.isdigit() else 22,
            password=password,
            remote_path=remote_path,
        )

        # Close the dialog
        self.app.pop_screen()


# Remote Files Browser Screen
class RemoteFilesBrowser(Screen):
    """Browser for remote files"""

    def compose(self) -> ComposeResult:
        """Create the remote files browser layout"""
        yield Header()

        with Horizontal():
            # Left panel: Remote files tree
            with Vertical(id="remote-files-panel"):
                with Horizontal():
                    yield Label("Remote Files", classes="title")
                    yield Button("Refresh", id="refresh-remote-btn")
                yield Tree("Remote Files", id="remote-files-tree")
                with Horizontal():
                    yield Button("Download", id="download-remote-btn")
                    yield Button("Upload", id="upload-remote-btn")
                    yield Button(
                        "Disconnect", id="disconnect-remote-btn", variant="error"
                    )
                    yield Button("Back", id="back-from-remote-btn")

            # Right panel: File preview
            with Vertical(id="remote-preview-panel"):
                yield Label("File Preview", classes="title")
                yield TextArea(id="remote-file-preview", read_only=True)

    def on_mount(self):
        """Initialize the remote files browser"""
        # Populate the remote files tree
        self.populate_remote_files()

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id

        if button_id == "refresh-remote-btn":
            self.populate_remote_files()
        elif button_id == "download-remote-btn":
            self.download_selected_file()
        elif button_id == "upload-remote-btn":
            self.app.action_upload_to_remote()
        elif button_id == "disconnect-remote-btn":
            self.app.disconnect_remote()
            self.app.pop_screen()
        elif button_id == "back-from-remote-btn":
            self.app.pop_screen()

    def populate_remote_files(self):
        """Populate the remote files tree"""
        if not self.app.remote_connected:
            self.app.notify("Not connected to remote server", severity="error")
            return

        # In a real implementation, fetch the remote files
        # For now, we'll use a placeholder
        tree = self.query_one("#remote-files-tree")
        tree.clear()

        # Add some dummy remote files
        remote_files = [
            "project/",
            "project/main.py",
            "project/utils.py",
            "project/data/",
            "project/data/config.json",
        ]

        # Create tree nodes
        root = tree.root
        root.expand()

        directories = {}

        for path in sorted(remote_files):
            parts = path.strip("/").split("/")

            if len(parts) == 1:
                # Top-level item
                if path.endswith("/"):
                    # Directory
                    directories[path] = root.add(path, expand=True)
                else:
                    # File
                    root.add_leaf(path)
            else:
                # Nested item
                parent_path = "/".join(parts[:-1]) + "/"
                if parent_path in directories:
                    parent = directories[parent_path]

                    if path.endswith("/"):
                        # Subdirectory
                        directories[path] = parent.add(parts[-1], expand=True)
                    else:
                        # File in directory
                        parent.add_leaf(parts[-1])

        self.app.notify("Remote files refreshed", severity="information")

    def download_selected_file(self):
        """Download the selected remote file"""
        # In a real implementation, download the selected file
        tree = self.query_one("#remote-files-tree")
        node = tree.cursor_node

        if node is None or not node.is_leaf:
            self.app.notify("Please select a file to download", severity="warning")
            return

        # Get the full path of the selected file
        file_path = self.get_node_path(node)

        self.app.notify(
            f"Downloading {file_path}... (simulation)", severity="information"
        )

        # Simulate download
        preview = self.query_one("#remote-file-preview")
        preview.text = f"# Content of {file_path}\n\nThis is a simulated file content."

    def get_node_path(self, node):
        """Get the full path of a tree node"""
        path_parts = []

        # Traverse up to build the path
        current = node
        while (
            current is not None and current != self.query_one("#remote-files-tree").root
        ):
            path_parts.append(current.label)
            current = current.parent

        path_parts.reverse()
        return "/".join(path_parts)

    def on_tree_node_selected(self, event: Tree.NodeSelected):
        """Handle node selection in the tree"""
        node = event.node

        if node.is_leaf:
            # Preview file
            file_path = self.get_node_path(node)

            # In a real implementation, get the file content
            preview = self.query_one("#remote-file-preview")
            preview.text = f"# Content of {file_path}\n\nThis is a simulated file content for a remote file."


# Semantic Search Screen
class SemanticSearchScreen(ModalScreen):
    """Screen for performing semantic code search using natural language"""

    def compose(self) -> ComposeResult:
        """Create the semantic search layout"""
        with Container(id="semantic-search-dialog"):
            yield Label("Semantic Code Search", classes="title")
            yield Input(
                placeholder="Describe what you're looking for...",
                id="semantic-search-input",
            )

            yield Label("Search Results:", id="search-results-label")
            yield ScrollableContainer(id="semantic-results-container")

            with Horizontal():
                yield Button("Search", id="semantic-search-btn", variant="primary")
                yield Button("Cancel", id="cancel-semantic-search", variant="error")

    def on_mount(self):
        """Set up the search screen"""
        self.query_one("#semantic-search-input").focus()

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id

        if button_id == "semantic-search-btn":
            asyncio.create_task(self.perform_semantic_search())
        elif button_id == "cancel-semantic-search":
            asyncio.create_task(self.app.pop_screen())

    async def on_input_submitted(self, event: Input.Submitted) -> None:
        """Handle input submission"""
        if event.input.id == "semantic-search-input":
            asyncio.create_task(self.perform_semantic_search())

    @work
    async def perform_semantic_search(self) -> None:
        """Perform semantic code search using the AI"""
        search_query = self.query_one("#semantic-search-input").value.strip()

        if not search_query:
            self.app.notify("Please enter a search query", severity="warning")
            return

        # Show a loading message
        results_container = self.query_one("#semantic-results-container")
        results_container.remove_children()
        results_container.mount(Static("Searching...", classes="search-loading"))

        # Craft a prompt for the AI to convert the natural language query into semantic search results
        prompt = f"""Perform a semantic code search for the following query:
        
        "{search_query}"
        
        You need to:
        1. Interpret what files or code snippets would best match this natural language query
        2. Analyze the code in the project to find the most relevant matches
        3. Return a list of files and relevant code snippets that match the search intent
        4. Include a brief explanation of why each result matches the query
        
        FORMAT YOUR RESPONSE AS JSON with the following structure:
        {{
            "results": [
                {{
                    "file": "file_path",
                    "snippet": "code snippet",
                    "explanation": "why this matches"
                }}
            ]
        }}
        """

        # Call the AI to perform the search
        try:
            # Get the response from the AI
            result = await run_agent_query(prompt, self.app.agent_context)
            response = result.get("response", "")

            # Extract the JSON from the response
            json_pattern = r"\{[\s\S]*\}"
            matches = re.search(json_pattern, response)

            if matches:
                json_str = matches.group(0)
                search_results = json.loads(json_str)

                # Display the results
                results_container.remove_children()

                if not search_results.get("results"):
                    results_container.mount(
                        Static("No results found.", classes="search-no-results")
                    )
                    return

                # Display each result
                for idx, result in enumerate(search_results.get("results", [])):
                    file_path = result.get("file", "Unknown file")
                    snippet = result.get("snippet", "")
                    explanation = result.get("explanation", "")

                    # Create a button for the file
                    result_container = Container(classes="search-result")
                    result_container.mount(
                        Label(
                            f"Result {idx+1}: {file_path}",
                            classes="search-result-title",
                        )
                    )
                    result_container.mount(
                        Static(explanation, classes="search-result-explanation")
                    )
                    result_container.mount(
                        TextArea(
                            snippet,
                            language="python",
                            classes="search-result-snippet",
                            read_only=True,
                        )
                    )
                    result_container.mount(
                        Button(
                            f"Open {os.path.basename(file_path)}",
                            id=f"open-result-{idx}",
                            classes="search-result-open-btn",
                        )
                    )

                    results_container.mount(result_container)
            else:
                # No JSON found in the response
                results_container.remove_children()
                results_container.mount(
                    Static(
                        "Failed to parse search results. Please try again.",
                        classes="search-error",
                    )
                )

        except Exception as e:
            # Handle search errors
            results_container.remove_children()
            results_container.mount(
                Static(f"Search error: {str(e)}", classes="search-error")
            )


# Collaboration Session Dialog Screen
class CollaborationSessionDialog(ModalScreen):
    """Dialog for starting or joining a collaboration session"""

    def compose(self) -> ComposeResult:
        """Create the collaboration dialog layout"""
        with Container(id="collab-dialog"):
            yield Label("Real-time Collaboration", classes="title")

            with Vertical():
                with Horizontal():
                    yield Label("Username:", classes="label")
                    yield Input(
                        placeholder="Your display name", id="collab-username-input"
                    )

                with Vertical():
                    yield Button(
                        "Create New Session", id="collab-create-btn", variant="primary"
                    )

                    with Horizontal():
                        yield Label("Join Session:", classes="label")
                        yield Input(
                            placeholder="Session ID", id="collab-session-id-input"
                        )
                        yield Button("Join", id="collab-join-btn")

                with Horizontal():
                    yield Button("Cancel", id="collab-cancel-btn", variant="error")

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id
        username_input = self.query_one("#collab-username-input", Input)
        username = username_input.value

        if not username:
            self.notify("Please enter a username", severity="warning")
            return

        if button_id == "collab-cancel-btn":
            self.dismiss()
        elif button_id == "collab-create-btn":
            # Create a new session
            self.dismiss({"action": "create", "username": username})
        elif button_id == "collab-join-btn":
            # Join an existing session
            session_id_input = self.query_one("#collab-session-id-input", Input)
            session_id = session_id_input.value

            if not session_id:
                self.notify("Please enter a session ID", severity="warning")
                return

            self.dismiss(
                {"action": "join", "username": username, "session_id": session_id}
            )


# Collaboration Screen
class CollaborationScreen(Screen):
    """Real-time collaboration interface screen"""

    def __init__(self, session_details: Dict[str, Any]):
        """
        Initialize the collaboration screen

        Args:
            session_details: Session details including username and session ID
        """
        super().__init__()
        self.session_details = session_details
        self.session_id = session_details.get("session_id", "")
        self.username = session_details.get("username", "")
        self.active_file = ""
        self.collaboration_status = "connecting"
        self.user_list = []

    def compose(self) -> ComposeResult:
        """Create the collaboration layout"""
        yield Header()

        with Horizontal():
            # Left panel: Users and chat
            with Vertical(id="collab-left-panel"):
                yield Label("Collaboration Session", classes="title")
                with Horizontal():
                    yield Label(
                        f"Session ID: {self.session_id}", id="collab-session-id"
                    )
                    yield Button("Copy", id="collab-copy-id-btn")

                yield Label("Connected Users", classes="subtitle")
                yield ScrollableContainer(id="collab-users-container")

                yield Label("Chat", classes="subtitle")
                with Vertical(id="collab-chat-container"):
                    yield ScrollableContainer(id="collab-chat-messages")
                    with Horizontal():
                        yield Input(
                            placeholder="Type a message...", id="collab-chat-input"
                        )
                        yield Button("Send", id="collab-chat-send-btn")

            # Right panel: Shared editor
            with Vertical(id="collab-editor-panel"):
                with Horizontal():
                    yield Label("Shared Editor", classes="title")
                    yield Label("Status: ", classes="label")
                    yield Label("Connecting...", id="collab-status")

                with Horizontal(id="collab-file-select"):
                    yield Label("File:", classes="label")
                    with Container(id="collab-file-dropdown-container"):
                        yield Input(
                            placeholder="Select or open a file", id="collab-file-input"
                        )
                        yield Button("Open", id="collab-open-file-btn")

                yield TextArea(language="python", id="collab-editor")

                with Horizontal():
                    yield Button("Save", id="collab-save-btn")
                    yield Button("End Session", id="collab-end-btn", variant="error")

    def on_mount(self) -> None:
        """Set up the collaboration screen"""
        # Set up the users container
        users_container = self.query_one("#collab-users-container", ScrollableContainer)

        # Add self as first user
        users_container.mount(
            Static(f" {self.username} (You)", classes="collab-user-item self")
        )

        # Initialize the chat container
        chat_container = self.query_one("#collab-chat-messages", ScrollableContainer)
        chat_container.mount(
            Static(
                " Welcome to the collaboration session! You can chat with other users here.",
                classes="collab-chat-system",
            )
        )

        # Set initial status
        self.update_status("Connecting to session...")

        # Connect to collaboration server
        self.connect_to_session()

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button presses"""
        button_id = event.button.id

        if button_id == "collab-copy-id-btn":
            # Copy session ID to clipboard
            # (Textual doesn't have clipboard API, so just notify)
            self.notify(f"Session ID copied: {self.session_id}")
        elif button_id == "collab-chat-send-btn":
            # Send chat message
            self.send_chat_message()
        elif button_id == "collab-open-file-btn":
            # Open file for collaboration
            self.open_file_for_collaboration()
        elif button_id == "collab-save-btn":
            # Save changes
            self.save_collaborative_file()
        elif button_id == "collab-end-btn":
            # End collaboration session
            self.end_collaboration_session()

    def on_input_submitted(self, event: Input.Submitted) -> None:
        """Handle input submitted events"""
        input_id = event.input.id

        if input_id == "collab-chat-input":
            # Send chat message
            self.send_chat_message()

    @work
    async def connect_to_session(self) -> None:
        """Connect to the collaboration session"""
        try:
            # In a real app, this would connect to the WebSocket server
            await asyncio.sleep(1)  # Simulate connection

            self.update_status("Connected")

            # Add some fake users for demo
            self.add_user("Alice", "user1")
            self.add_user("Bob", "user2")

            # Add welcome system message
            chat_container = self.query_one(
                "#collab-chat-messages", ScrollableContainer
            )
            chat_container.mount(
                Static(
                    " Alice and Bob have joined the session.",
                    classes="collab-chat-system",
                )
            )

        except Exception as e:
            self.update_status(f"Connection error: {str(e)}")

    def update_status(self, status: str) -> None:
        """
        Update the collaboration status

        Args:
            status: New status message
        """
        status_label = self.query_one("#collab-status", Label)
        status_label.update(status)
        self.collaboration_status = status

    def add_user(self, username: str, client_id: str) -> None:
        """
        Add a user to the collaboration session

        Args:
            username: User's display name
            client_id: Client ID
        """
        users_container = self.query_one("#collab-users-container", ScrollableContainer)
        users_container.mount(
            Static(f" {username}", classes=f"collab-user-item {client_id}")
        )
        self.user_list.append({"username": username, "client_id": client_id})

    def remove_user(self, client_id: str) -> None:
        """
        Remove a user from the collaboration session

        Args:
            client_id: Client ID
        """
        users_container = self.query_one("#collab-users-container", ScrollableContainer)

        # Find and remove the user element
        for user_element in users_container.query(f".{client_id}"):
            user_element.remove()

        # Remove from user list
        self.user_list = [
            user for user in self.user_list if user["client_id"] != client_id
        ]

    def send_chat_message(self) -> None:
        """Send a chat message"""
        chat_input = self.query_one("#collab-chat-input", Input)
        message = chat_input.value

        if not message:
            # Add notification for empty message
            self.notify("Please enter a message", severity="warning")
            return

        # Add message to chat container
        chat_container = self.query_one("#collab-chat-messages", ScrollableContainer)
        chat_container.mount(Static(f" You: {message}", classes="collab-chat-self"))

        # Clear input
        chat_input.value = ""

        # In a real app, this would send the message to the WebSocket server
        # For demo, add fake responses
        self.add_fake_chat_response(message)

    @work
    async def add_fake_chat_response(self, message: str) -> None:
        """
        Add a fake chat response for demo purposes

        Args:
            message: Original message
        """
        await asyncio.sleep(1)

        # Fake response
        chat_container = self.query_one("#collab-chat-messages", ScrollableContainer)

        if "hello" in message.lower():
            chat_container.mount(
                Static(
                    " Alice: Hi there! How's the coding going?",
                    classes="collab-chat-other",
                )
            )
        elif "help" in message.lower():
            chat_container.mount(
                Static(
                    " Bob: I can help with that! What do you need?",
                    classes="collab-chat-other",
                )
            )
        else:
            chat_container.mount(
                Static(
                    " Alice: Interesting! Let's work on this together.",
                    classes="collab-chat-other",
                )
            )

    def open_file_for_collaboration(self) -> None:
        """Open a file for collaboration"""
        file_input = self.query_one("#collab-file-input", Input)
        file_path = file_input.value

        if not file_path:
            self.notify("Please enter a file path", severity="warning")
            return

        # Update active file
        self.active_file = file_path

        # Update editor with file contents (simulated)
        editor = self.query_one("#collab-editor", TextArea)

        # Simulate loading file
        editor.load_text(
            f"# Collaborative editing of {file_path}\n\ndef main():\n    print('Hello, Collaborators!')\n\nif __name__ == '__main__':\n    main()"
        )

        # In a real app, this would load the file and sync it to all users
        self.notify(f"Opened {file_path} for collaboration")

    def save_collaborative_file(self) -> None:
        """Save the collaborative file"""
        if not self.active_file:
            self.notify("No file is currently active", severity="warning")
            return

        editor = self.query_one("#collab-editor", TextArea)
        content = editor.text

        # In a real app, this would save the file
        self.notify(f"Saved {self.active_file} successfully")

    def end_collaboration_session(self) -> None:
        """End the collaboration session"""
        # In a real app, this would notify all users and close the WebSocket connection
        self.notify("Ending collaboration session...")
        self.app.pop_screen()


# Collaboration User Presence Indicator Widget
class UserPresenceIndicator(Static):
    """Widget to show user cursor position in collaborative editing"""

    def __init__(self, username: str, client_id: str, color: str = "#3498db"):
        """
        Initialize the user presence indicator

        Args:
            username: User's display name
            client_id: Client ID
            color: User's color
        """
        super().__init__()
        self.username = username
        self.client_id = client_id
        self.color = color
        self.position = (0, 0)  # (line, column)

    def on_mount(self) -> None:
        """Set up the user presence indicator"""
        self.update_style()

    def update_position(self, position: tuple[int, int]) -> None:
        """
        Update the cursor position

        Args:
            position: New position (line, column)
        """
        self.position = position
        self.update_style()

    def update_style(self) -> None:
        """Update the indicator's style based on position"""
        # In a real app, this would calculate the actual UI position from editor coordinates
        self.styles.background = self.color
        self.styles.color = "#ffffff"
        self.update(f" {self.username}")

    def render(self) -> str:
        """Render the indicator"""
        return f" {self.username}"


# Command Palette Screen
class CommandPalette(ModalScreen):
    """Command palette screen for quick access to commands"""

    def __init__(self):
        super().__init__()
        self.commands = {
            "Save": "save",
            "Open": "open",
            "Run": "run",
            "Format Code": "format_code",
            "Git Commit": "git_commit",
            "Git Pull": "git_pull",
            "Git Push": "git_push",
            "Toggle Split View": "toggle_split_view",
            "Switch Editor": "switch_editor",
            "Toggle Terminal": "toggle_terminal",
            "Analyze Code": "analyze_code",
            "AI Request": "ai_request",
            "Quit": "quit",
        }

    def compose(self) -> ComposeResult:
        """Create the command palette layout"""
        with Container(id="command-palette"):
            yield Label("Command Palette", classes="title")
            yield Input(
                placeholder="Search commands...",
                id="command-search",
                suggester=SuggestFromList(list(self.commands.keys())),
            )
            yield ScrollableContainer(id="command-list")

    def on_mount(self):
        """Called when screen is mounted"""
        # Focus the search box
        self.query_one("#command-search").focus()

        # Display all commands initially
        self.display_commands(list(self.commands.keys()))

    def display_commands(self, commands: List[str]):
        """Display a filtered list of commands"""
        command_list = self.query_one("#command-list")
        command_list.remove_children()

        for command in commands:
            command_list.mount(Button(command, id=f"cmd-{self.commands[command]}"))

    def on_input_changed(self, event: Input.Changed) -> None:
        """Handle input changes to filter commands"""
        search_text = event.value.lower()

        # Filter commands
        if search_text:
            filtered_commands = [
                cmd for cmd in self.commands.keys() if search_text in cmd.lower()
            ]
        else:
            filtered_commands = list(self.commands.keys())

        # Update the display
        self.display_commands(filtered_commands)

    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle command selection"""
        button_id = event.button.id

        if button_id.startswith("cmd-"):
            command = button_id[4:]  # Remove "cmd-" prefix
            self.app.action(command)
            self.app.pop_screen()


# Main application class
class TerminatorApp(App):
    """
    Terminator - A terminal-based Python IDE with AI superpowers
    """

    # Event classes
    class CodeAnalysisComplete(Message):
        """Event fired when code analysis is complete"""

        def __init__(self, analysis_result: str) -> None:
            self.analysis_result = analysis_result
            super().__init__()

    # CSS section for the app

    CSS = """
        /* Panel layout */
        #main-layout {
            height: 100%;
        }
        
        #sidebar {
            width: 20%;
            min-width: 20;
            max-width: 40%;
            background: $surface-darken-1;
        }
        
        #editor-container {
            width: 60%;
            min-width: 30%;
            max-width: 80%;
        }
        
        #ai-panel {
            width: 20%;
            min-width: 15%;
            max-width: 40%;
        }
        
        /* Gutter styles for resizing */
        .gutter {
            width: 1;
            background: $accent-darken-2;
            color: $text-muted;
            text-align: center;
            transition: background 0.1s;
        }
        
        .gutter:hover {
            background: $accent;
            color: $text;
        }
        
        /* Real-time Collaboration Styles */
        #collab-dialog {
            background: $surface;
            padding: 1;
            border: solid $accent;
            min-width: 50%;
            max-width: 80%;
            margin: 0 1;
        }
        
        #collab-left-panel {
            width: 30%;
            min-width: 20;
            background: $surface-darken-1;
            padding: 1;
        }
        
        #collab-editor-panel {
            width: 70%;
            padding: 1;
        }
        
        #collab-users-container {
            height: 20%;
            border: solid $panel-darken-1;
            padding: 1;
            margin-bottom: 1;
        }
        
        #collab-chat-messages {
            height: 60%;
            border: solid $panel-darken-1;
            padding: 1;
            margin-bottom: 1;
        }
        
        #collab-editor {
            height: 80%;
            border: solid $panel-darken-1;
        }
        
        .collab-user-item {
            padding: 1 2;
            margin: 1 0;
            border: solid $accent;
        }
        
        .collab-user-item.self {
            background: $accent-darken-2;
            color: $text;
        }
        
        .collab-chat-system {
            color: $text-muted;
            padding: 1;
        }
        
        .collab-chat-self {
            background: $accent-darken-2;
            padding: 1 2;
            margin: 1 0;
            border: solid $accent;
            text-align: right;
        }
        
        .collab-chat-other {
            background: $panel-darken-2;
            padding: 1 2;
            margin: 1 0;
            border: solid $panel-darken-1;
        }
        
        #ai-panel {
            width: 30%;
            min-width: 30;
            border-left: solid $primary;
        }
        
        #file-explorer {
            height: 60%;
            border-bottom: solid $primary;
        }
        
        #git-status {
            height: 40%;
        }
        
        #editor-split-view {
            height: 100%;
        }
        
        #editor-primary {
            height: 100%;
            width: 100%;
        }
        
        #editor-secondary {
            height: 100%;
            width: 50%;
            border-left: solid $primary;
        }
        
        #editor-secondary.hidden {
            display: none;
        }
        
        .split-view #editor-primary {
            width: 50%;
        }
        
        .multi-cursor .cursor {
            background: $accent;
        }
        
        #editor-tabs {
            height: 3;
        }
        
        #editor-content, #terminal-content {
            height: 100%;
        }
        
        #terminal-content.hidden {
            display: none;
        }
        
        #terminal-output {
            height: 85%;
            background: $surface-darken-2;
            color: $text;
        }
        
        #terminal-input {
            width: 80%;
        }
        
        #terminal-execute-btn {
            width: 20%;
            background: $success;
        }
        
        #ai-output {
            height: 75%;
            border-bottom: solid $primary;
            overflow-y: scroll;
        }
        
        #ai-input {
            height: 25%;
        }
        
        .title {
            background: $primary;
            color: $text;
            padding: 1 2;
            text-align: center;
            text-style: bold;
        }
        
        Button {
            margin: 1 1;
        }
        
        #run-btn {
            background: $success;
        }
        
        #ai-submit {
            background: $accent;
        }
        
        /* Modal dialog styling */
        #commit-dialog, #analysis-dialog, #command-palette {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 60%;
            width: 60%;
            margin: 2 2;
        }
        
        #command-palette {
            height: 50%;
            width: 50%;
        }
        
        #command-search {
            margin-bottom: 1;
            border: solid $primary;
        }
        
        #command-list {
            height: 100%;
            overflow-y: auto;
        }
        
        #command-list Button {
            width: 100%;
            margin: 0 0 1 0;
            text-align: left;
        }
        
        /* Debugger styling */
        #debug-code-panel {
            width: 60%;
            height: 100%;
        }
        
        #debug-info-panel {
            width: 40%;
            height: 100%;
            border-left: solid $primary;
        }
        
        #debug-code {
            height: 80%;
        }
        
        #debug-variables, #debug-stack {
            height: 30%;
            margin-bottom: 1;
        }
        
        #debug-output {
            height: 35%;
        }
        
        .current-debug-line {
            background: $accent-lighten-2;
        }
        
        .breakpoint-line {
            background: $error-lighten-2;
        }
        
        /* Branch visualization styling */
        #branch-tree-panel {
            width: 60%;
            height: 100%;
        }
        
        #branch-info-panel {
            width: 40%;
            height: 100%;
            border-left: solid $primary;
        }
        
        #branch-graph {
            height: 85%;
            background: $surface-darken-2;
        }
        
        #all-branches-container, #recent-commits-container {
            height: 30%;
            margin-bottom: 1;
            overflow-y: auto;
        }
        
        .branch-button {
            background: $success-darken-1;
            margin: 0 0 1 0;
            width: 100%;
            text-align: left;
        }
        
        .remote-branch-button {
            background: $accent-darken-1;
            margin: 0 0 1 0;
            width: 100%;
            text-align: left;
        }
        
        .commit-button {
            background: $surface-darken-1;
            margin: 0 0 1 0;
            width: 100%;
            text-align: left;
        }
        
        /* Remote development styling */
        #remote-dialog {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 70%;
            width: 60%;
            margin: 2 2;
        }
        
        #remote-files-panel {
            width: 60%;
            height: 100%;
        }
        
        #remote-preview-panel {
            width: 40%;
            height: 100%;
            border-left: solid $primary;
        }
        
        #remote-files-tree {
            height: 80%;
            margin-bottom: 1;
            overflow-y: auto;
        }
        
        #remote-file-preview {
            height: 95%;
        }
        
        /* Semantic search styling */
        #semantic-search-dialog {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 80%;
            width: 80%;
            margin: 2 2;
        }
        
        #semantic-search-input {
            margin-bottom: 1;
            border: solid $primary;
        }
        
        #semantic-results-container {
            height: 85%;
            margin-bottom: 1;
            overflow-y: auto;
        }
        
        .search-result {
            margin-bottom: 2;
            border: solid $primary;
            padding: 1;
        }
        
        .search-result-title {
            background: $primary-darken-1;
            padding: 0 1;
            margin-bottom: 1;
        }
        
        .search-result-explanation {
            margin-bottom: 1;
            padding: 0 1;
            color: $text-muted;
        }
        
        .search-result-snippet {
            height: 10;
            margin-bottom: 1;
            background: $surface-darken-1;
        }
        
        .search-result-open-btn {
            background: $success;
        }
        
        .search-loading {
            text-align: center;
            margin-top: 2;
            color: $text-muted;
        }
        
        .search-error {
            text-align: center;
            margin-top: 2;
            color: $error;
        }
        
        /* Diff view styling */
        #diff-view-container {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 90%;
            width: 95%;
            margin: 1 2;
        }
        
        #diff-split-view {
            height: 80%;
            margin-bottom: 1;
        }
        
        #diff-original-panel {
            width: 50%;
            height: 100%;
            border-right: solid $primary;
            padding-right: 1;
        }
        
        #diff-modified-panel {
            width: 50%;
            height: 100%;
            padding-left: 1;
        }
        
        #unified-diff-panel {
            height: 30%;
            margin-top: 1;
            border-top: solid $primary;
            padding-top: 1;
        }
        
        #diff-buttons {
            margin-top: 1;
            height: 3;
            align: center middle;
        }
        
        #diff-original-content .line-deleted {
            background: rgba(255, 0, 0, 0.2);
            color: $error;
        }
        
        #diff-modified-content .line-added {
            background: rgba(0, 255, 0, 0.2);
            color: $success;
        }
        
        #diff-title {
            text-align: center;
            background: $primary;
            color: $text;
            margin-bottom: 1;
        }
        
        .subtitle {
            background: $primary-darken-2;
            color: $text;
            text-align: center;
            margin-bottom: 1;
        }
        
        /* Git commit dialog styling */
        #commit-header {
            width: 100%;
            margin-bottom: 1;
        }
        
        .escape-hint {
            color: $text-muted;
            text-align: right;
            padding-right: 1;
            width: 50%;
        }
        
        /* Resizable panel styling */
        #main-layout {
            width: 100%;
            height: 100%;
        }
        
        #sidebar {
            width: 20%;
            min-width: 10%;
            max-width: 40%;
        }
        
        #editor-container {
            width: 60%;
            min-width: 30%;
            max-width: 80%;
        }
        
        #ai-panel {
            width: 20%;
            min-width: 10%;
            max-width: 40%;
        }
        
        .panel {
            overflow: auto;
        }
        
        .gutter {
            background: $primary;
            color: $text;
            width: 1;
            text-align: center;
            margin: 0 1;
        }
                
        .search-no-results {
            text-align: center;
            margin-top: 2;
            color: $warning;
        }
        
        #commit-message {
            height: 80%;
            margin: 1;
        }
        
        #analysis-result {
            height: 85%;
            margin: 1;
            overflow-y: scroll;
        }
        
        /* Status indicators */
        .git-status {
            color: $success;
        }
        
        .git-modified {
            color: $warning;
        }
        
        .git-untracked {
            color: $error;
        }
        
        /* AI styling */
        .ai-thinking {
            color: $accent;
            text-style: italic;
        }
        
        .ai-response {
            border-left: solid $primary;
            padding-left: 1;
        }
        
        .code-block {
            background: $surface-darken-2;
            margin: 1;
            padding: 1;
        }
        /* Add these to your CSS section */

        #theme-dialog {
            background: $surface;
            padding: 1;
            border: solid $primary;
            height: 60%;
            width: 40%;
            margin: 2 2;
        }

        #theme-list {
            height: 80%;
            overflow-y: auto;
        }

        .theme-button {
            width: 100%;
            margin: 0 0 1 0;
            text-align: left;
        }
    """

    BINDINGS = [
        Binding("ctrl+s", "save", "Save"),
        Binding("ctrl+o", "open", "Open"),
        Binding("f5", "run", "Run"),
        Binding("ctrl+q", "quit", "Quit"),
        Binding("ctrl+g", "git_commit", "Git"),
        Binding("ctrl+r", "ai_request", "AI"),
        Binding("ctrl+d", "add_multi_cursor", "Multi-cursor"),
        Binding("ctrl+k", "toggle_split_view", "Split View"),
        Binding("ctrl+tab", "switch_editor", "Switch Editor"),
        Binding("ctrl+t", "toggle_terminal", "Terminal"),
        Binding("ctrl+p", "show_command_palette", "Command Palette"),
        Binding("ctrl+space", "code_completion", "AI Code Completion"),
        Binding("f9", "toggle_breakpoint", "Toggle Breakpoint"),
        Binding("f10", "debug_current_file", "Debug"),
        Binding("ctrl+b", "show_branch_visualization", "Git Branches"),
        Binding("ctrl+shift+p", "toggle_pair_programming", "AI Pair Programming"),
        Binding("ctrl+shift+r", "connect_remote", "Remote Development"),
        Binding("ctrl+shift+c", "start_collaboration", "Collaborate"),
    ]

    # Track application state
    current_file = reactive(None)
    git_repository = reactive(None)
    editor_theme = reactive("monokai")

    # Add these variables to your class
    _last_status_update_time = 0
    _status_update_debounce = 0.5  # seconds

    # Define screens
    SCREENS = {
        "commit": CommitDialog,
        "analysis": CodeAnalysisDialog,
        "command_palette": CommandPalette,
        "debugger": DebuggerScreen,
        "branch_visualization": BranchVisualizationScreen,
        "remote_connection": RemoteConnectionDialog,
        "remote_browser": RemoteFilesBrowser,
        "theme_selection": ThemeSelectionScreen,
        "diff_view": DiffViewScreen,
    }

    def compose(self) -> ComposeResult:
        """Create the UI layout with resizable panels"""
        yield Header()

        # Main layout with resizable panels using the gutter parameter
        # This horizontal container will have resizable children
        with Horizontal(id="main-layout"):
            # Left sidebar with file explorer and git status - initially 20% width
            with Vertical(id="sidebar", classes="panel"):
                yield Label("File Explorer", classes="title")
                yield DirectoryTree(".", id="file-explorer")

                yield Label("Git Status", classes="title")
                with Vertical(id="git-status"):
                    yield Static("", id="git-output")
                    with Horizontal():
                        yield Button("Commit", id="commit-btn")
                        yield Button("Pull", id="pull-btn")
                        yield Button("Push", id="push-btn")
                        yield Button("Branches", id="branches-btn")

            # Resizer element between sidebar and editor
            yield Static("|", classes="gutter")

            # Center code editor and terminal - initially 60% width
            with Vertical(id="editor-container", classes="panel"):
                # Use TabPane directly instead of add_pane method
                with TabbedContent(id="editor-tabs"):
                    with TabPane("Editor", id="editor-tab-pane"):
                        yield Static(id="editor-tab")
                    with TabPane("Terminal", id="terminal-tab-pane"):
                        yield Static(id="terminal-tab")

                # Editor Tab
                with Container(id="editor-content"):
                    yield Label("Code Editor", classes="title")
                    with Horizontal(id="editor-split-view"):
                        
                        # Check if we have syntax extras before trying to use code_editor
                        try:
                            import tree_sitter_languages

                            # If syntax highlighting is available, use code_editor
                            # Create a Theme object for the editors
                            try:
                                theme_obj = Theme("monokai")
                            except Exception:
                                # Fallback for newer Textual versions that might use string directly
                                theme_obj = "monokai"

                            theme_str = "monokai"  # Default theme name as string

                            yield TextArea.code_editor(
                                language="python",
                                theme=theme_str,
                                show_line_numbers=True,
                                tab_behavior="indent",
                                id="editor-primary",
                            )
                            yield TextArea.code_editor(
                                language="python",
                                theme=theme_str,
                                show_line_numbers=True,
                                tab_behavior="indent",
                                id="editor-secondary",
                                classes="hidden",
                            )
                        except ImportError:
                            # Fall back to standard TextArea if syntax highlighting isn't available
                            yield TextArea(language="python", id="editor-primary")
                            yield TextArea(
                                language="python",
                                id="editor-secondary",
                                classes="hidden",
                            )
                    with Horizontal():
                        yield Button("Run", id="run-btn")
                        yield Button("Debug", id="debug-btn")
                        yield Button("Save", id="save-btn")
                        yield Button("Format", id="format-btn")
                        yield Button("Split View", id="split-view-btn")
                        yield Button("Theme", id="theme-btn")
                        yield Button(
                            "Pair Program", id="pair-program-btn", variant="primary"
                        )

                # Terminal Tab
                with Container(id="terminal-content", classes="hidden"):
                    yield Label("Integrated Terminal", classes="title")
                    yield TextArea(
                        language="bash", id="terminal-output", read_only=True
                    )
                    with Horizontal():
                        yield Input(
                            placeholder="Enter terminal command...", id="terminal-input"
                        )
                        yield Button("Execute", id="terminal-execute-btn")
                        yield Button("Remote", id="open-remote-btn")

            # Resizer element between editor and AI panel
            yield Static("|", classes="gutter")

            # Right AI panel - initially 20% width
            with Vertical(id="ai-panel", classes="panel"):
                yield Label("AI Assistant", classes="title")
                yield Markdown(
                    "Welcome to Terminator v1! Ask me anything about your code.",
                    id="ai-output",
                )
                with Vertical(id="ai-input"):
                    yield Input(placeholder="Ask the AI...", id="ai-prompt")
                    yield Button("Submit", id="ai-submit")

        yield Footer()

    def get_language_from_extension(self, extension):
        """Map file extension to language for syntax highlighting"""
        extension_map = {
            ".py": "python",
            ".js": "javascript",
            ".html": "html",
            ".css": "css",
            ".json": "json",
            ".md": "markdown",
            ".markdown": "markdown",
            ".txt": None,  # Use None for plain text files
            ".xml": "xml",
            ".yaml": "yaml",
            ".yml": "yaml",
            ".toml": "toml",
            ".rs": "rust",
            ".go": "go",
            ".sh": "bash",
            ".bash": "bash",
            ".sql": "sql",
            ".java": "java",
            # Add more mappings as needed
        }
        # Return None for text files instead of "text" to avoid tree-sitter error
        return extension_map.get(extension, None)

    @work(thread=False)
    async def set_editor_theme(self, theme_name: str) -> None:
        """Set the theme for all editors asynchronously"""
        try:
            # Store the theme name as string
            self.current_theme_name = theme_name

            # Apply theme to primary editor
            primary_editor = self.query_one("#editor-primary")
            primary_editor.theme = theme_name

            # Apply theme to secondary editor if it exists
            try:
                secondary_editor = self.query_one("#editor-secondary")
                secondary_editor.theme = theme_name
            except Exception:
                pass  # Secondary editor might not exist yet

            self.notify(f"Theme changed to: {theme_name}", severity="information")
        except Exception as e:
            self.notify(f"Error setting theme: {str(e)}", severity="error")

    def show_diff_view(
        self,
        original_content: str,
        modified_content: str,
        title: str = "Code Changes",
        language: str = "python",
        original_title: str = "Original",
        modified_title: str = "Modified",
    ) -> None:
        """
        Show the diff view popup for comparing original and modified content

        Args:
            original_content: The original content
            modified_content: The modified content
            title: Title for the diff view
            language: Language for syntax highlighting
            original_title: Title for the original content panel
            modified_title: Title for the modified content panel
        """
        try:
            # Create a wrapper function for the async callback
            async def apply_callback(content):
                await self.apply_diff_changes(content)

            # Create the diff screen with the async callback
            diff_screen = DiffViewScreen(
                original_content=original_content,
                modified_content=modified_content,
                title=title,
                original_title=original_title,
                modified_title=modified_title,
                highlight_language=language,
                on_apply_callback=apply_callback,  # Pass the async wrapper
            )

            # Push the screen
            self.push_screen(diff_screen)

        except Exception as e:
            self.notify(f"Error showing diff view: {str(e)}", severity="error")
            logging.error(f"Error showing diff view: {str(e)}", exc_info=True)

    def show_code_suggestion(
        self,
        original_content: str,
        new_content: str,
        title: str = "AI Suggested Changes",
    ) -> None:
        """
        Show code suggestions from the AI agent with a diff view
    
        Args:
            original_content: The original file content
            new_content: The suggested new content
            title: Title for the diff view popup
        """
        try:
            # Create a wrapper function for the async callback
            async def apply_callback(content):
                await self.apply_diff_changes(content)
                
            # Create a diff view screen with the provided content
            diff_screen = DiffViewScreen(
                original_content=original_content,
                modified_content=new_content,
                title=title,
                original_title="Current Code",
                modified_title="AI Suggestion",
                highlight_language=self._get_language_from_filename(self.current_file)
                if self.current_file
                else "python",
                on_apply_callback=apply_callback
            )
    
            # Push the screen
            self.push_screen(diff_screen)
    
            # Show a notification about the suggestion
            self.notify(
                "AI has suggested changes. Review and apply if desired.",
                severity="information",
            )
    
        except Exception as e:
            self.notify(f"Error showing code suggestion: {str(e)}", severity="error")
            logging.error(f"Error showing code suggestion: {str(e)}", exc_info=True)

    def _get_language_from_filename(self, filename: str) -> str:
        """
        Get the language for syntax highlighting based on file extension

        Args:
            filename: The filename to check

        Returns:
            Language identifier for syntax highlighting
        """
        if not filename:
            return "python"

        ext = os.path.splitext(filename)[1].lower()

        language_map = {
            ".py": "python",
            ".js": "javascript",
            ".ts": "typescript",
            ".jsx": "javascript",
            ".tsx": "typescript",
            ".html": "html",
            ".css": "css",
            ".json": "json",
            ".md": "markdown",
            ".xml": "xml",
            ".yaml": "yaml",
            ".yml": "yaml",
            ".sh": "bash",
            ".c": "c",
            ".cpp": "cpp",
            ".java": "java",
            ".go": "go",
            ".rs": "rust",
        }

        return language_map.get(ext, "python")

    @work(thread=True)
    async def apply_diff_changes(self, new_content: str) -> None:
        """
        Apply changes from diff view to the current file
    
        Args:
            new_content: The new content to apply
        """
        if not self.current_file:
            self.notify("No file selected to save changes to", severity="error")
            return
    
        try:
            # Get the active editor
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")
    
            # Update the editor content
            if hasattr(editor, "load_text"):
                editor.load_text(new_content)
            else:
                editor.text = new_content
    
            # Save the changes to the file asynchronously
            async with aiofiles.open(self.current_file, "w", encoding="utf-8") as file:
                await file.write(new_content)
    
            self.notify(
                f"Changes applied and saved to {os.path.basename(self.current_file)}",
                severity="success",
            )
    
            # Update git status if applicable
            if hasattr(self, "git_repository") and self.git_repository:
                asyncio.create_task(self.update_git_status())
    
        except Exception as e:
            self.notify(f"Error applying changes: {str(e)}", severity="error")
            logging.error(f"Error applying changes: {str(e)}", exc_info=True)

    def ensure_syntax_dependencies(self):
        """Ensure the syntax highlighting dependencies are installed"""
        try:
            # Try to import tree_sitter_languages, which is part of textual[syntax]
            import importlib

            importlib.import_module("tree_sitter_languages")
            return True
        except ImportError:
            # Don't try to highlight the word 'syntax' in the notification
            self.notify(
                "Syntax highlighting requires textual[syntax] package",
                severity="warning",
            )
            return False

    def initialize_agent_context(self):
        """Initialize the agent context with proper validation"""
        try:
            # Ensure current_directory is valid
            if not hasattr(self, "current_directory") or not self.current_directory:
                self.current_directory = os.getcwd()
                logging.info(
                    f"Set default current directory to: {self.current_directory}"
                )

            # Validate the directory exists
            if not os.path.exists(self.current_directory):
                logging.error(
                    f"Current directory doesn't exist: {self.current_directory}"
                )
                self.notify(
                    f"Invalid working directory: {self.current_directory}",
                    severity="error",
                )
                self.current_directory = os.getcwd()
                logging.info(
                    f"Falling back to current working directory: {self.current_directory}"
                )

            # Create agent context with validated directory
            self.agent_context = AgentContext(current_dir=self.current_directory)
            logging.info(
                f"Agent context initialized with directory: {self.current_directory}"
            )

            # Verify context was created correctly
            if not self.agent_context or not hasattr(self.agent_context, "current_dir"):
                logging.error("Failed to create valid agent context")
                self.notify("Failed to initialize AI agent context", severity="error")
                return False

            return True

        except Exception as e:
            logging.error(f"Error initializing agent context: {str(e)}", exc_info=True)
            self.notify(
                f"Error initializing AI agent context: {str(e)}", severity="error"
            )
            return False

    def on_mount(self):
        """Called when the app is mounted"""
        # Set up initial directory
        self.current_directory = os.getcwd()

        # Initialize editor state tracking
        self.active_editor = "primary"
        self.split_view_active = False
        self.multi_cursor_positions = []
        self.active_tab = "editor"
        self.terminal_history = []

        # Initialize resizable panel tracking
        self.resizing = False
        self.resizing_panel = None
        self.start_x = 0
        self.current_widths = {
            "sidebar": 20,  # Default sidebar width 20%
            "editor-container": 60,  # Default editor width 60%
            "ai-panel": 20,  # Default AI panel width 20%
        }

        # Check for syntax highlighting dependencies
        self.ensure_syntax_dependencies()

        # Initialize debugger state
        self.debug_session = None
        self.breakpoints = {}  # Format: {file_path: [line_numbers]}

        # Initialize AI pair programming state
        self.pair_programming_active = False
        self.pair_programming_timer = None
        self.last_edit_time = time.time()

        # Initialize remote development state
        self.remote_connected = False
        self.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None,
        }

        # Initialize logging
        logging.basicConfig(
            level=logging.INFO,
            format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
            handlers=[logging.FileHandler("terminator.log"), logging.StreamHandler()],
        )

        # Initialize the agent system
        self.notify("Initializing AI agents...")
        agent_initialized = initialize_agent_system()
        if agent_initialized:
            self.notify("AI agents ready", severity="information")
        else:
            self.notify(
                "Failed to initialize AI agents. Check OpenAI API key.",
                severity="error",
            )

        # Initialize agent context
        if self.initialize_agent_context():
            self.notify("AI agent context ready", severity="information")
        else:
            self.notify(
                "Failed to initialize AI agent context - some AI features may not work correctly",
                severity="error",
            )

        # Focus the file explorer by default
        self.query_one("#file-explorer").focus()

        # Check for git repository
        self.check_git_repository()

        # Initialize AI panel
        self.initialize_ai_panel()

        # Apply initial panel widths
        self._apply_panel_widths()

    async def on_input_submitted(self, event: Input.Submitted) -> None:
        """Handle input submission events for all inputs in the app"""
        input_id = event.input.id

        if input_id == "ai-prompt":
            # Execute AI request when Enter is pressed in the prompt input
            asyncio.create_task(self.action_ai_request())
        elif input_id == "terminal-input":
            # Execute terminal command when Enter is pressed
            asyncio.create_task(self.execute_terminal_command())
        elif input_id == "command-search":
            # Handle command palette search submission
            commands = self.screen.query_one("#command-list").query(Button)
            if commands:
                # Execute the first command in the filtered list
                command_id = commands[0].id
                if command_id.startswith("cmd-"):
                    command = command_id[4:]  # Remove "cmd-" prefix
                    self.action(command)
                    self.pop_screen()

    def check_git_repository(self):
        """Check if the current directory is a git repository"""
        is_repo, repo_root = GitManager.check_git_repo(self.current_directory)

        if is_repo:
            self.git_repository = repo_root
            self.update_git_status()
        else:
            self.git_repository = None
            git_output = self.query_one("#git-output")
            git_output.update("No Git repository found")

    def _apply_panel_widths(self):
        """Apply the current panel widths to the UI"""
        try:
            # Apply widths to each panel
            sidebar = self.query_one("#sidebar")
            editor = self.query_one("#editor-container")
            ai_panel = self.query_one("#ai-panel")

            sidebar.styles.width = f"{self.current_widths['sidebar']}%"
            editor.styles.width = f"{self.current_widths['editor-container']}%"
            ai_panel.styles.width = f"{self.current_widths['ai-panel']}%"
        except Exception as e:
            logging.error(f"Error applying panel widths: {str(e)}", exc_info=True)

    async def on_static_click(self, event) -> None:
        """Handle click events on static elements, including gutters"""
        if event.static.has_class("gutter"):
            # Determine which panel is being resized
            if event.static.query_one("#sidebar", default=None) is not None:
                self.resizing_panel = "sidebar"
            elif event.static.query_one("#editor-container", default=None) is not None:
                self.resizing_panel = "editor-container"
            else:
                self.resizing_panel = None
                return

            # Start resizing
            self.resizing = True
            self.start_x = event.screen_x

    async def on_mouse_down(self, event: MouseDown) -> None:
        """Handle mouse down events for gutter resizing"""
        # Check if we clicked on a gutter element
        target = self.get_widget_at(event.screen_x, event.screen_y)

        if target and isinstance(target, Static) and target.has_class("gutter"):
            # Find the adjacent panels for this gutter
            gutter_idx = list(self.query(".gutter")).index(target)

            if gutter_idx == 0:
                # First gutter - between sidebar and editor
                self.resizing_panel = "sidebar"
            elif gutter_idx == 1:
                # Second gutter - between editor and AI panel
                self.resizing_panel = "editor-container"

            # Start resizing
            self.resizing = True
            self.start_x = event.screen_x

            # Capture the mouse to receive events outside the gutter
            self.capture_mouse()

    async def on_mouse_up(self, event: MouseUp) -> None:
        """Handle mouse up events to stop resizing"""
        if self.resizing:
            self.resizing = False
            self.resizing_panel = None

            # Release the mouse capture
            self.release_mouse()

    async def on_mouse_move(self, event: MouseMove) -> None:
        """Handle mouse move events for panel resizing"""
        if not self.resizing or not self.resizing_panel:
            return

        # Calculate movement
        delta_x = event.screen_x - self.start_x
        if delta_x == 0:
            return

        # Convert to percentage of total width based on app width
        app_width = self.size.width
        delta_percent = (delta_x / app_width) * 100

        # Update panel widths with constraints
        if self.resizing_panel == "sidebar":
            # Resizing sidebar affects editor width
            new_sidebar_width = self.current_widths["sidebar"] + delta_percent
            new_editor_width = self.current_widths["editor-container"] - delta_percent

            # Apply constraints
            if 10 <= new_sidebar_width <= 40 and 30 <= new_editor_width <= 80:
                self.current_widths["sidebar"] = new_sidebar_width
                self.current_widths["editor-container"] = new_editor_width

                # Apply new widths
                sidebar = self.query_one("#sidebar")
                editor = self.query_one("#editor-container")
                sidebar.styles.width = f"{new_sidebar_width}%"
                editor.styles.width = f"{new_editor_width}%"

        elif self.resizing_panel == "editor-container":
            # Resizing editor affects AI panel width
            new_editor_width = self.current_widths["editor-container"] + delta_percent
            new_ai_width = self.current_widths["ai-panel"] - delta_percent

            # Apply constraints
            if 30 <= new_editor_width <= 80 and 15 <= new_ai_width <= 40:
                self.current_widths["editor-container"] = new_editor_width
                self.current_widths["ai-panel"] = new_ai_width

                # Apply new widths
                editor = self.query_one("#editor-container")
                ai_panel = self.query_one("#ai-panel")
                editor.styles.width = f"{new_editor_width}%"
                ai_panel.styles.width = f"{new_ai_width}%"

        # Update the start position for the next move
        self.start_x = event.screen_x

    def get_widget_at(self, x: int, y: int):
        """
        Get the widget at a specific screen coordinate

        Args:
            x: The x screen coordinate
            y: The y screen coordinate

        Returns:
            The widget at the given coordinates, or None if no widget is found
        """
        # Convert screen coordinates to app coordinates
        app_x = x
        app_y = y

        # Find the widget at the given position
        for widget in self.query("*"):
            # Get widget's region
            region = widget.region
            if region and region.contains(app_x, app_y):
                # Calculate offset within the widget
                offset = (app_x - region.x, app_y - region.y)
                return widget, offset

        return None, (0, 0)

    def initialize_ai_panel(self):
        """Initialize AI panel elements"""
        try:
            ai_prompt = self.query_one("#ai-prompt")
            ai_submit = self.query_one("#ai-submit")

            # Make sure the input can receive focus
            ai_prompt.can_focus = True

            # Make sure the button can be clicked and has the right styling
            ai_submit.can_focus = True
            ai_submit.variant = "primary"

            # Log successful initialization
            logging.info("AI panel initialized successfully")
        except Exception as e:
            logging.error(f"Error initializing AI panel: {str(e)}", exc_info=True)

    @work
    async def update_git_status(self):
        """Update the git status display"""
        current_time = time.time()
        if current_time - self._last_status_update_time < self._status_update_debounce:
            return
        self._last_status_update_time = current_time
        if not self.git_repository:
            return

        git_output = self.query_one("#git-output")
        git_output.update("Checking Git status...")

        try:
            # Use GitManager to get status
            status = GitManager.get_git_status(self.git_repository)

            if "error" in status:
                git_output.update(f"Git error: {status['error']}")
                return

            if status.get("clean", False):
                git_output.update("Working tree clean")
                return

            # Format the status output
            status_text = ""

            if status.get("modified_files"):
                status_text += " Modified files:\n"
                for file in status["modified_files"]:
                    status_text += f"  {file}\n"

            if status.get("untracked_files"):
                if status_text:
                    status_text += "\n"
                status_text += " Untracked files:\n"
                for file in status["untracked_files"]:
                    status_text += f"  {file}\n"

            if status.get("staged_files"):
                if status_text:
                    status_text += "\n"
                status_text += " Staged files:\n"
                for file in status["staged_files"]:
                    status_text += f"  {file}\n"

            git_output.update(status_text)

        except Exception as e:
            git_output.update(f"Error: {str(e)}")

    @work(thread=True)
    async def git_commit(self, message: str):
        """Commit changes to the Git repository asynchronously"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return
    
        try:
            # Use asyncio.subprocess for async process execution
            import asyncio.subprocess
    
            # First add all changes
            process = await asyncio.subprocess.create_subprocess_exec(
                "git", "add", ".",
                stdout=asyncio.subprocess.PIPE,
                stderr=asyncio.subprocess.PIPE,
                cwd=self.git_repository
            )
            
            stdout, stderr = await process.communicate()
            
            if process.returncode != 0:
                self.notify(f"Git add failed: {stderr.decode()}", severity="error")
                return
    
            # Then commit with GitManager (assuming it supports async)
            success, result_msg = await GitManager.git_commit_async(self.git_repository, message)
    
            if success:
                self.notify("Changes committed successfully", severity="success")
                # Update status after commit
                await self.update_git_status()
            else:
                self.notify(f"Commit failed: {result_msg}", severity="error")
    
        except Exception as e:
            self.notify(f"Error making commit: {str(e)}", severity="error")

    @work(thread=True)
    async def on_directory_tree_file_selected(self, event: DirectoryTree.FileSelected):
        """Handle file selection in the directory tree"""
        try:
            # Safely extract the path using proper error handling
            path = getattr(event, "path", None)
            if not path:
                # Try to access path through event.node.data (for newer Textual versions)
                node = getattr(event, "node", None)
                if node:
                    path = getattr(node, "data", None)

            # If still no path, use event as fallback (some Textual versions pass path directly)
            if not path and isinstance(event, (str, Path)):
                path = str(event)

            # Last resort - try to get path from the message itself
            if not path and hasattr(event, "_Message__data"):
                data = getattr(event, "_Message__data", {})
                path = data.get("path", None)

            if not path:
                raise ValueError("Could not determine file path from event")

            # Store the path and update window title
            self.current_file = path
            self.title = f"Terminator - {path}"

            # Load the file content
            async with aiofiles.open(path, "r", encoding="utf-8") as file:
                content = await file.read()

            # Get file extension for language detection
            extension = os.path.splitext(path)[1].lower()
            language = self.get_language_from_extension(extension)

            # Update the active editor
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")

            # Set the language and content
            editor.language = language

            # Use load_text if available (newer Textual versions)
            if hasattr(editor, "load_text"):
                editor.load_text(content)
            else:
                # Fallback to direct text assignment
                editor.text = content

            # Apply the current theme
            if hasattr(self, "current_theme_name") and self.current_theme_name is not None:
            # Use the theme name string directly
                editor.theme = self.current_theme_name
            elif hasattr(self, "editor_theme") and self.editor_theme is not None:
            # Use the editor_theme string directly
                editor.theme = self.editor_theme
            else:
                # Fallback to default theme
                editor.theme = "monokai"

            # Focus the editor
            editor.focus()

            # Add the file to recent files list if we maintain one
            if hasattr(self, "recent_files") and isinstance(self.recent_files, list):
                if path in self.recent_files:
                    self.recent_files.remove(path)
                self.recent_files.insert(0, path)
                # Keep list at reasonable size
                self.recent_files = self.recent_files[:10]

            # Notify about the detected language
            self.notify(
                f"File opened with {language} highlighting", severity="information"
            )

        except Exception as e:
            error_msg = str(e)
            logging.error(f"Error opening file: {error_msg}", exc_info=True)
            self.notify(f"Error opening file: {error_msg}", severity="error")

    async def toggle_split_view(self):
        """Toggle split view mode"""
        editor_split = self.query_one("#editor-split-view")
        secondary_editor = self.query_one("#editor-secondary")

        if self.split_view_active:
            # Disable split view
            secondary_editor.add_class("hidden")
            editor_split.remove_class("split-view")
            self.split_view_active = False
            self.active_editor = "primary"
            self.notify("Split view disabled")
        else:
            # Enable split view
            secondary_editor.remove_class("hidden")
            editor_split.add_class("split-view")
            self.split_view_active = True

            # Copy content from primary to secondary if secondary is empty
            primary_editor = self.query_one("#editor-primary")
            if not secondary_editor.text:
                secondary_editor.language = primary_editor.language
                secondary_editor.text = primary_editor.text

            self.notify("Split view enabled")

    def get_language_from_extension(self, extension):
        """Map file extension to language for syntax highlighting"""
        extension_map = {
            ".py": "python",
            ".js": "javascript",
            ".html": "html",
            ".css": "css",
            ".json": "json",
            ".md": "markdown",
            ".txt": None,  # Use None for plain text files
            # Add more as needed
        }
        # Return None for text files instead of "text" to avoid tree-sitter error
        return extension_map.get(extension, None)

    @work(thread=True)
    async def action_save(self):
        """Save the current file asynchronously"""
        if not self.current_file:
            self.notify("No file selected to save", severity="warning")
            return
    
        try:
            # Get the active editor
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")
    
            # Get content
            content = editor.text
    
            # Use aiofiles for async file I/O
            async with aiofiles.open(self.current_file, "w", encoding="utf-8") as file:
                await file.write(content)
    
            # Notify success
            self.notify(f"Saved {self.current_file}")
    
            # Schedule git status update as a separate task
            asyncio.create_task(self.update_git_status())
    
        except Exception as e:
            self.notify(f"Error saving file: {str(e)}", severity="error")

    @work(thread=True)
    async def action_run(self):
        """Run the current Python file"""
        if not self.current_file:
            self.notify("No file selected to run", severity="warning")
            return

        if not self.current_file.endswith(".py"):
            self.notify("Only Python files can be executed", severity="warning")
            return

        # Save before running
        self.action_save()

        try:
            # Show running indicator
            ai_output = self.query_one("#ai-output")
            ai_output.update(f"Running {os.path.basename(self.current_file)}...\n\n")

            # Execute the Python file
            import subprocess

            result = subprocess.run(
                [sys.executable, self.current_file],
                capture_output=True,
                text=True,
                cwd=os.path.dirname(self.current_file),
            )

            # Display the output
            output = (
                f"## Execution Results for {os.path.basename(self.current_file)}\n\n"
            )

            if result.stdout:
                output += f"### Output:\n```\n{result.stdout}\n```\n\n"

            if result.stderr:
                output += f"### Errors:\n```\n{result.stderr}\n```\n\n"

            if result.returncode == 0:
                output += f" Program completed successfully (exit code: 0)"
            else:
                output += f" Program failed with exit code: {result.returncode}"

            ai_output.update(output)

        except Exception as e:
            self.notify(f"Error running file: {str(e)}", severity="error")
            ai_output = self.query_one("#ai-output")
            ai_output.update(f"## Execution Error\n\n```\n{str(e)}\n```")

    async def action_ai_request(self):
        """Process an AI request"""
        # Get the prompt from the input field
        prompt_input = self.query_one("#ai-prompt")
        prompt = prompt_input.value.strip()

        # For debugging, add a notification to confirm this method is called
        self.notify(f"Processing AI request: {prompt}", severity="information")

        if not prompt:
            self.notify("Please enter a prompt for the AI", severity="warning")
            return

        # Clear the input field
        prompt_input.value = ""

        # Get current code from active editor for context
        try:
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")

            code_context = editor.text
        except Exception as e:
            self.notify(f"Error getting editor context: {str(e)}", severity="error")
            code_context = ""

        # Update the AI output with the query
        try:
            ai_output = self.query_one("#ai-output")
            # Get current content as a string - Markdown widgets use str() in newer Textual
            current_content = str(ai_output)

            # Update the markdown with the query
            ai_output.update(
                f"{current_content}\n\n### Your Question:\n{prompt}\n\n### AI Assistant:\n*Thinking...*"
            )
        except Exception as e:
            self.notify(f"Error updating AI output: {str(e)}", severity="error")

        # Call the AI agent
        try:
            worker = self.call_ai_agent(prompt, code_context)
            # Don't await here - worker will process in the background
        except Exception as e:
            self.notify(f"Error calling AI agent: {str(e)}", severity="error")
            logging.error(f"AI agent error: {str(e)}", exc_info=True)

    def _prepare_agent_prompt(self, prompt, code_context):
        """Prepare the full prompt for the AI agent"""
        is_code_completion = False
        if (
            prompt.lower().startswith("complete")
            or "autocomplete" in prompt.lower()
            or "finish this code" in prompt.lower()
        ):
            is_code_completion = True

        if code_context:
            if is_code_completion:
                return f"""Complete or suggest the next part of this code. 
                Analyze the code patterns and provide a detailed completion that follows the style and logic of the existing code.
                Return complete functions or code blocks, not just a single line.

                Code to complete:
                ```python
                {code_context}
                ```

                Provide the completed code only, without explanations."""
            else:
                return f"I'm working with this code:\n```python\n{code_context}\n```\n\nMy question: {prompt}"
        return prompt

    @work(thread=True)
    async def call_ai_agent(self, prompt, code_context):
        """Call the AI agent with the prompt and code context"""
        # Must return a value from work decorator
        return await self._process_ai_agent_call(prompt, code_context)

    async def _process_ai_agent_call(self, prompt, code_context):
        """Internal method to process AI agent call"""
        try:
            full_prompt = self._prepare_agent_prompt(prompt, code_context)
            result = await run_agent_query(full_prompt, self.agent_context)
            response = result.get("response", "I couldn't process that request.")
            self.call_after_refresh(self._update_ai_output_with_response, response)
            return response
        except Exception as e:
            error_message = f"An error occurred: {str(e)}"
            self.call_after_refresh(self._update_ai_output_with_response, error_message)
            return error_message

    def _update_ai_output_with_response(self, response):
        """Update the AI output widget with the response"""
        try:
            ai_output = self.query_one("#ai-output")
            # Get current content as a string - Markdown widgets use .update() in newer Textual
            current_content = str(ai_output)

            # Remove the "Thinking..." placeholder and add the real response
            if "*Thinking...*" in current_content:
                # Find everything before "Thinking..."
                thinking_pos = current_content.find("*Thinking...*")
                if thinking_pos > 0:
                    current_content = current_content[:thinking_pos]
                else:
                    # Just use a clean slate if we can't find the position
                    current_content = ""

            # Add the response and update the markdown
            ai_output.update(f"{current_content}{response}")

            # Check for code edits in the response
            self._check_for_code_suggestions(response)

        except Exception as e:
            self.notify(f"Error updating AI output: {str(e)}", severity="error")
            logging.error(f"Error updating AI output: {str(e)}", exc_info=True)

    async def on_mouse_down(self, event: MouseDown) -> None:
        """Handle mouse down events for gutter resizing"""
        # Check if we clicked on a gutter element
        target, _ = self.get_widget_at(event.screen_x, event.screen_y)

        if target and isinstance(target, Static) and target.has_class("gutter"):
            # Find the adjacent panels for this gutter
            gutter_idx = list(self.query(".gutter")).index(target)

            if gutter_idx == 0:
                # First gutter - between sidebar and editor
                self.resizing_panel = "sidebar"
            elif gutter_idx == 1:
                # Second gutter - between editor and AI panel
                self.resizing_panel = "editor-container"

            # Start resizing
            self.resizing = True
            self.start_x = event.screen_x

            # Capture the mouse to receive events outside the gutter
            self.capture_mouse()

            # Set the cursor to indicate resizing
            event.prevent_default()

    async def on_mouse_up(self, event) -> None:
        """Handle mouse up events to end panel resizing"""
        if self.resizing:
            self.resizing = False
            self.resizing_panel = None

    async def on_mouse_move(self, event) -> None:
        """Handle mouse move events for panel resizing"""
        if not self.resizing or not self.resizing_panel:
            return

        # Calculate movement
        delta_x = event.screen_x - self.start_x
        if delta_x == 0:
            return

        # Convert to percentage of total width based on app width
        app_width = self.size.width
        delta_percent = (delta_x / app_width) * 100

        # Update panel widths with constraints
        if self.resizing_panel == "sidebar":
            # Resizing sidebar affects editor width
            new_sidebar_width = self.current_widths["sidebar"] + delta_percent
            new_editor_width = self.current_widths["editor-container"] - delta_percent

            # Apply constraints
            if 10 <= new_sidebar_width <= 40 and 30 <= new_editor_width <= 80:
                self.current_widths["sidebar"] = new_sidebar_width
                self.current_widths["editor-container"] = new_editor_width

                # Apply new widths
                sidebar = self.query_one("#sidebar")
                editor = self.query_one("#editor-container")
                sidebar.styles.width = f"{new_sidebar_width}%"
                editor.styles.width = f"{new_editor_width}%"

        elif self.resizing_panel == "editor-container":
            # Resizing editor affects AI panel width
            new_editor_width = self.current_widths["editor-container"] + delta_percent
            new_ai_width = self.current_widths["ai-panel"] - delta_percent

            # Apply constraints
            if 30 <= new_editor_width <= 80 and 10 <= new_ai_width <= 40:
                self.current_widths["editor-container"] = new_editor_width
                self.current_widths["ai-panel"] = new_ai_width

                # Apply new widths
                editor = self.query_one("#editor-container")
                ai_panel = self.query_one("#ai-panel")
                editor.styles.width = f"{new_editor_width}%"
                ai_panel.styles.width = f"{new_ai_width}%"

        # Update start position for next movement
        self.start_x = event.screen_x

    def _check_for_code_suggestions(self, response):
        """
        Check if the AI response contains code suggestions and offer to show diff

        Args:
            response: The AI response text
        """
        try:
            # Only proceed if we have an active file
            if not self.current_file:
                return

            # Get the active editor content for comparison
            if self.active_editor == "primary":
                editor = self.query_one("#editor-primary")
            else:
                editor = self.query_one("#editor-secondary")

            current_content = editor.text

            # Check if the response contains code blocks
            code_blocks = re.findall(r"```(\w*)\n(.*?)```", response, re.DOTALL)

            if code_blocks:
                # Find the first Python code block that's a significant edit
                for lang, code in code_blocks:
                    # Skip if not Python code or if it's just a short snippet
                    if lang.lower() not in ["python", "py"] or len(code.strip()) < 10:
                        continue

                    # Skip if the code is too different from the current file
                    # This is a simple heuristic to avoid comparing unrelated code
                    if len(current_content) > 0 and len(code) > 0:
                        # If the suggested code is less than 20% similar to current content,
                        # it's probably unrelated
                        similarity = difflib.SequenceMatcher(
                            None, current_content, code
                        ).ratio()
                        if similarity < 0.2:
                            continue

                    # Calculate how different the code is
                    diff = CodeAnalyzer.create_diff(current_content, code)

                    # If there are actual differences, offer to preview and apply them
                    if diff and "+" in diff and "-" in diff:
                        # Start a background task to show a notification with action buttons
                        asyncio.create_task(
                            self._show_code_suggestion_notification(
                                current_content, code
                            )
                        )
                        break

        except Exception as e:
            logging.error(
                f"Error checking for code suggestions: {str(e)}", exc_info=True
            )

    async def _show_code_suggestion_notification(self, current_content, suggested_code):
        """
        Show notification for code suggestions with action buttons

        Args:
            current_content: Current file content
            suggested_code: Suggested code from AI
        """
        # Slight delay to ensure notification appears after the main response
        await asyncio.sleep(0.5)

        # Create a notification with action buttons
        extension = os.path.splitext(self.current_file)[1].lower()
        language = self.get_language_from_extension(extension)

        from textual.notifications import Notification

        class CodeSuggestionNotification(Notification):
            def __init__(self, app, current, suggested):
                self.app = app
                self.current = current
                self.suggested = suggested
                super().__init__(
                    title="AI Code Suggestion",
                    message="The AI has suggested code changes. Would you like to view them?",
                    timeout=20,
                )

            def on_button_pressed(self, event):
                button_id = event.button.id
                if button_id == "view-diff":
                    self.app.show_diff_view(
                        self.current,
                        self.suggested,
                        title="AI Suggested Changes",
                        language=language,
                    )
                elif button_id == "apply-directly":
                    self.app.apply_diff_changes(self.suggested)

            def compose(self):
                yield from super().compose()
                with self.content:
                    yield Button("View Changes", id="view-diff", variant="primary")
                    yield Button(
                        "Apply Directly", id="apply-directly", variant="warning"
                    )

        # Show the custom notification
        self.notify(
            CodeSuggestionNotification(self, current_content, suggested_code),
            severity="information",
        )

    # Handle tab changes
    async def on_tabbed_content_tab_activated(
        self, event: TabbedContent.TabActivated
    ) -> None:
        """Handle tab activation"""
        tab_id = event.tab.id

        self.notify(f"Tab activated: {tab_id}", severity="information")
        if tab_id == "editor-tab-pane":
            self.active_tab = "editor"
            self.query_one("#editor-content").remove_class("hidden")
            self.query_one("#terminal-content").add_class("hidden")
            # Focus the active editor
            if self.active_editor == "primary":
                self.query_one("#editor-primary").focus()
            else:
                self.query_one("#editor-secondary").focus()
        elif tab_id == "terminal-tab-pane":
            self.active_tab = "terminal"
            self.query_one("#terminal-content").remove_class("hidden")
            self.query_one("#editor-content").add_class("hidden")
            # Focus the terminal input
            self.query_one("#terminal-input").focus()

    # Handle button events
    async def on_button_pressed(self, event: Button.Pressed) -> None:
        """Handle button press events"""
        button_id = event.button.id

        if button_id == "commit-btn":
            self.action_git_commit()
        elif button_id == "pull-btn":
            self.action_git_pull()
        elif button_id == "push-btn":
            self.action_git_push()
        elif button_id == "branches-btn":
            self.action_show_branch_visualization()
        elif button_id == "run-btn":
            self.action_run()
        elif button_id == "debug-btn":
            self.action_debug_current_file()
        elif button_id == "save-btn":
            self.action_save()
        elif button_id == "format-btn":
            self.action_format_code()
        elif button_id == "split-view-btn":
            self.toggle_split_view()
        elif button_id == "theme-btn":
            self.push_screen(ThemeSelectionScreen())
        elif button_id == "pair-program-btn":
            self.action_toggle_pair_programming()
        elif button_id == "ai-submit":
            # Create a task to run the async method
            asyncio.create_task(self.action_ai_request())
        elif button_id == "terminal-execute-btn":
            self.execute_terminal_command()
        elif button_id == "open-remote-btn":
            self.action_connect_remote()

    async def action_show_branch_visualization(self) -> None:
        """Show the branch visualization screen"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return

        self.push_screen("branch_visualization")

    async def action_git_commit(self) -> None:
        """Show the commit dialog screen"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return

        self.push_screen(CommitDialog())

    @work
    async def action_git_pull(self) -> None:
        """Pull changes from remote repository"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return

        git_output = self.query_one("#git-output")
        git_output.update("Pulling changes...")

        try:
            # Use GitManager to pull changes
            success, result = GitManager.git_pull(self.git_repository)

            if success:
                self.notify("Changes pulled successfully", severity="success")
                git_output.update(f"Pull successful:\n{result}")
                # Update status after pull
                self.update_git_status()
            else:
                self.notify(f"Pull failed", severity="error")
                git_output.update(f"Pull failed:\n{result}")

        except Exception as e:
            self.notify(f"Error pulling changes: {str(e)}", severity="error")
            git_output.update(f"Error: {str(e)}")

    @work
    async def action_git_push(self) -> None:
        """Push changes to remote repository"""
        if not self.git_repository:
            self.notify("Not a Git repository", severity="error")
            return

        git_output = self.query_one("#git-output")
        git_output.update("Pushing changes...")

        try:
            # Use GitManager to push changes
            success, result = GitManager.git_push(self.git_repository)

            if success:
                self.notify("Changes pushed successfully", severity="success")
                git_output.update(f"Push successful:\n{result}")
            else:
                self.notify(f"Push failed", severity="error")
                git_output.update(f"Push failed:\n{result}")

        except Exception as e:
            self.notify(f"Error pushing changes: {str(e)}", severity="error")
            git_output.update(f"Error: {str(e)}")

    @work
    async def action_format_code(self) -> None:
        """Format the current code using autopep8 or black if available"""
        if not self.current_file:
            self.notify("No file selected for formatting", severity="warning")
            return

        if not self.current_file.endswith(".py"):
            self.notify("Only Python files can be formatted", severity="warning")
            return

        # Get the active editor content
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")

        code = editor.text

        # Use an agent query to format the code
        if hasattr(self, "agent_context"):
            self.notify("Formatting code with AI...", severity="information")

            try:
                # Format code query
                result = await run_agent_query(
                    "Format this Python code according to PEP 8 without changing functionality:\n```python\n"
                    + code
                    + "\n```",
                    self.agent_context,
                )

                response = result.get("response", "")

                # Extract the code from markdown code blocks
                code_block_pattern = r"```python\s*\n(.*?)```"
                matches = re.findall(code_block_pattern, response, re.DOTALL)

                if matches:
                    formatted_code = matches[0].strip()
                    editor.text = formatted_code
                    self.notify("Code formatted successfully", severity="success")
                else:
                    # Try without language specifier
                    code_block_pattern = r"```\s*\n(.*?)```"
                    matches = re.findall(code_block_pattern, response, re.DOTALL)
                    if matches:
                        formatted_code = matches[0].strip()
                        editor.text = formatted_code
                        self.notify("Code formatted successfully", severity="success")
                    else:
                        self.notify("Failed to format code", severity="error")

            except Exception as e:
                self.notify(f"Error formatting code: {str(e)}", severity="error")
        else:
            self.notify(
                "AI agent not initialized, cannot format code", severity="error"
            )

    @work(thread=True)
    async def action_analyze_code(self) -> None:
        """Analyze the current code for issues"""
        if not self.current_file:
            self.notify("No file selected for analysis", severity="warning")
            return
    
        if not self.current_file.endswith(".py"):
            self.notify("Only Python files can be analyzed", severity="warning")
            return
    
        # Get the active editor content
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")
    
        code = editor.text
    
        # Show the analysis dialog
        self.push_screen(CodeAnalysisDialog())
    
        try:
            # Use CodeAnalyzer with async features
            analysis = await CodeAnalyzer.analyze_python_code_async(code)
    
            # Format the result as markdown
            result_md = f"# Analysis of {os.path.basename(self.current_file)}\n\n"
    
            # Add issues
            if analysis.get("issues"):
                result_md += f"## Issues Found ({len(analysis['issues'])})\n\n"
                for issue in analysis["issues"]:
                    result_md += f"- **Line {issue.get('line', '?')}**: {issue.get('message', 'Unknown issue')} ({issue.get('type', 'unknown')})\n"
            else:
                result_md += "## No Issues Found\n\n"
    
            # Add recommendations
            if analysis.get("recommendations"):
                result_md += f"\n## Recommendations\n\n"
                for rec in analysis["recommendations"]:
                    result_md += f"- {rec}\n"
    
            # Also add code stats
            stats = await CodeAnalyzer.count_code_lines_async(code)
            result_md += f"\n## Code Statistics\n\n"
            result_md += f"- Total lines: {stats.get('total_lines', 0)}\n"
            result_md += f"- Code lines: {stats.get('code_lines', 0)}\n"
            result_md += f"- Comment lines: {stats.get('comment_lines', 0)}\n"
            result_md += f"- Blank lines: {stats.get('blank_lines', 0)}\n"
    
            # Post the completion event with the result
            self.post_message(self.CodeAnalysisComplete(result_md))
    
        except Exception as e:
            error_msg = f"# Error During Analysis\n\n{str(e)}"
            self.post_message(self.CodeAnalysisComplete(error_msg))

    async def on_code_analysis_complete(self, event: CodeAnalysisComplete) -> None:
        """Handle code analysis completion"""
        # Update the analysis result in the dialog
        analysis_dialog = self.query_one(CodeAnalysisDialog)
        analysis_result = analysis_dialog.query_one("#analysis-result")
        analysis_result.query_one(Markdown).update(event.analysis_result)

    def action_toggle_pair_programming(self) -> None:
        """Toggle AI pair programming mode"""
        # Toggle the pair programming mode
        self.pair_programming_active = not self.pair_programming_active

        # Update the button visual state
        pair_btn = self.query_one("#pair-program-btn")

        if self.pair_programming_active:
            # Start pair programming mode
            self.notify("AI Pair Programming mode activated", severity="success")
            pair_btn.variant = "error"  # Red button when active
            pair_btn.label = "Stop Pair Programming"

            # Create a timer to check for inactivity and provide suggestions
            if self.pair_programming_timer:
                self.pair_programming_timer.stop()

            # Start a timer that checks for inactivity every 5 seconds
            self.pair_programming_timer = self.set_interval(
                5, self.check_for_pair_programming_suggestion
            )

            # Start tracking edit time
            self.last_edit_time = time.time()

            # Add event handlers for text editing
            self.watch_text_area()
        else:
            # Stop pair programming mode
            self.notify("AI Pair Programming mode deactivated", severity="information")
            pair_btn.variant = "primary"  # Normal button when inactive
            pair_btn.label = "Pair Program"

            # Stop the timer
            if self.pair_programming_timer:
                self.pair_programming_timer.stop()
                self.pair_programming_timer = None

    def watch_text_area(self) -> None:
        """Start watching text area for changes in pair programming mode"""
        # In a full implementation, we would set up event handlers to track edits
        # We'll simulate this by updating the last_edit_time in certain methods
        pass

    def check_for_pair_programming_suggestion(self) -> None:
        """Check if it's time to provide a pair programming suggestion"""
        if not self.pair_programming_active:
            return

        # Check if the user has been inactive for more than 5 seconds
        current_time = time.time()
        if current_time - self.last_edit_time >= 5:
            # Time to generate a suggestion
            self.generate_pair_programming_suggestion()

    @work
    async def generate_pair_programming_suggestion(self) -> None:
        """Generate a pair programming suggestion based on current code"""
        if not self.current_file:
            return

        # Get the current code
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")

        code_context = editor.text

        if not code_context:
            return

        self.notify("AI suggesting improvements...", severity="information")

        # Create a specific prompt for pair programming suggestions
        prompt = """As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements."""

        # Call the AI with the pair programming prompt
        worker = self.call_ai_agent(prompt, code_context)

        # Reset the timer to avoid constant suggestions
        self.last_edit_time = time.time()

    async def on_text_area_changed(self, event: TextArea.Changed) -> None:
        """Handle text area changes"""
        # Update the last edit time when the user makes changes
        self.last_edit_time = time.time()

    def action_connect_remote(self) -> None:
        """Show the remote connection dialog"""
        self.push_screen("remote_connection")

    def configure_remote(self, **config) -> None:
        """Configure the remote connection"""
        # Update the remote config
        for key, value in config.items():
            if key in self.remote_config:
                self.remote_config[key] = value

        # Simulate connection
        self.notify(
            f"Connecting to {self.remote_config['host']}...", severity="information"
        )

        # In a real implementation, we would actually connect
        # For now, we'll just simulate a successful connection
        self.remote_connected = True

        # Show connection success message
        self.notify(
            f"Connected to {self.remote_config['host']} as {self.remote_config['username']}",
            severity="success",
        )

        # Show the remote files browser
        self.push_screen("remote_browser")

    def disconnect_remote(self) -> None:
        """Disconnect from the remote server"""
        if not self.remote_connected:
            self.notify("Not connected to a remote server", severity="warning")
            return

        # Simulate disconnection
        self.notify("Disconnecting from remote server...", severity="information")

        # Reset connection state
        self.remote_connected = False

        # Show disconnection message
        self.notify("Disconnected from remote server", severity="success")

    def action_upload_to_remote(self) -> None:
        """Upload a file to the remote server"""
        if not self.remote_connected:
            self.notify("Not connected to a remote server", severity="warning")
            return

        if not self.current_file:
            self.notify("No file selected for upload", severity="warning")
            return

        # Simulate file upload
        self.notify(
            f"Uploading {self.current_file} to remote server...", severity="information"
        )

        # In a real implementation, we would actually upload the file
        # For now, we'll just simulate a successful upload
        self.notify(
            f"Uploaded {os.path.basename(self.current_file)} to {self.remote_config['remote_path']}",
            severity="success",
        )

    def action_switch_editor(self) -> None:
        """Switch focus between primary and secondary editors in split view"""
        if not self.split_view_active:
            self.notify("Split view is not active", severity="warning")
            return

        if self.active_editor == "primary":
            self.active_editor = "secondary"
            self.query_one("#editor-secondary").focus()
            self.notify("Switched to secondary editor")
        else:
            self.active_editor = "primary"
            self.query_one("#editor-primary").focus()
            self.notify("Switched to primary editor")

    def action_add_multi_cursor(self) -> None:
        """Add a cursor at the current position or word"""
        # Get the active editor
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")

        # Get the current cursor position
        cursor_position = editor.cursor_position
        cursor_row, cursor_column = cursor_position

        # In a real implementation, this would add a cursor
        # For this example, we'll just notify about the cursor position
        self.notify(
            f"Multi-cursor added at row {cursor_row+1}, column {cursor_column+1}",
            severity="information",
        )

        # Track multi-cursor positions for future implementation
        self.multi_cursor_positions.append(cursor_position)

        # Add multi-cursor class to the editor
        editor.add_class("multi-cursor")

    async def action_toggle_split_view(self) -> None:
        """Toggle split view mode using keyboard shortcut"""
        self.toggle_split_view()

    async def action_show_command_palette(self) -> None:
        """Show the command palette for quick access to commands"""
        self.push_screen("command_palette")

    async def action_code_completion(self) -> None:
        """Trigger AI code completion on the current cursor position"""
        if not self.current_file:
            self.notify("No file open for code completion", severity="warning")
            return

        # Get the active editor
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")

        # Get the code context
        code_context = editor.text

        # Show a notification
        self.notify("Generating code completion...", severity="information")

        # Request code completion from AI
        worker = self.call_ai_agent("Complete this code", code_context)

    def action_toggle_breakpoint(self) -> None:
        """Toggle a breakpoint at the current line in the editor"""
        if not self.current_file:
            self.notify("No file open for setting breakpoints", severity="warning")
            return

        if not self.current_file.endswith(".py"):
            self.notify(
                "Breakpoints can only be set in Python files", severity="warning"
            )
            return

        # Get the active editor
        if self.active_editor == "primary":
            editor = self.query_one("#editor-primary")
        else:
            editor = self.query_one("#editor-secondary")

        # Get the current cursor position
        cursor_row, _ = editor.cursor_position
        line_number = cursor_row + 1  # Convert to 1-based line number

        # Initialize breakpoints for this file if not already present
        if self.current_file not in self.breakpoints:
            self.breakpoints[self.current_file] = []

        # Toggle the breakpoint
        if line_number in self.breakpoints[self.current_file]:
            self.breakpoints[self.current_file].remove(line_number)
            self.notify(
                f"Breakpoint removed at line {line_number}", severity="information"
            )
        else:
            # Verify that we can set a breakpoint at this line
            success, message = PythonDebugger.set_breakpoint(
                self.current_file, line_number
            )

            if success:
                self.breakpoints[self.current_file].append(line_number)
                self.notify(
                    f"Breakpoint set at line {line_number}", severity="information"
                )
            else:
                self.notify(f"Cannot set breakpoint: {message}", severity="error")

    def action_debug_current_file(self) -> None:
        """Start debugging the current file"""
        if not self.current_file:
            self.notify("No file open for debugging", severity="warning")
            return

        if not self.current_file.endswith(".py"):
            self.notify("Only Python files can be debugged", severity="warning")
            return

        # Save the file before debugging
        self.action_save()

        # Start a debugging session
        success, debug_session = PythonDebugger.start_debugging(self.current_file)

        if not success:
            error = debug_session.get("error", "Unknown error starting debugger")
            self.notify(f"Debug failed: {error}", severity="error")
            return

        # Store the debug session
        self.debug_session = debug_session

        # Add any existing breakpoints
        if self.current_file in self.breakpoints:
            self.debug_session["breakpoints"] = self.breakpoints[self.current_file]

        # Switch to the debugger screen
        self.push_screen("debugger")

        # Update the debugger UI
        self.update_debugger_ui()

    def update_debugger_ui(self) -> None:
        """Update the debugger UI with the current debug session info"""
        if not self.debug_session or not self.app.screen_stack[-1].id == "debugger":
            return

        # Update code view
        debug_code = self.app.screen_stack[-1].query_one("#debug-code")
        debug_code.text = self.debug_session.get("code", "")

        # Highlight current line
        current_line = self.debug_session.get("current_line", 1)

        # This would require extending TextArea to support line highlighting
        # For now, we'd just notify about the current line position
        self.notify(f"Debugging at line {current_line}", severity="information")

        # Update variables table
        variables_table = self.app.screen_stack[-1].query_one("#debug-variables")
        variables_table.clear()

        for var_name, var_info in self.debug_session.get("variables", {}).items():
            variables_table.add_row(
                var_name, var_info.get("type", "unknown"), var_info.get("value", "")
            )

        # Update call stack
        stack_table = self.app.screen_stack[-1].query_one("#debug-stack")
        stack_table.clear()

        for frame in self.debug_session.get("call_stack", []):
            stack_table.add_row(
                str(frame.get("frame", "")),
                frame.get("function", ""),
                frame.get("file", ""),
                str(frame.get("line", "")),
            )

        # Update output
        debug_output = self.app.screen_stack[-1].query_one("#debug-output")
        debug_output.text = self.debug_session.get("output", "")

    def debug_step_over(self) -> None:
        """Execute a step over command in the debugger"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return

        if not self.debug_session.get("active", False):
            self.notify("Debugging session has ended", severity="information")
            return

        # Execute step over
        self.debug_session = PythonDebugger.debug_step(self.debug_session, "step_over")

        # Update the UI
        self.update_debugger_ui()

    def debug_step_into(self) -> None:
        """Execute a step into command in the debugger"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return

        if not self.debug_session.get("active", False):
            self.notify("Debugging session has ended", severity="information")
            return

        # Execute step into
        self.debug_session = PythonDebugger.debug_step(self.debug_session, "step_into")

        # Update the UI
        self.update_debugger_ui()

    def debug_step_out(self) -> None:
        """Execute a step out command in the debugger"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return

        if not self.debug_session.get("active", False):
            self.notify("Debugging session has ended", severity="information")
            return

        # Execute step out
        self.debug_session = PythonDebugger.debug_step(self.debug_session, "step_out")

        # Update the UI
        self.update_debugger_ui()

    def debug_continue(self) -> None:
        """Execute a continue command in the debugger"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return

        if not self.debug_session.get("active", False):
            self.notify("Debugging session has ended", severity="information")
            return

        # Execute continue
        self.debug_session = PythonDebugger.debug_step(self.debug_session, "continue")

        # Update the UI
        self.update_debugger_ui()

    def debug_stop(self) -> None:
        """Stop the current debugging session"""
        if not self.debug_session:
            self.notify("No active debugging session", severity="error")
            return

        # Stop debugging
        self.debug_session = PythonDebugger.stop_debugging(self.debug_session)

        # Update the UI
        self.update_debugger_ui()

        # Return to the main screen if debugging has ended
        if not self.debug_session.get("active", False):
            self.pop_screen()
            self.notify("Debugging session terminated", severity="information")

    async def action_toggle_terminal(self) -> None:
        """Toggle between editor and terminal tabs"""
        editor_tabs = self.query_one("#editor-tabs")

        if self.active_tab == "editor":
            # Activate terminal tab
            editor_tabs.active = "terminal-tab"
            self.active_tab = "terminal"
            self.query_one("#terminal-content").remove_class("hidden")
            self.query_one("#editor-content").add_class("hidden")
            self.query_one("#terminal-input").focus()
        else:
            # Activate editor tab
            editor_tabs.active = "editor-tab"
            self.active_tab = "editor"
            self.query_one("#editor-content").remove_class("hidden")
            self.query_one("#terminal-content").add_class("hidden")
            if self.active_editor == "primary":
                self.query_one("#editor-primary").focus()
            else:
                self.query_one("#editor-secondary").focus()

    async def action_start_collaboration(self) -> None:
        """Start or join a real-time collaboration session"""
        # Show collaboration dialog
        self.push_screen(
            CollaborationSessionDialog(), callback=self.handle_collaboration_dialog
        )

    def handle_collaboration_dialog(self, result: Optional[Dict[str, Any]]) -> None:
        """
        Handle the result from the collaboration dialog

        Args:
            result: Dialog result
        """
        if result is None:
            # Dialog was canceled
            return

        action = result.get("action")

        if action == "create":
            # Create a new session
            username = result.get("username", "User")
            # Generate a random session ID
            import random

            session_id = "".join(
                random.choices("ABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789", k=6)
            )

            session_details = {
                "username": username,
                "session_id": session_id,
                "is_host": True,
            }

            # Start the collaboration server in a background task
            self.start_collaboration_server()

            # Open the collaboration screen
            self.push_screen(CollaborationScreen(session_details))

        elif action == "join":
            # Join an existing session
            username = result.get("username", "User")
            session_id = result.get("session_id", "")

            session_details = {
                "username": username,
                "session_id": session_id,
                "is_host": False,
            }

            # Open the collaboration screen
            self.push_screen(CollaborationScreen(session_details))

    @work
    async def start_collaboration_server(self) -> None:
        """Start the WebSocket server for real-time collaboration"""
        try:
            # Initialize collaboration manager if not already created
            if not hasattr(self, "collaboration_manager"):
                self.collaboration_manager = CollaborationManager()

            # Start the server
            await self.collaboration_manager.start_server()
            self.notify("Collaboration server started", severity="information")

        except Exception as e:
            self.notify(
                f"Failed to start collaboration server: {str(e)}", severity="error"
            )

    async def stop_collaboration_server(self) -> None:
        """Stop the WebSocket server for real-time collaboration"""
        if (
            hasattr(self, "collaboration_manager")
            and self.collaboration_manager.running
        ):
            await self.collaboration_manager.stop_server()
            self.notify("Collaboration server stopped", severity="information")

    async def on_text_area_cursor_moved(self, event) -> None:
        """
        Handle cursor movement events for collaborative editing

        Args:
            event: Cursor moved event
        """
        # Only process if we're in a collaboration session
        if not self.app.is_screen_active(CollaborationScreen):
            return

        # Get current position and forward to collaboration session
        text_area = event.text_area
        position = (text_area.cursor_location.row, text_area.cursor_location.column)

        # In a real implementation, this would send the cursor position to other users
        # For now, we just log it
        if (
            hasattr(self, "last_cursor_position")
            and self.last_cursor_position == position
        ):
            return

        self.last_cursor_position = position
        self.query_one("#terminal-content").add_class("hidden")
        if self.active_editor == "primary":
            self.query_one("#editor-primary").focus()
        else:
            self.query_one("#editor-secondary").focus()

    @work
    async def execute_terminal_command(self) -> None:
        """Execute command in the terminal"""
        terminal_input = self.query_one("#terminal-input")
        command = terminal_input.value.strip()

        if not command:
            self.notify("Please enter a command", severity="warning")
            return

        # Clear the input
        terminal_input.value = ""

        # Add command to terminal output
        terminal_output = self.query_one("#terminal-output")
        current_output = terminal_output.text
        terminal_output.text = f"{current_output}\n$ {command}\n"

        # Add to history
        self.terminal_history.append(command)

        try:
            # Execute the command in the current directory
            import subprocess

            result = subprocess.run(
                command,
                shell=True,
                capture_output=True,
                text=True,
                cwd=self.current_directory,
            )

            # Add output to terminal
            output = ""
            if result.stdout:
                output += result.stdout
            if result.stderr:
                output += result.stderr

            # Update terminal with output
            terminal_output.text = f"{terminal_output.text}{output}"

            # Auto-scroll to bottom
            terminal_output.scroll_to_line(len(terminal_output.text.splitlines()) - 1)

        except Exception as e:
            # Show error
            terminal_output.text = f"{terminal_output.text}Error: {str(e)}\n"

        # Focus input for next command
        terminal_input.focus()


if __name__ == "__main__":
    app = TerminatorApp()
    app.run()

```

My question: Can you see the file in the editor?
2025-04-03 13:01:28,668 - terminator_agents - INFO - Added 11979 tokens for query, total: 11979
2025-04-03 13:01:28,686 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: list.*file
2025-04-03 13:01:29,804 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 13:05:26,403 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-03 13:05:26,415 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-03 13:05:26,415 - terminator_agents - INFO - Agent system initialized successfully
2025-04-03 13:05:26,416 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 13:05:26,419 - root - INFO - AI panel initialized successfully
2025-04-03 13:05:52,071 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: Can you see the file in the editor?
2025-04-03 13:05:52,080 - terminator_agents - INFO - Added 1443 tokens for query, total: 1443
2025-04-03 13:05:52,086 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 13:05:56,651 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 13:05:58,494 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 13:05:58,607 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 13:06:02,285 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 13:06:46,108 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: Can you add some debugging to the file?
2025-04-03 13:06:46,116 - terminator_agents - INFO - Added 1443 tokens for query, total: 2886
2025-04-03 13:06:46,122 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 13:06:46,200 - openai._base_client - INFO - Retrying request to /responses in 0.416110 seconds
2025-04-03 13:06:48,900 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 13:08:06,223 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 13:08:11,216 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 13:11:21,030 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 13:11:25,985 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 13:11:58,791 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 13:11:58,949 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 13:11:58,953 - root - ERROR - Error checking for code suggestions: name 'difflib' is not defined
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 3290, in _check_for_code_suggestions
    similarity = difflib.SequenceMatcher(
                 ^^^^^^^
NameError: name 'difflib' is not defined
2025-04-03 13:12:02,314 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 13:29:56,029 - terminator_agents - ERROR - OPENAI_API_KEY not set in environment variables
2025-04-03 13:29:56,030 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 13:29:56,031 - root - INFO - AI panel initialized successfully
2025-04-03 13:30:59,381 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: Can you help me improve the test?
2025-04-03 13:30:59,386 - terminator_agents - INFO - Added 1442 tokens for query, total: 1442
2025-04-03 13:30:59,391 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 13:30:59,398 - terminator_agents - ERROR - Error in agent query: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1809, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 215, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 739, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 892, in _get_new_response
    model = cls._get_model(agent, run_config)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 942, in _get_model
    return run_config.model_provider.get_model(agent.model)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_provider.py", line 85, in get_model
    client = self._get_client()
             ^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_provider.py", line 71, in _get_client
    self._client = _openai_shared.get_default_openai_client() or AsyncOpenAI(
                                                                 ^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_client.py", line 345, in __init__
    raise OpenAIError(
openai.OpenAIError: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable
2025-04-03 13:31:01,910 - openai.agents - WARNING - OPENAI_API_KEY is not set, skipping trace export
2025-04-03 13:31:39,457 - httpx - INFO - HTTP Request: GET https://api.openai.com/v1/models "HTTP/1.1 200 OK"
2025-04-03 13:31:39,467 - terminator_agents - INFO - API key verification successful, found 65 models
2025-04-03 13:31:39,467 - terminator_agents - INFO - Agent system initialized successfully
2025-04-03 13:31:39,467 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 13:31:39,469 - root - INFO - AI panel initialized successfully
2025-04-03 13:32:09,106 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: Can you improve the test?
2025-04-03 13:32:09,110 - terminator_agents - INFO - Added 1440 tokens for query, total: 1440
2025-04-03 13:32:09,113 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 13:32:14,961 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 13:42:09,171 - openai._base_client - INFO - Retrying request to /responses in 0.458450 seconds
2025-04-03 13:52:09,712 - openai._base_client - INFO - Retrying request to /responses in 0.937132 seconds
2025-04-03 14:02:10,752 - openai.agents - ERROR - Error getting response: Request timed out.. (request_id: None)
2025-04-03 14:02:10,753 - terminator_agents - ERROR - Error in agent query: Request timed out.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 101, in map_httpcore_exceptions
    yield
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection.py", line 103, in handle_async_request
    return await self._connection.handle_async_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 136, in handle_async_request
    raise exc
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 106, in handle_async_request
    ) = await self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 177, in _receive_response_headers
    event = await self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 217, in _receive_event
    data = await self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_backends/anyio.py", line 32, in read
    with map_exceptions(exc_map):
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_exceptions.py", line 14, in map_exceptions
    raise to_exc(exc) from exc
httpcore.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1500, in _request
    response = await self._client.send(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 393, in handle_async_request
    with map_httpcore_exceptions():
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/contextlib.py", line 155, in __exit__
    self.gen.throw(typ, value, traceback)
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 118, in map_httpcore_exceptions
    raise mapped_exc(message) from exc
httpx.ReadTimeout

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1809, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 215, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 739, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 896, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 75, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 234, in _fetch_response
    return await self._client.responses.create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/resources/responses/responses.py", line 1415, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1461, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1509, in _request
    return await self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in _retry_request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1509, in _request
    return await self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in _retry_request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1519, in _request
    raise APITimeoutError(request=request) from err
openai.APITimeoutError: Request timed out.
2025-04-03 14:02:11,911 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:28:33,544 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 17:28:33,545 - root - INFO - AI panel initialized successfully
2025-04-03 17:29:31,557 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:29:31,564 - terminator_agents - INFO - Added 1470 tokens for query, total: 1470
2025-04-03 17:29:31,565 - terminator_agents - ERROR - Exception running agent query: RunConfig.__init__() got an unexpected keyword argument 'timeout'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1779, in run_agent_query
    run_config = RunConfig(
                 ^^^^^^^^^^
TypeError: RunConfig.__init__() got an unexpected keyword argument 'timeout'
2025-04-03 17:29:41,562 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:29:41,565 - terminator_agents - INFO - Added 1470 tokens for query, total: 2940
2025-04-03 17:29:41,566 - terminator_agents - ERROR - Exception running agent query: RunConfig.__init__() got an unexpected keyword argument 'timeout'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1779, in run_agent_query
    run_config = RunConfig(
                 ^^^^^^^^^^
TypeError: RunConfig.__init__() got an unexpected keyword argument 'timeout'
2025-04-03 17:29:51,558 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:29:51,560 - terminator_agents - INFO - Added 1470 tokens for query, total: 4410
2025-04-03 17:29:51,560 - terminator_agents - ERROR - Exception running agent query: RunConfig.__init__() got an unexpected keyword argument 'timeout'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1779, in run_agent_query
    run_config = RunConfig(
                 ^^^^^^^^^^
TypeError: RunConfig.__init__() got an unexpected keyword argument 'timeout'
2025-04-03 17:40:08,031 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 17:40:08,032 - root - INFO - AI panel initialized successfully
2025-04-03 17:40:49,683 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 17:40:49,684 - root - INFO - AI panel initialized successfully
2025-04-03 17:41:06,748 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:41:06,754 - terminator_agents - INFO - Added 1470 tokens for query, total: 1470
2025-04-03 17:41:06,762 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:41:10,409 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:41:16,750 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:41:16,754 - terminator_agents - INFO - Added 1470 tokens for query, total: 2940
2025-04-03 17:41:16,760 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:41:21,361 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:41:26,745 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:41:26,749 - terminator_agents - INFO - Added 1470 tokens for query, total: 4410
2025-04-03 17:41:26,754 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:41:29,254 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:41:29,789 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:41:29,949 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:41:31,752 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:41:31,756 - terminator_agents - INFO - Added 1470 tokens for query, total: 5880
2025-04-03 17:41:31,762 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:41:31,794 - openai._base_client - INFO - Retrying request to /responses in 0.407205 seconds
2025-04-03 17:41:32,182 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:41:33,162 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:41:41,748 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:41:41,751 - terminator_agents - INFO - Added 1470 tokens for query, total: 7350
2025-04-03 17:41:41,754 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:41:43,919 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:41:47,391 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:41:47,522 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:41:48,062 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:41:48,247 - openai._base_client - INFO - Retrying request to /responses in 0.390316 seconds
2025-04-03 17:41:49,694 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:41:51,748 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:41:51,749 - terminator_agents - INFO - Added 1470 tokens for query, total: 8820
2025-04-03 17:41:51,751 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:41:52,692 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:41:52,844 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:41:55,466 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:01,085 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:42:01,238 - openai._base_client - INFO - Retrying request to /responses in 0.413393 seconds
2025-04-03 17:42:01,747 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:42:01,751 - terminator_agents - INFO - Added 1470 tokens for query, total: 10290
2025-04-03 17:42:01,755 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:42:06,285 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:42:06,331 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:06,443 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:42:11,051 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:42:11,208 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:42:11,753 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:42:11,757 - terminator_agents - INFO - Added 1470 tokens for query, total: 11760
2025-04-03 17:42:11,763 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:42:11,797 - openai._base_client - INFO - Retrying request to /responses in 0.388466 seconds
2025-04-03 17:42:11,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:12,192 - openai._base_client - INFO - Retrying request to /responses in 0.868565 seconds
2025-04-03 17:42:12,594 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:21,750 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:42:21,754 - terminator_agents - INFO - Added 1470 tokens for query, total: 13230
2025-04-03 17:42:21,760 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:42:22,174 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:42:23,323 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:31,681 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:42:31,751 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:42:31,755 - terminator_agents - INFO - Added 1470 tokens for query, total: 14700
2025-04-03 17:42:31,761 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:42:34,113 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:41,751 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:42:41,755 - terminator_agents - INFO - Added 1470 tokens for query, total: 16170
2025-04-03 17:42:41,761 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:42:44,879 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:44,964 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:42:45,121 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:42:46,565 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:42:46,722 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:42:46,806 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:42:46,814 - terminator_agents - INFO - Added 1470 tokens for query, total: 17640
2025-04-03 17:42:46,819 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:42:46,834 - openai._base_client - INFO - Retrying request to /responses in 0.398535 seconds
2025-04-03 17:42:47,239 - openai._base_client - INFO - Retrying request to /responses in 0.932316 seconds
2025-04-03 17:42:50,629 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:52,989 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:42:53,148 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:42:56,360 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:42:56,739 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:42:56,743 - terminator_agents - INFO - Added 1470 tokens for query, total: 19110
2025-04-03 17:42:56,749 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:42:56,782 - openai._base_client - INFO - Retrying request to /responses in 0.389045 seconds
2025-04-03 17:43:01,949 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:43:06,117 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:43:06,735 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:43:06,739 - terminator_agents - INFO - Added 1470 tokens for query, total: 20580
2025-04-03 17:43:06,745 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:43:07,620 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:43:14,359 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:43:14,514 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:43:16,733 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:43:16,737 - terminator_agents - INFO - Added 1470 tokens for query, total: 22050
2025-04-03 17:43:16,743 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:43:16,776 - openai._base_client - INFO - Retrying request to /responses in 0.446169 seconds
2025-04-03 17:43:18,373 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:43:22,034 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:43:24,116 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:43:26,729 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:43:26,734 - terminator_agents - INFO - Added 1470 tokens for query, total: 23520
2025-04-03 17:43:26,740 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:43:29,678 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:43:30,879 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:43:31,036 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:43:35,402 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:43:35,485 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:43:35,653 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:43:36,731 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:43:36,737 - terminator_agents - INFO - Added 1470 tokens for query, total: 24990
2025-04-03 17:43:36,743 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:43:36,781 - openai._base_client - INFO - Retrying request to /responses in 0.406378 seconds
2025-04-03 17:43:37,190 - openai._base_client - INFO - Retrying request to /responses in 0.999339 seconds
2025-04-03 17:43:41,113 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:43:46,729 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:43:46,734 - terminator_agents - INFO - Added 1470 tokens for query, total: 26460
2025-04-03 17:43:46,740 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:43:52,275 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:43:56,756 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:43:56,759 - terminator_agents - INFO - Added 1470 tokens for query, total: 27930
2025-04-03 17:43:56,762 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:43:57,977 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:44:04,910 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:44:06,725 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:44:06,728 - terminator_agents - INFO - Added 1470 tokens for query, total: 29400
2025-04-03 17:44:06,730 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:44:08,882 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:44:16,726 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:44:16,730 - terminator_agents - INFO - Added 1470 tokens for query, total: 30870
2025-04-03 17:44:16,735 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:44:17,154 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:44:17,312 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:44:19,586 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:44:26,732 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:44:26,737 - terminator_agents - INFO - Added 1470 tokens for query, total: 32340
2025-04-03 17:44:26,742 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:44:26,773 - openai._base_client - INFO - Retrying request to /responses in 0.454142 seconds
2025-04-03 17:44:30,325 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:44:36,724 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:44:36,726 - terminator_agents - INFO - Added 1470 tokens for query, total: 33810
2025-04-03 17:44:36,730 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:44:41,281 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:44:46,732 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:44:46,733 - terminator_agents - INFO - Added 1470 tokens for query, total: 35280
2025-04-03 17:44:46,735 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:44:52,205 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:45:29,076 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:45:29,383 - terminator_agents - INFO - Successfully completed agent query
2025-04-03 17:45:29,388 - terminator_agents - INFO - Processing agent query: I'm working with this code:
```python
import os
import sys
import unittest
import pytest
from unittest.mock import patch, MagicMock, ANY
import time
import cProfile
import pstats
import io

# Add parent directory to path so we can import the application modules
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Import the application modules
from TerminatorV1_main import TerminatorApp

class TestTerminatorApp(unittest.TestCase):
    """Test suite for Terminator IDE application"""
    
    @patch('TerminatorV1_agents.initialize_agent_system')
    @patch('TerminatorV1_main.TerminatorApp.check_git_repository')
    @patch('TerminatorV1_main.TerminatorApp.initialize_agent_context')
    def test_app_initialization(self, mock_init_agent_context, mock_check_git, mock_init_agent):
        """Test application initialization for performance issues"""
        # Configure mocks
        mock_init_agent.return_value = True
        mock_init_agent_context.return_value = True
        
        # Profile the initialization
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a patch for query_one to avoid UI component errors
        mock_query_result = MagicMock()
        mock_query_result.focus = MagicMock()
        
        # Measure initialization time
        start_time = time.time()
        
        # Create the app but patch UI-dependent methods
        with patch.object(TerminatorApp, 'query_one', return_value=mock_query_result):
            with patch.object(TerminatorApp, 'update_git_status'):
                with patch.object(TerminatorApp, '_apply_panel_widths'):
                    with patch.object(TerminatorApp, 'initialize_ai_panel'):
                        app = TerminatorApp()
                        # Skip UI initialization in on_mount by patching problematic methods
                        with patch.object(app, 'query_one', return_value=mock_query_result):
                            # Call a modified version of on_mount that skips UI operations
                            self._modified_on_mount(app)
                            
                            # Explicitly call initialize_agent_context since it's not called in _modified_on_mount
                            app.initialize_agent_context()
        
        end_time = time.time()
        profiler.disable()
        
        # Output initialization time
        init_time = end_time - start_time
        print(f"\nApp initialization took {init_time:.2f} seconds")
        
        # Output profiling stats to string buffer
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(20)  # Print top 20 time-consuming functions
        print(s.getvalue())
        
        # Verify that core initialization completed
        mock_init_agent_context.assert_called_once()
        mock_init_agent.assert_called_once()
        
        # Ensure initialization time is reasonable
        self.assertLess(init_time, 2.0, "App initialization is too slow (> 2s)")
    
    def _modified_on_mount(self, app):
        """Modified version of on_mount that skips UI operations"""
        # Set up initial directory
        app.current_directory = os.getcwd()
        
        # Initialize editor state tracking
        app.active_editor = "primary"
        app.split_view_active = False
        app.multi_cursor_positions = []
        app.active_tab = "editor"
        app.terminal_history = []
        
        # Initialize resizable panel tracking
        app.resizing = False
        app.resizing_panel = None
        app.start_x = 0
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        
        # Initialize debugger state
        app.debug_session = None
        app.breakpoints = {}
        
        # Initialize AI pair programming state
        app.pair_programming_active = False
        app.pair_programming_timer = None
        app.last_edit_time = time.time()
        
        # Initialize remote development state
        app.remote_connected = False
        app.remote_config = {
            "connection_type": None,
            "host": None,
            "username": None,
            "port": 22,
            "password": None,
            "remote_path": None
        }
        
        # These calls would interact with UI, so skip them
        # app.check_git_repository()
        # app.initialize_ai_panel()
        # app._apply_panel_widths()
        
    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test content")
    def test_file_operations(self, mock_file):
        """Test file operations for performance issues"""
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/file.py"
        app.active_editor = "primary"
        
        # Create mock editor
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        
        # Mock get_language_from_extension
        app.get_language_from_extension.return_value = "python"
        
        # Profile file saving operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual action_save method
        original_save = TerminatorApp.action_save
        
        # Create a wrapper to call the original method with our mock
        async def run_save():
            # Extract just the synchronous part of action_save
            with patch('asyncio.create_task'):
                await original_save(app)
                
        # Run the save operation in a sync context for testing
        import asyncio
        asyncio.run(run_save())
        
        profiler.disable()
        
        # Verify the file write operation
        mock_file.assert_called_once_with("/test/file.py", "w", encoding="utf-8")
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile operation profiling:\n{s.getvalue()}")

    @patch('builtins.open', new_callable=unittest.mock.mock_open, read_data="test python content\ndef function():\n    pass")
    def test_file_loading(self, mock_file):
        """Test file loading performance"""
        app = MagicMock(spec=TerminatorApp)
        # Mock DirectoryTree.FileSelected event
        mock_event = MagicMock()
        mock_event.path = "/test/file.py"
        
        # Mock the editor components
        mock_editor = MagicMock()
        app.query_one.return_value = mock_editor
        app.active_editor = "primary"
        app.active_tab = "editor"
        
        # Mock language detection method
        app.get_language_from_extension.return_value = "python"
        
        # Profile the file loading operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual file selection handler
        original_handler = TerminatorApp.on_directory_tree_file_selected
        
        # Create a wrapper to call the original method with our mock
        async def run_file_load():
            with patch('os.path.splitext', return_value=(".py", ".py")):
                await original_handler(app, mock_event)
        
        # Run the file loading operation in a sync context
        import asyncio
        asyncio.run(run_file_load())
        
        profiler.disable()
        
        # Verify file was opened
        mock_file.assert_called_once_with("/test/file.py", "r", encoding="utf-8")
        
        # Verify editor was updated
        mock_editor.language.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nFile loading profiling:\n{s.getvalue()}")
    
    @patch('TerminatorV1_agents.run_agent_query')
    def test_ai_request_performance(self, mock_run_agent):
        """Test performance of AI request processing"""
        # Setup mock response
        mock_response = {"response": "This is a test AI response"}
        mock_run_agent.return_value = mock_response
        
        # Import AsyncMock for mocking async methods
        from unittest.mock import AsyncMock
        
        # Create application mock with appropriate async methods
        app = MagicMock(spec=TerminatorApp)
        app.agent_context = MagicMock()
        
        # Make methods that might be awaited into AsyncMocks
        app._update_ai_output_with_response = AsyncMock()
        app.call_after_refresh = AsyncMock()
        
        # Mock the UI elements
        mock_prompt_input = MagicMock()
        mock_prompt_input.value = "Test prompt"
        mock_ai_output = MagicMock()
        mock_ai_output.__str__ = MagicMock(return_value="Current content")
        
        app.query_one.side_effect = lambda selector: {
            "#ai-prompt": mock_prompt_input,
            "#ai-output": mock_ai_output,
            "#editor-primary": MagicMock(text="Test code")
        }.get(selector, MagicMock())
        
        app.active_editor = "primary"
        app._prepare_agent_prompt = TerminatorApp._prepare_agent_prompt
        
        # Profile the AI request operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Create a custom version of call_ai_agent for testing
        async def patched_call_ai(self, prompt, code):
            # Simplified version that just calls the agent
            context = self.agent_context or {"role": "assistant"}
            
            # Actually call the mocked function
            response = mock_run_agent(prompt=prompt, code=code, context=context)
            
            await self._update_ai_output_with_response(response)
            return response
        
        # Run the patched function
        import asyncio
        asyncio.run(patched_call_ai(app, "Test prompt", "Test code"))
        
        profiler.disable()
        
        # Verify AI agent was called
        mock_run_agent.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nAI request profiling:\n{s.getvalue()}")
    
    @patch('subprocess.run')
    def test_git_status_performance(self, mock_subprocess):
        """Test performance of Git status operations"""
        # Setup mock subprocess response for git status
        mock_process = MagicMock()
        mock_process.returncode = 0
        mock_process.stdout = "M file1.py\n?? file2.py"
        mock_subprocess.return_value = mock_process
        
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.git_repository = "/test/repo"
        
        # Mock git output widget
        mock_git_output = MagicMock()
        app.query_one.return_value = mock_git_output
        
        # Profile the git status update operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Use the actual git status method with patches
        with patch('TerminatorV1_main.GitManager.get_git_status') as mock_get_status:
            # Simulate modified and untracked files
            mock_get_status.return_value = {
                "modified_files": ["file1.py"],
                "untracked_files": ["file2.py"],
                "staged_files": [],
                "clean": False
            }
            
            # Run update_git_status
            original_update = TerminatorApp.update_git_status
            
            async def run_git_update():
                # Set _last_status_update_time to ensure update runs
                app._last_status_update_time = 0
                await original_update(app)
                
            # Run in sync context
            import asyncio
            asyncio.run(run_git_update())
        
        profiler.disable()
        
        # Verify git status was checked and output was updated
        mock_get_status.assert_called_once_with("/test/repo")
        mock_git_output.update.assert_called()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nGit status profiling:\n{s.getvalue()}")
    
    def test_code_analysis_performance(self):
        """Test performance of code analysis functionality"""
        test_code = """
def fibonacci(n):
    if n <= 1:
        return n
    else:
        return fibonacci(n-1) + fibonacci(n-2)
        
def main():
    for i in range(10):
        print(fibonacci(i))
        
if __name__ == "__main__":
    main()
"""
        # Create application mock
        app = MagicMock(spec=TerminatorApp)
        app.current_file = "/test/test_script.py"
        
        # Mock editor
        mock_editor = MagicMock()
        mock_editor.text = test_code
        app.active_editor = "primary"
        app.query_one.return_value = mock_editor
        
        # Mock the CodeAnalyzer methods
        with patch('TerminatorV1_main.CodeAnalyzer.analyze_python_code') as mock_analyze:
            mock_analyze.return_value = {
                "issues": [
                    {"line": 5, "message": "Recursive function could be optimized", "type": "performance"}
                ],
                "recommendations": ["Consider using memoization for the fibonacci function"]
            }
            
            with patch('TerminatorV1_main.CodeAnalyzer.count_code_lines') as mock_count:
                mock_count.return_value = {
                    "total_lines": 12,
                    "code_lines": 10,
                    "comment_lines": 0,
                    "blank_lines": 2
                }
                
                # Profile the code analysis operation
                profiler = cProfile.Profile()
                profiler.enable()
                
                # Mock the screen to post message to
                mock_screen = MagicMock()
                app.query_one.return_value = mock_screen
                app.post_message = MagicMock()
                
                # Use the original action with our mocks
                original_analyze = TerminatorApp.action_analyze_code
                
                async def run_analysis():
                    await original_analyze(app)
                    
                # Run in sync context
                import asyncio
                asyncio.run(run_analysis())
                
                profiler.disable()
                
                # Output profiling stats
                s = io.StringIO()
                stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
                stats.print_stats(10)
                print(f"\nCode analysis profiling:\n{s.getvalue()}")
                
                # Verify analysis was performed
                mock_analyze.assert_called_once_with(test_code)
                mock_count.assert_called_once_with(test_code)

    @patch('TerminatorV1_main.TerminatorApp._apply_panel_widths')
    def test_ui_responsiveness_resize(self, mock_apply_widths):
        """Test UI responsiveness during resizing operations"""
        # Create application instance with mocked components
        app = MagicMock(spec=TerminatorApp)
        app.resizing = True
        app.resizing_panel = "sidebar"
        app.start_x = 100
        app.current_widths = {
            "sidebar": 20,
            "editor-container": 60,
            "ai-panel": 20
        }
        app.size.width = 1000
        
        # Mock UI components
        mock_sidebar = MagicMock()
        mock_editor = MagicMock()
        app.query_one.side_effect = lambda selector: {
            "#sidebar": mock_sidebar,
            "#editor-container": mock_editor
        }.get(selector, MagicMock())
        
        # Create mock mouse event
        mock_event = MagicMock()
        mock_event.screen_x = 120  # 20px to the right of start_x
        
        # Profile the resize operation
        profiler = cProfile.Profile()
        profiler.enable()
        
        # Run the mouse move handler with our mocks
        original_mouse_move = TerminatorApp.on_mouse_move
        
        async def run_resize():
            await original_mouse_move(app, mock_event)
            
        # Run in sync context
        import asyncio
        asyncio.run(run_resize())
        
        profiler.disable()
        
        # Verify resize calculations were performed
        self.assertNotEqual(app.current_widths["sidebar"], 20)
        self.assertNotEqual(app.current_widths["editor-container"], 60)
        
        # Verify styles were updated
        mock_sidebar.styles.width.assert_called_once()
        mock_editor.styles.width.assert_called_once()
        
        # Output profiling stats
        s = io.StringIO()
        stats = pstats.Stats(profiler, stream=s).sort_stats('cumulative')
        stats.print_stats(10)
        print(f"\nUI resize profiling:\n{s.getvalue()}")
    
    def test_memory_usage(self):
        """Test memory usage of the application"""
        import psutil
        import gc
        
        # Force garbage collection to get accurate baseline
        gc.collect()
        
        # Get baseline memory usage
        process = psutil.Process(os.getpid())
        baseline_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Create minimal application instance
        with patch.object(TerminatorApp, 'on_mount'):
            with patch.object(TerminatorApp, 'compose'):
                app = TerminatorApp()
        
        # Force garbage collection again
        gc.collect()
        
        # Measure memory after app creation
        app_memory = process.memory_info().rss / 1024 / 1024  # Convert to MB
        
        # Calculate memory used by the app
        app_memory_usage = app_memory - baseline_memory
        
        print(f"\nMemory usage test results:")
        print(f"Baseline memory: {baseline_memory:.2f} MB")
        print(f"Memory after app creation: {app_memory:.2f} MB")
        print(f"Application memory usage: {app_memory_usage:.2f} MB")
        
        # Memory usage should be reasonable
        self.assertLess(app_memory_usage, 100.0, "Application uses too much memory (>100MB)")

if __name__ == "__main__":
    unittest.main()

```

My question: As your AI pair programmer, I'm analyzing your code. 
        Please provide detailed suggestions for improvements, optimizations, 
        potential bugs, or code style enhancements. Focus on being helpful but 
        concise. Don't rewrite everything, just suggest targeted improvements.
2025-04-03 17:45:29,393 - terminator_agents - INFO - Added 1470 tokens for query, total: 36750
2025-04-03 17:45:29,398 - terminator_agents - INFO - Security guardrail: Allowing file operation matching pattern: ls 
2025-04-03 17:45:29,419 - openai._base_client - INFO - Retrying request to /responses in 0.427165 seconds
2025-04-03 17:45:33,609 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:45:43,208 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=CancelledError()>
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_markdown.py", line 1007, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError
2025-04-03 17:45:43,211 - asyncio - ERROR - _GatheringFuture exception was never retrieved
future: <_GatheringFuture finished exception=CancelledError()>
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/textual/widgets/_markdown.py", line 1007, in await_update
    tokens = await asyncio.get_running_loop().run_in_executor(
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
asyncio.exceptions.CancelledError
2025-04-03 17:50:49,599 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 17:50:49,600 - root - INFO - AI panel initialized successfully
2025-04-03 17:52:06,476 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:52:13,499 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:52:17,205 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 17:54:17,979 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 17:54:20,524 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:08:27,418 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 18:08:27,419 - root - INFO - AI panel initialized successfully
2025-04-03 18:10:04,795 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:18:10,117 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:18:20,832 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:18:26,174 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:18:28,135 - openai._base_client - INFO - Retrying request to /responses in 0.468579 seconds
2025-04-03 18:18:31,667 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:18:42,362 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:18:48,959 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:18:53,039 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:18:58,137 - openai._base_client - INFO - Retrying request to /responses in 0.466907 seconds
2025-04-03 18:18:58,663 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:19:04,186 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:19:08,134 - openai._base_client - INFO - Retrying request to /responses in 0.406741 seconds
2025-04-03 18:19:09,367 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:19:16,192 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:19:20,137 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:19:58,054 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 502 Bad Gateway"
2025-04-03 18:19:58,055 - openai._base_client - INFO - Retrying request to /responses in 0.970320 seconds
2025-04-03 18:19:58,407 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:19:59,030 - openai.agents - ERROR - Error getting response: Connection error.. (request_id: None)
2025-04-03 18:19:59,030 - terminator_agents - ERROR - Error in agent query: Connection error.
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1500, in _request
    response = await self._client.send(
               ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1629, in send
    response = await self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1657, in _send_handling_auth
    response = await self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1694, in _send_handling_redirects
    response = await self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_client.py", line 1730, in _send_single_request
    response = await transport.handle_async_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpx/_transports/default.py", line 394, in handle_async_request
    resp = await self._pool.handle_async_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 256, in handle_async_request
    raise exc from None
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection_pool.py", line 236, in handle_async_request
    response = await connection.handle_async_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/connection.py", line 103, in handle_async_request
    return await self._connection.handle_async_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 135, in handle_async_request
    await self._response_closed()
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 250, in _response_closed
    await self.aclose()
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_async/http11.py", line 258, in aclose
    await self._network_stream.aclose()
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/httpcore/_backends/anyio.py", line 53, in aclose
    await self._stream.aclose()
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/anyio/streams/tls.py", line 216, in aclose
    await self.transport_stream.aclose()
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/anyio/_backends/_asyncio.py", line 1314, in aclose
    self._transport.close()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/selector_events.py", line 839, in close
    self._loop.call_soon(self._call_connection_lost, None)
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 761, in call_soon
    self._check_closed()
  File "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py", line 519, in _check_closed
    raise RuntimeError('Event loop is closed')
RuntimeError: Event loop is closed

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_agents.py", line 1841, in run_agent_query
    result = await Runner.run(
             ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 215, in run
    input_guardrail_results, turn_result = await asyncio.gather(
                                           ^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 739, in _run_single_turn
    new_response = await cls._get_new_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/run.py", line 896, in _get_new_response
    new_response = await model.get_response(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 75, in get_response
    response = await self._fetch_response(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/agents/models/openai_responses.py", line 234, in _fetch_response
    return await self._client.responses.create(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/resources/responses/responses.py", line 1415, in create
    return await self._post(
           ^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1767, in post
    return await self.request(cast_to, opts, stream=stream, stream_cls=stream_cls)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1461, in request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1524, in _request
    return await self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in _retry_request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1524, in _request
    return await self._retry_request(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1594, in _retry_request
    return await self._request(
           ^^^^^^^^^^^^^^^^^^^^
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/Imacvenv/lib/python3.11/site-packages/openai/_base_client.py", line 1534, in _request
    raise APIConnectionError(request=request) from err
openai.APIConnectionError: Connection error.
2025-04-03 18:20:01,761 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:20:04,282 - openai._base_client - INFO - Retrying request to /responses in 0.409628 seconds
2025-04-03 18:20:27,915 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:21:04,110 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:21:09,249 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:21:18,727 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:21:20,155 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:22:00,482 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:22:00,638 - openai._base_client - INFO - Retrying request to /responses in 0.970684 seconds
2025-04-03 18:22:36,089 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:22:37,381 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:23:11,240 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:23:13,955 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:28:08,187 - openai._base_client - INFO - Retrying request to /responses in 0.474026 seconds
2025-04-03 18:28:27,975 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:28:30,982 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:33:11,437 - openai._base_client - INFO - Retrying request to /responses in 0.450236 seconds
2025-04-03 18:33:29,921 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 18:33:30,658 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 18:36:31,270 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 18:36:31,272 - root - INFO - AI panel initialized successfully
2025-04-03 19:05:15,448 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 19:05:15,449 - root - INFO - AI panel initialized successfully
2025-04-03 19:08:09,491 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:09:52,256 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:10:03,100 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:10:13,964 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:10:24,759 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:10:35,773 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:10:39,255 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:10:41,449 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:10:49,962 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:10:52,440 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:10:53,412 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:10:53,585 - root - INFO - Found 0 code blocks in response
2025-04-03 19:10:58,105 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:11:00,754 - openai._base_client - INFO - Retrying request to /responses in 0.489168 seconds
2025-04-03 19:11:03,732 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:11:10,862 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:11:11,021 - root - INFO - Found 3 code blocks in response
2025-04-03 19:11:11,044 - root - INFO - Code block similarity: 0.01
2025-04-03 19:11:11,113 - root - INFO - Code block similarity: 0.01
2025-04-03 19:11:11,124 - root - INFO - Code block similarity: 0.02
2025-04-03 19:11:11,124 - root - INFO - Not showing code suggestion: similarity=0.02
2025-04-03 19:11:16,160 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:11:27,528 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:11:27,689 - openai._base_client - INFO - Retrying request to /responses in 0.990172 seconds
2025-04-03 19:11:42,066 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:11:42,199 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:11:42,229 - root - INFO - Found 1 code blocks in response
2025-04-03 19:11:42,269 - root - INFO - Code block similarity: 0.01
2025-04-03 19:11:42,270 - root - INFO - Not showing code suggestion: similarity=0.01
2025-04-03 19:11:47,911 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:13:10,017 - openai._base_client - INFO - Retrying request to /responses in 0.424029 seconds
2025-04-03 19:13:15,220 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:13:31,936 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:13:32,111 - root - INFO - Found 0 code blocks in response
2025-04-03 19:13:36,229 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:14:22,742 - openai._base_client - INFO - Retrying request to /responses in 0.379659 seconds
2025-04-03 19:14:27,961 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:15:45,307 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 502 Bad Gateway"
2025-04-03 19:15:45,310 - openai._base_client - INFO - Retrying request to /responses in 0.457685 seconds
2025-04-03 19:16:00,620 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:16:05,476 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:18:07,430 - openai._base_client - INFO - Retrying request to /responses in 0.430588 seconds
2025-04-03 19:18:34,325 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 19:18:34,326 - root - INFO - AI panel initialized successfully
2025-04-03 19:19:20,689 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-03 19:19:20,689 - root - INFO - AI panel initialized successfully
2025-04-03 19:20:06,968 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:20:38,100 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:20:43,297 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-03 19:20:57,466 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-03 19:20:57,643 - root - INFO - Found 1 code blocks in response
2025-04-03 19:20:57,643 - root - INFO - Skipping non-Python code block: diff
2025-04-03 19:20:57,644 - root - INFO - Not showing code suggestion: similarity=0.00
2025-04-03 19:20:59,070 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-04 19:46:48,146 - terminator_agents - ERROR - OPENAI_API_KEY not set in environment variables
2025-04-04 19:46:48,147 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-04 19:46:48,147 - root - INFO - AI panel initialized successfully
2025-04-04 19:47:18,551 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-04 19:47:18,558 - root - INFO - AI panel initialized successfully
2025-04-04 19:48:10,710 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-04 19:48:46,883 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-04 19:48:52,874 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-04 19:49:47,127 - openai._base_client - INFO - Retrying request to /responses in 0.426801 seconds
2025-04-04 19:50:48,360 - openai._base_client - INFO - Retrying request to /responses in 0.836961 seconds
2025-04-04 19:51:45,139 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/responses "HTTP/1.1 200 OK"
2025-04-04 19:51:45,302 - root - ERROR - Error showing diff view: DiffViewScreen.__init__() got an unexpected keyword argument 'on_apply_callback'
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2347, in show_diff_view
    diff_screen = DiffViewScreen(
                  ^^^^^^^^^^^^^^^
TypeError: DiffViewScreen.__init__() got an unexpected keyword argument 'on_apply_callback'
2025-04-04 19:51:45,305 - asyncio - ERROR - Task exception was never retrieved
future: <Task finished name='Task-3125' coro=<TerminatorApp._process_agent_code_suggestions() done, defined at /Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py:2258> exception=TypeError("object NoneType can't be used in 'await' expression")>
Traceback (most recent call last):
  File "/Users/kevinvanosch/Documents/TextualAgents/Textual/TerminatorV1_main.py", line 2312, in _process_agent_code_suggestions
    await self.show_diff_view(
TypeError: object NoneType can't be used in 'await' expression
2025-04-04 19:51:47,817 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/traces/ingest "HTTP/1.1 204 No Content"
2025-04-04 19:59:53,687 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-04 19:59:53,688 - root - INFO - AI panel initialized successfully
2025-04-07 12:04:08,883 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-07 12:04:08,884 - root - INFO - AI panel initialized successfully
2025-04-07 12:06:00,469 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-07 12:06:00,478 - root - INFO - AI panel initialized successfully
2025-04-07 12:06:20,589 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-07 12:06:20,590 - root - INFO - AI panel initialized successfully
2025-04-07 12:08:51,965 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-07 12:08:51,966 - root - INFO - AI panel initialized successfully
2025-04-07 12:08:53,728 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,738 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,755 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,787 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,821 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,839 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,856 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,872 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,889 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:53,905 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,256 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,273 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,354 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,456 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,509 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,542 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,808 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,839 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:54,986 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:55,253 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:08:55,564 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:55,779 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:08:55,779 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:55,805 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:55,889 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:55,942 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,009 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,032 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,159 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,276 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,302 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,369 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,521 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,540 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,569 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,572 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,605 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,619 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,636 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,653 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,682 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,697 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:56,756 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:08:56,942 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:08:56,955 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,044 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,046 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,046 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,105 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,126 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:08:57,257 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:08:57,361 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:08:57,457 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,492 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:08:57,493 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,506 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,533 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,539 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,568 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,572 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,586 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,605 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,619 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,637 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,890 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,906 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,922 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,938 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,955 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,975 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:57,987 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,009 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,089 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,105 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,122 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,142 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,153 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,171 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,189 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,209 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,241 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,271 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,288 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,325 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,407 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,438 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,454 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,571 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:08:58,676 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:08:58,741 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,767 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:08:58,768 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,839 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:08:58,893 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:08:58,994 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:08:59,077 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:00,333 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:00,338 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:00,338 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:00,338 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:00,340 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:00,492 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:00,591 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:00,678 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:00,774 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:00,822 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:00,875 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:00,959 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:01,033 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:01,158 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:01,245 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:01,326 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:01,390 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:01,511 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:01,573 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:01,695 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:01,778 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:01,825 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:01,926 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:02,010 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:02,094 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:02,192 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:02,259 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:02,378 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:02,446 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:02,562 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:02,563 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:02,627 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:02,628 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:02,705 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:02,742 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:02,812 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:02,872 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:02,892 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:03,188 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,204 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,222 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,238 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,255 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,272 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,289 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,306 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,322 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:03,338 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:04,829 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:04,947 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:05,312 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:05,442 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:05,520 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,537 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,559 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:05,675 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,696 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:05,697 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,724 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,740 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,753 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,770 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,790 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,817 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,820 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,855 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:05,959 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:06,104 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:06,127 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:06,259 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:06,305 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:06,362 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:06,465 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:06,466 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:06,523 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:06,549 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:06,644 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:06,734 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:06,807 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:06,884 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:06,992 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:07,038 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,053 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,073 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,100 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,103 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,135 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,139 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,195 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,197 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,197 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,205 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,221 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,235 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,251 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,268 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,588 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,657 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,670 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,691 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,705 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,721 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,738 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,755 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,772 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:07,873 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,024 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,036 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,054 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,072 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,099 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,133 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,174 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,326 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,358 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,359 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,370 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,401 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,415 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,419 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,438 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,452 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,471 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,622 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,638 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,658 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,685 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,788 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,856 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,888 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,905 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:08,936 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:09,006 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:09,190 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:09,369 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:09,423 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:09,531 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:09,643 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:10,364 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:10,441 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:10,689 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:10,845 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:10,984 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:11,087 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:11,163 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:11,240 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:11,331 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:11,445 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:11,543 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:11,601 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:11,666 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:11,760 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:11,857 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:11,916 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:11,996 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:12,144 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:12,203 - root - ERROR - Error in mouse down handler: object MouseDown can't be used in 'await' expression
2025-04-07 12:09:12,276 - root - ERROR - Error in mouse up handler: object MouseUp can't be used in 'await' expression
2025-04-07 12:09:12,418 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,452 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,469 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,486 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,515 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,523 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,535 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,552 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,571 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,586 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,602 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,703 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,755 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,771 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,789 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,805 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,822 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,839 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:09:12,855 - root - ERROR - Error in mouse move handler: object MouseMove can't be used in 'await' expression
2025-04-07 12:15:09,949 - root - INFO - Agent context initialized with directory: /Users/kevinvanosch/Documents/TextualAgents/Textual
2025-04-07 12:15:09,955 - root - INFO - AI panel initialized successfully
